[
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to easily produce a data dictionary straight from the dataset itself.\n\n\nThis tutorial was co-written with Dr Dan Evans, drawing on her existing resources for Qualtrics and her extensive experience supporting dissertation students.\nThe material in this tutorial was originally co-conceived with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin.\n\n\n\nThe first section of the tutorial gives advice for setting up a Qualtrics questionnaire, and will help you understand how the questionnaire you build will correspond to the dataset you get at the end.\nIf you are in a live workshop or already have data to work with, jump down to Setup to get started with the data portion."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#overview",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#overview",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to easily produce a data dictionary straight from the dataset itself.\n\n\nThis tutorial was co-written with Dr Dan Evans, drawing on her existing resources for Qualtrics and her extensive experience supporting dissertation students.\nThe material in this tutorial was originally co-conceived with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin.\n\n\n\nThe first section of the tutorial gives advice for setting up a Qualtrics questionnaire, and will help you understand how the questionnaire you build will correspond to the dataset you get at the end.\nIf you are in a live workshop or already have data to work with, jump down to Setup to get started with the data portion."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#qualtrics",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#qualtrics",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Qualtrics",
    "text": "Qualtrics\nQualtrics is a survey-building tool very commonly used for questionnaire-type studies, as well as some experimental work. The University of Sussex has an institutional licence for Qualtrics, so all staff and students can log in with their Sussex details and easily construct and collaborate on surveys.\nFor help using Qualtrics itself, the Qualtrics support pages are generally excellent. This tutorial will only briefly touch on the options within Qualtrics itself.\nOnce the study is complete and responses have been collected, you will need to export your data from Qualtrics so that you can analyse it. Qualtrics offers a variety of export data types, including our familiar CSV type. However, we’re going to instead explore a new option: SAV data.\n\nSAV Data\nThe .sav file type is associated with SPSS, a widely used statistical analysis programme. So, why are we using SPSS files when working in R?\nImporting via .sav has two key advantages. First, it results in a much cleaner import format. If you try importing the same data via .csv file, you’ll find that you need to do some very fiddly and pointless cleanup first. For instance, the .csv version of the same dataset will introduce some empty rows that have to be deleted with dplyr::slice() or similar. The .sav version of the dataset doesn’t have any comparable formatting issues.\nMost importantly, however, importing .sav file types into R with particular packages like {haven} gets us a dataset with a special type of data: namely, labelled data. The labels allow us to preserve important information about the questions asked and response options in Qualtrics, and to (mostly) painlessly create codebooks for datasets. We will explore these features in depth in this tutorial.\n\n\nSetting Up Qualtrics\n\n\n\n\n\n\nImportant\n\n\n\nThe following section is most useful when you are creating your Qualtrics questionnaire. If you are just starting your study, you’re recommended to read this section in full.\nIf you already have a Qualtrics questionnaire, be very careful about editing it after data collection has begun. Minimally, if you do decide you want to make changes, export copies of both your dataset and your questionnaire before you make any edits.\nIf you have a dataset in Qualtrics, jump down to exporting data.\nIf you already have data in .sav format to work with, jump down to the next section.\n\n\nIn this section we’ll have a quick look at how to set up Qualtrics to work as smoothly as possible with R. This has also previously been covered in a QQM Skills Lab.\n\nUsing Blocks\nBlocks are the way that Qualtrics organises pieces of the survey. Essentially, everything in the same block becomes a unit. You can have multiple questions per block, or just one. Blocks are vital for creating a study that appears as you want, but they won’t have any substantial impact on the format of the data.\nExplaining blocks and how they can be arranged is a bit outside the scope of this tutorial, so see the Block Options page in the Qualtrics guide for more details.\n\n\nUsing Questions\nThe core of Qualtrics are questions, which you can create within blocks. By default, a new question is a multiple-choice question (MCQ), but you can customise this in depth in the “Edit question” sidebar to the left of the survey. To edit a question, you have to click on each question, which will outline the question in a blue box; you can then change the settings for that question in the sidebar.\n\n\n\n\n\n\nTip\n\n\n\nFor extensive help on creating and work with questions, see the Qualtrics Guide.\n\n\n\nQuestions in R\nLet’s have a look at the default question, which appears like this:\n\nAs you can see here, the way that you set up your questions translates directly into the way your dataset will appear.\n\nNames: All questions are automatically given a name, by default Q[number], e.g. Q1, Q2, etc. This question name will appear as the variable name in your exported dataset. These names are not visible to your participants.\nText: Question text is the actual question that your participants see. This question text will appear as the variable label in your exported dataset.\nChoices: For questions with a specific set of choices, like multiple-choice questions and rating scales, the choices you list here are the response options that your participants see. These choices will appear as the value labels in your exported dataset.\n\nYou may notice that there’s no evidence of the underlying numerical values for each choice. Although Qualtrics doesn’t make this immediately obvious, they are always worth checking, because sometimes they’re…creative. This doesn’t matter so much for questions that are going to become factors - whether the underlying number is 1 or 14 or 73 doesn’t matter because they’re just a marker for a unique category. However, we’ll see in a moment an example where it does matter, namely rating scales.\nTo check the values, click on the question, scroll down to the bottom of the Edit Question sidebar, and click on “x -&gt; Recode values”. This opens a new pop-up window where you can edit a few options:\n\nTick Recode Values to change the numeric values for each choice. These values are the underlying values that will appear as numbers in the dataset in R.\nTick Variable Naming to give different value labels to the choices than the ones the participants see. (Personally I’d be very wary of doing this, as it would be easy to lose track of what participants actually saw/responded to!)\n\n\nAs you can see from this simple “What’s your favourite pie?” question, these underlying numeric values can go wonky quickly. I have four options, “apple”, “cherry”, “pecan”, and “pumpkin”, which are numbered 1, 6, 2, and 3 respectively! What’s happened is that I created “apple”, “pecan”, and “pumpkin”, and then a couple other options; then I changed my mind, removed the other options (which would have been 4 and 5) and added “cherry” after “apple”. Values are assigned based on the order they are added, which is why the values came out weird and out of numerical order. If I wanted these to go in order (which isn’t a bad idea, since you want your data to be predictable), I can tick “Recode Values” and then manually enter the numeric values I want for each choice.\n\n\nMatrix Questions\nMatrix questions are very commonly used as an efficient way to present multiple questions or items with the same response scale - for example, items on a scale or subscale with a consistent Likert response scale.\nTo create one, create a “Matrix table” type question. The typical setup is for the items/questions to be presented down the left-hand side as “statements”, and the rating scale to be presented along the top as “scale points”.\nThe “Scale points” section of the Edit Question sidebar lets you control how these scale points appear. You can add or remove the number of points, and for many scales in Psychology, you can use suggested rating scales by switching the toggle on, which automatically insert labels for each scale point for you.\nMatrix tables are especially prone to issues with the underlying numeric values, especially if you use these automatic scale points. You’ll end up with really weird ranges, like 61-65, instead of 1-5, which will do a number on the interpretation of any descriptives. Even better, the numeric values change themselves every time you make changes to them! So, I’d strongly recommend you update the numeric values using “Recode values” as the last step to make sure you don’t have any surprises when you get round to looking at the data.\n\n\n\n\nExporting Data\nIf you’d like to work with your own study data, you will need to export your data in SAV format from Qualtrics first. To do this, open your Qualtrics survey and select the “Data & Analysis” tab along the top, just under the name of your survey.\nIn the Data Table view, look to the right-hand side of the screen. Click on the drop-down menu labelled “Export & Import”, then select the first option, “Export Data…”\n\nIn the “Download a data table” menu, choose “SPSS” from the choices along the top. Make sure “Download all fields” is ticked, then click “Download”.\n\nThe dataset will download automatically to your computer’s Downloads folder. From there, you should rename it to something sensible and move it into a data folder within your project folder. From there, you can read it in using the here::here() %&gt;% haven::read_sav() combo that we will seee in the Data section in just a moment.\n\n\n\n\n\n\nSensible Naming Conventions and Folder Structure\n\n\n\n\n\nSensible file and folder names will make your life so much easier for working in R (and generally).\nFor folder structure, make sure you do the following:\n\nAlways always ALWAYS use an R Project for working in R.\nHave a consistent set of folders for each project: for example, images, data, and docs.\nUse sub-folders where necessary, but consider using sensible naming conventions instead.\n\nFor naming conventions, your file name should make it obvious what information it contains and when it was created, especially for datasets like this. I recommend longer and more explicit file names over brevity.\nSo, for a download like this, I’d name it something like qtrics_diss_2024_03_20.sav. The qtrics tells me it’s a Qualtrics export, the diss tells me it’s a dissertation project, and the last bit is the full date in easily machine-readable format. Imagine if I continue to recruit participants and download a new dataset later, say a month from now, and name it qtrics_diss_2024_04_20.sav. I could easily distinguish which dataset was which by the date, but also see that they are different versions of the same thing by their shared prefix.\nThis is a much more reliable system than calling them, say, Qualtrics output.sav and Dissertation FINAL REAL.sav. This kind of naming “convention” contains no information about which is which or when they were exported, or even that they’re two versions of the same study dataset! Future You trying to figure out which dataset to use weeks or months later will feel the difference."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#setup",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#setup",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Setup",
    "text": "Setup\nThe rest of this tutorial walks you through the basics of importing, inspecting, cleaning, and converting your Qualtrics data, including automatically generating a data dictionary for reference. Data is provided to practice with in workshops, but you are welcome to follow along with your own data if you prefer.\n\nPackages\nWe will need the following packages:\n\n{tidyverse} for data wrangling.\n{haven} for importing data. This package is installed with {tidyverse} but not loaded with the core packages so needs to be loaded separately.\n{labelled} for working with labelled data.\n{sjPlot} for a data dictionary convenience function\n\n\n\n\n\n\n\nExercise\n\n\n\nLoad the packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(sjPlot)\n\n\n\n\n\n\n\n\nData\nToday’s example dataset focuses on various aspects of meaning in life (MiL), and has been randomly generated based on a real dataset kindly contributed by Hanna Eldarwish and Vlad Costin. All variables have been randomly generated, but they are based on the patterns in the original dataset. The original, bigger dataset will be made available alongside article publication in the future, so keep an eye out for it!\n\n\n\n\n\n\nNew File Type\n\n\n\nYou might notice that instead of the familiar readr::read_csv(), today we have haven::read_sav(). We need a different function since we are using a different type of data. See the section above on .sav data for more details.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRead in the mil_data.sav object from folder, or alternatively from Github via URL, as you prefer.\nOn the Cloud, you can read in this dataset from the data folder using here::here().\nElsewhere, you can download the dataset, or copy the dataset URL, from the Data and Workbooks page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn the Cloud:\n\nmil_data &lt;- here::here(\"data/mil_data.sav\") %&gt;% haven::read_sav()\n\nFrom a folder:\n\nmil_data &lt;- here::here(\"data/mil_data_wkshp.sav\") %&gt;% haven::read_sav()\n\nFrom URL:\n\nmil_data &lt;- haven::read_sav(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/mil_data_wkshp.sav\")\n\n\n\n\n\n\n\n\nCodebook\nThis codebook is intentionally sparse, because we’ll be generating our own from the dataset in just a moment. This table covers only the demographic and questionnaire measures to help you understand the variables.\n\n\n\n\n\n\nCodebook\n\n\n\n\n\n\n\n\n\n\n\nVariable\nItem/Scale: Subscale\n\n\n\n\nQ1\nHow well can you speak English?\n\n\nQ2\nHow old are you?\n\n\nQ3\nWhat is your gender identity?\n\n\nQ4\nWhat is your annual income?\n\n\nQ5\nWhat is your occupation?\n\n\nQ6_1\nMeaning in Life: Global Meaning (item 1)\n\n\nQ6_2\nMeaning in Life: Global Meaning (item 2)\n\n\nQ6_3\nMeaning in Life: Global Meaning (item 3)\n\n\nQ6_4\nMeaning in Life: Global Meaning (item 4)\n\n\nQ7_1\nMeaning in Life: Mattering (item 1)\n\n\nQ7_2\nMeaning in Life: Mattering (item 2)\n\n\nQ7_3\nMeaning in Life: Mattering (item 3)\n\n\nQ7_4\nMeaning in Life: Mattering (item 4)\n\n\nQ8_1\nMeaning in Life: Coherence (item 1)\n\n\nQ8_2\nMeaning in Life: Coherence (item 2)\n\n\nQ8_3\nMeaning in Life: Coherence (item 3)\n\n\nQ8_4\nMeaning in Life: Coherence (item 4)\n\n\nQ9_1\nMeaning in Life: Purpose (item 1)\n\n\nQ9_2\nMeaning in Life: Purpose (item 2)\n\n\nQ9_3\nMeaning in Life: Purpose (item 3)\n\n\nQ9_4\nMeaning in Life: Purpose (item 4)\n\n\nQ10_1\nSymbolic Immortality (item 1)\n\n\nQ10_2\nSymbolic Immortality (item 2)\n\n\nQ11_1\nBelonging (item 1)\n\n\nQ11_2\nBelonging (item 2)\n\n\nQ11_3\nBelonging (item 3)\n\n\nQ11_4\nBelonging (item 4)\n\n\nQ11_5\nBelonging (item 5)\n\n\nQ11_6\nBelonging (item 6)\n\n\nQ11_7\nBelonging (item 7)\n\n\nQ11_8\nBelonging (item 8)\n\n\nQ11_9\nBelonging (item 9)\n\n\nQ11_10\nBelonging (item 10)\n\n\nQ11_11\nBelonging (item 11)\n\n\nQ11_12\nBelonging (item 12)\n\n\n\n\n\n\n\n\n\n\nFor easy navigation, jump to: Renaming, Exercises: Names"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#variable-names",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#variable-names",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Variable Names",
    "text": "Variable Names\nQualtrics datasets are often large and unwieldy. However, they also often have a consistent structure, which we can take advantage of to work with them consistently.\n\nDefault Variable Names\nIn your dataset, you will by default have some variables that are automatically created by Qualtrics, with (somewhat) sensible names, like DistributionChannel and StartDate. You will also have all the questions that you created, and what they are called depends on what you (or, rather, the author of the questionnaire) called them.\nIf you changed the name of the questions, they will have the name that you gave them. If not, they will have a default name from Qualtrics, usually the capital letter “Q” followed by a number, like this: Q15, Q34, etc.\nIf you have matrix questions, the variable names will have a further number indicating which item in the matrix they correspond to. If, for example, your matrix question was Q23, then the responses to the first item in that matrix will be stored in Q23_1, the second in Q23_2, and so on.\nThese default variable names should be changed as a first step, before you carry on with your data processing. This is because they are easy to mix up or mistype, and difficult to remember (was it Q23 or Q32 that contained the question I wanted…?), which will lead to both unnecessary errors and extra time spent fixing problems or cross-checking which question is which.\nTo do this, we’ll get round to the dplyr::rename() function by way of a detour revising dplyr::select().\n\n\nSelecting\nYou have already encountered the function dplyr::select() as a function for keeping or dropping columns in a dataset. As a reminder, there’s some easy notation to use for variable names to quickly select single variables or ranges, or to drop variables.\n\n1dataset_name %&gt;%\n2  dplyr::select(\n3    variable_to_include,\n4    -variable_to_exclude,\n5    keep_this_one:through_this_one,\n6    new_name = variable_to_rename,\n7    variable_number\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n2\n\nSelect the following variables:\n\n3\n\nThe name of a variable to be included in the output. Multiple variables can be selected separated by commas.\n\n4\n\nThe name of a variable to be excluded from the output. Use either an exclamation mark (!) or a minus sign (-) in front of each variable to exclude. Multiple variables can be dropped, separated by commas with a ! (or -) before each.\n\n5\n\nA range of variables to include in the output. All the variables between and including the two named will be selected (or dropped, with !(drop_this_one:through_this_one)).\n\n6\n\nInclude variable_to_rename in the output, but call it new_name.\n\n7\n\nInclude a variable in the output by where it appears in the dataset, numbered left to right. For example, “2” will select the second column in the original dataset.\n\n\n\n\nColumns will appear in the output in the order they are selected in select(), so this function can also be used to reorder columns.\n\nSelection Helpers\nHowever, the real power in this and other {tidyverse} functions is in a system of helper functions and notations collectively called “selection helpers”, or &lt;tidyselect&gt;. The overall goal of “&lt;tidyselect&gt; semantics” (as you will see it referred to in help documentation) is to make selecting variables easy, efficient, and clear.\nThese helper functions can be combined with the selection methods above in any combination. Some very convenient options include:\n\neverything() for all columns\nlast_col() for the last column in the dataset\nstarts_with(), ends_with(), and contains() for selecting columns by shared name elements, which will be our key focus today.\nwhere() for selecting with a function, not described here (see ?where() for more)\n\nFor example, we can select specific groups of variables using the shared portions of their names, such as:\n\n“Date” for the three default Qualtrics variables containing date information\n“Q8” for the four Coherence items\n\n\nmil_data %&gt;% \n  dplyr::select(\n    contains(\"Date\"), starts_with(\"Q8\")\n  )\n\n\n\n  \n\n\n\n\n\n\nRenaming\nNow that we know how to easily select groups of variables, we need sensible names in order to make best use of those selection helpers. There are three main options for renaming variables, depending on access to the original Qualtrics questionnaire, and proficiency in R.\n\n\n\n\n\n\nImportant\n\n\n\nYou are strongly advised not to manually change the names in your dataset, e.g. in a .csv file/Excel. Not only will you lose the labels, but this is very prone to error with no record of the changes made.\n\n\n\nOption 1: Rename in Qualtrics\nThis option requires that you have have access to, and are willing to edit, the original Qualtrics questionnaire. Rather than being a coding option, this entails going back to the Qualtrics questionnaire and changing the question labels before you export the dataset.\nFor more on this, see Setting Up Qualtrics.\n\n\nOption 2: rename()\nThe friendly dplyr::rename() function does exactly what it says on the tin. In general:\n\n1dataset_name %&gt;%\n2  dplyr::rename(\n3    new_name = old_name\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n2\n\nRename the following variables:\n\n3\n\nThe new name (new_name) you would like to give to an existing variable (old_name).\n\n\n\n\nYou can list as many of these new_name = old_name pairs as you like. For example, let’s rename the Global Meaning items so they have sensible prefixes (refer to the Codebook for which variables these are!). We should keep the item numbers as they are, so we know which one is which.\n\nmil_data %&gt;% \n  dplyr::rename(\n    meaning_1 = Q6_1,\n    meaning_2 = Q6_2,\n    meaning_3 = Q6_3,\n    meaning_4 = Q6_4,\n  )\n\n\n\n  \n\n\n\nThis option allows you to easily keep track of the renaming you’ve done in your code, but it is very tedious and intensive, especially if you have many variables that need renaming.\n\n\nOption 3: rename_with()\nThis option requires considerable proficiency and experience with R. It is by far the quickest and most efficient of these options, but you must be able to write anonymous functions, use regular expressions and selection helpers, and have good working knowledge of how to debug errors and check output. If any of those things are unfamiliar, use one of the two previous options instead.\n\n\n\n\n\n\nHaRd Mode: Using rename_with()\n\n\n\n\n\nThe versatile dplyr::rename_with() function allows quick, efficient, and accurate renaming of large groups of variables at once. The general form is:\n\ndataset_name %&gt;%\n  dplyr::rename_with(\n     .fn = function_to_apply,\n     .cols = variables_to_rename\n  )\n\n\n\n\n\n\nThe “function to apply” here could be simply the name of an existing function, for example tolower (convert to lowercase). You can also write a “purrr-style lambda” function, which will allow you to write your own custom function to change the variable names however you please.\nAs an example, let’s convert the Q11 variables in the dataset at once. We know from the codebook that these are all items on the Belonging subscale, so we want to replace the string “Q11” in the variable names to “belonging”.\n\n1mil_data %&gt;%\n  dplyr::rename_with(\n2    .fn = ~ gsub(\"Q11\", \"belonging\", .x),\n3    .cols = dplyr::starts_with(\"Q11\")\n  )\n\n\n1\n\nTake the mil_data dataset and then rename variables as follows\n\n2\n\nReplace every instance of the string “Q11” with the string “belonging”\n\n3\n\nDo this for every column that currently starts with the string “Q11”\n\n\n\n\n\n\n  \n\n\n\nIn this command, our “purrr-style lambda” is the anonymous function ~ gsub(\"Q23\", \"belonging\", .x). The ~ (apparently pronounced “twiddle”) at the beginning is a shortcut for the longer function(x) ... notation for creating functions. The .x is a placeholder for each of the variables that the function will be applied to. These are both used in a customised version of the base-R gsub() function, which generally substitutes every match with its first argument with the replacement in its second argument for the vector of possibilities in its third argument; see ?gsub() for details.\nAs you can see from the output, this only replaces the relevant portion of the column name, leaving the numbered item suffixes unchanged. If you are proficient in working with regular expressions and string manipulation, you can use this technique to programmatically rename variables very easily."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#exercises-names",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#exercises-names",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Exercises: Names",
    "text": "Exercises: Names\nBefore we go on, it’s time to get the variables in this dataset sorted out. You must do this, or the solutions further on in the document won’t work!\n\n\n\n\n\n\nExercise\n\n\n\nClean up your dataset by doing the following. You can do the steps in whatever order works for you.\n\nKeep all the demographic questions, items measuring Global Meaning, and Mattering, and all the Belonging items.\nRename any default-named Qualtrics variables (starting with “Q”) to a sensible name.\n\nRefer to the Codebook to figure out which variables are which.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can accomplish this task in either order:\n\nFirst, select the variables you want, then rename them.\nSecond, rename the variables, then select them.\n\nThis solution will give answers in the order of the sections above, so first select and then rename. However, consider that if you need to go back and change your selection later, this will be easier if the variables are named something sensible, so it’s worth considering renaming first, before you do anything else, for your own data.\nBoth tasks could be done in one pipeline, but to break it down, in this first chunk we have selected the range of demographic variables, and the Global Meaning and Mattering scale items by using the the colon between Q1 and Q7_4, and have selected the Belonging scale items which all start with 'Q11'.\n\nmil_data &lt;- mil_data %&gt;% \n  dplyr::select(\n    Q1:Q7_4,\n    starts_with('Q11')\n    ) \n\nFor the second task, we need to decide on “sensible” names. You can choose anything that makes sense to you, but we will use global_meaning for Q6, mattering for Q7, and belonging for Q11.\nIn our second pipeline, we’re using rename() to replace the names of each variable individually. If you did this yourself before looking at the solution, you will likely have found this to be a laborious, tedious, and error-prone process, so for your own data, make sure you allow time to both do and check this code.\n\nmil_data &lt;- mil_data %&gt;% \n  dplyr::rename(\n    english_fluency = Q1, \n    age = Q2,\n    gender = Q3,\n    income = Q4,\n    occupation = Q5,\n    global_meaning_1 = Q6_1,\n    global_meaning_2 = Q6_2,\n    global_meaning_3 = Q6_3,\n    global_meaning_4 = Q6_4,\n    mattering_1 = Q7_1,\n    mattering_2 = Q7_2,\n    mattering_3 = Q7_3,\n    mattering_4 = Q7_4,\n    belonging_1 = Q11_1,\n    belonging_2 = Q11_2,\n    belonging_3 = Q11_3,\n    belonging_4 = Q11_4,\n    belonging_5 = Q11_5,\n    belonging_6 = Q11_6,\n    belonging_7 = Q11_7,\n    belonging_8 = Q11_8,\n    belonging_9 = Q11_9,\n    belonging_10 = Q11_10,\n    belonging_11 = Q11_11,\n    belonging_12 = Q11_12\n  )\n\nAlternatively, if you ventured into Option 3 for renaming above, you could instead use rename_with() to rename all the items starting with 'Q6' to have the prefix of 'global_meaning', all the items starting with 'Q7' to have the prefix of 'mattering', and all the items starting with 'Q11' to have the prefix of 'belonging'.\n\nmil_data &lt;- mil_data %&gt;% \n  dplyr::rename(\n    english_fluency = Q1, \n    age = Q2,\n    gender = Q3,\n    income = Q4,\n    occupation = Q5 \n  ) %&gt;% \n  dplyr::rename_with( \n    .fn = ~ gsub(\"Q6\", \"global_meaning\", .x),  \n    .cols = dplyr::starts_with(\"Q6\")\n  ) %&gt;% \n  dplyr::rename_with( \n    .fn = ~ gsub(\"Q7\", \"mattering\", .x),  \n    .cols = dplyr::starts_with(\"Q7\")\n  ) %&gt;% \n  dplyr::rename_with( \n    .fn = ~ gsub(\"Q11\", \"belonging\", .x),  \n    .cols = dplyr::starts_with(\"Q11\") \n  )"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#labelled-data",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#labelled-data",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Labelled Data",
    "text": "Labelled Data\nWith the minimal necessary cleaning out of the way, we can now move on to exploring labelled data.\n\n\n\n\n\n\nThe Plan\n\n\n\nOur workflow for this dataset will be slightly different than you may have encountered before.\nWe’ll start by checking the labels and producing a codebook, or “data dictionary”, drawing on the label metadata in the SAV file. For the purpose of practice, we’ll also have a look at how to work with those labels, and optionally manage different types of missing values.\nAs useful as labels are, they will get in the way when we want to work with our dataset further. So, we’ll next convert the variables in the dataset into either factors, for categorical data, or numeric, for continuous data 1. From that point forward, we can work with the dataset using the techniques and functions we’ve covered throughout first and second year.\n\n\n\nWorking with Labels\nThe SAV data we’re using has a special property: labels. Labelled data has a number of features, which we will explore in depth shortly:\n\nVariable labels. The label associated with a whole variable will contain the text of the item that the participants responded to. This is analogous to the “Label” column of the Variable View in SPSS.\nValue labels. The label associated with individual values within a variable will contain the text associated with individual choices, for instance the points on a Likert scale or the options on a multiple-choice question. This is analogous to the “Values” column of the Variable View in SPSS.\nMissing values. Within value labels, you can designate particular values as indicative of missing responses, refusal to respond, etc. This is analogous to the “Missing” column of the Variable View in SPSS.\n\nWe’re first going to look at how you can work with each of these elements. The reason to do this is that once our dataset has been thoroughly checked, we’re going to generate a final data dictionary, then convert any categorical variables into factors, the levels of which will correspond to the labels for that variable. We’ll also convert any numeric variables into numeric data type, which will discard the labels; that will make it possible to do analyses with them, but that’s why we have to create the data dictionary first.\nMost of the following examples are drawn from the “Introduction to labelled” vignette from the {labelled} package. If you want to do something with labelled data that isn’t covered here, that’s a good place to start!\n\n\n\n\n\n\nImportant\n\n\n\nThese features will work optimally only if you have set up your Qualtrics questionnaire appropriately. Make sure to refer to the Setting Up Qualtrics section to get the most out of your labelled data and save yourself data cleaning and wrangling headaches later.\n\n\n\n\nVariable Labels\nVariable labels contain information about the whole variable, and for Qualtrics data, will by default contain either an automatically generated Qualtrics value (like “Start Date”), or the question text that that variable contains the responses to.\n\nGetting Labels\nTo begin, let’s just get out a single variable label to work with using labelled::var_label().\nTo specify the variable we want, we will need to subset it from the dataset, using either $ or dplyr::pull().\n\nlabelled::var_label(mil_data$gender)\n\n[1] \"What is your gender identity?  This question is optional. - Selected Choice\"\n\n\n\n\nCreating/Updating Labels\nIf you’d like to edit labels, you can do it “manually” - that is, just writing a whole new label from scratch.\nThe structure of the following code might look a little unfamiliar. For the most part, we’ve seen code that contains longer and more complex instructions on the right-hand side of the &lt;-, and a single object being created or updated on the left-hand side. In the structure below, the left-hand side contains longer and more complex code that identifies the value(s) to be updated or created, and the right-hand side contains the value(s) to create or update. It’s the same logic, just with a different structure.\n\nlabelled::var_label(mil_data$StartDate) &lt;- \"Date and time questionnaire was started\"\n\nlabelled::var_label(mil_data$StartDate)\n\n\n\n[1] \"Date and time questionnaire was started\"\n\n\n\n\n\n\n\n\nHaRd Mode: Using Regular Expressions\n\n\n\n\n\nRegular expressions are the magic of working with code. They are also fiddly, confusing, and difficult. If you’re not keen on spending a lot of time learning what is in essence a new mini-language, skip this section!\nEditing labels is a good opportunity to start working with regular expressions. For example, if we want to keep only the first bit of the label for gender, then we can keep everything only up to and including the question mark, and re-assign that to the variable label. This style is a bit more dynamic and resilient to changes or updates.\n\nlabelled::var_label(mil_data$gender) &lt;- labelled::var_label(mil_data$gender) %&gt;%\n  gsub(\"(.*\\\\?).*\", \"\\\\1\", x = .)\n\nlabelled::var_label(mil_data$gender)\n\n\n\n[1] \"What is your gender identity?\"\n\n\nLet’s pick apart this gsub() command a bit at a time. First, gsub() has three arguments:\n\npattern, here \"(.*\\\\?).*\", which is the regex statement representing the string to match.\nreplacement, here \"\\\\1\", which is the string that should replace the match in pattern.\nx, the string to look in.\n\nThe pattern has essentially two parts: the bit in the rounded brackets, and the bit outside. The rounded brackets designate a “capturing group” - a portion of the string that should be grouped together as a unit. The benefit of this grouping is in the second argument of gsub(); \\\\1 isn’t the number 1, but rather is a pronoun referring to the first capturing group. In other words, as a whole, this gsub() command captures a subset of the incoming string, and then replaces the entire string with that captured string, essentially dropping everything outside the capturing group.\nTo understand the regex statement \"(.*\\\\?).*\", we need to look at the incoming text, x. In this case, x is being piped in from above and looks like this:\n\nlabelled::var_label(mil_data$gender)\n\n[1] \"What is your gender identity?  This question is optional. - Selected Choice\"\n\n\n.* is a common regex shorthand that means “match any character, as many times as possible.” It’s essentially an “any number of anything” wildcard. This wildcard appears both inside and outside the brackets. So, how does gsub() know which bit should belong in the capturing group?\nThe answer is \\\\?. This is a “literal” question mark. Some symbols, like . and ?, are regex operators, but we might want to also match the “literal” symbols full-stop “.” and question mark “?” in a string. In this case we need an “escape” character “\\\", that escapes regex and turns the symbol into a literal one. So, the capturing group ends with a literal question mark - in the target string, that’s the question mark after”identity”, which is the only one in the string.\nAs an aside, if you’re wondering why there are two escape characters instead of one - i.e., why is it \\\\? and not \\?, well, you and me both. There’s an explanation in vignette(\"regular-expressions\") that never completely makes sense to me. Also, this seems to be an R thing - regex outside of R seems to use only a single escape character, so a literal question mark would be \\?. If you are ever trying to adapt regex from e.g. StackOverflow or regex101 and it isn’t working, check whether the escape characters are right!\nAnyway. We can now read \"(.*\\\\?)\" as “capture all characters up to and including a literal question mark” - which matches the substring “What is your gender identity?” in x. However, we don’t just want to replace that portion of the string - instead, we want to replace the whole string with that bit of it. So, the second .* outside the brackets matches the rest of the string. If we didn’t include this last bit, the capturing group would just be replaced with itself, which would result in the same string as we started with, as below:\n\nlabelled::var_label(mil_data$gender) %&gt;%\n  gsub(\"(.*\\\\?)\", \"\\\\1\", x = .)\n\n[1] \"What is your gender identity?  This question is optional. - Selected Choice\"\n\n\nSo, altogether, we can read this gsub() command as: “Capture everything up to an including the question mark, and replace the entire string with that capturing group.”\nNow. Why, you might wonder, is all this faff better?\nWell, it might not be. You might find it more frustrating or effortful to generate the right regex pattern than to replace the label “manually”, and in that case, there’s nothing wrong with just writing out the label you want.\nOn the other hand, the regex command will always drop everything after the question mark, no matter what that text is. If there is no match, it won’t replace anything. So, unlike the “manual” option, there’s much less danger of accidentally mixing up labels or overwriting the wrong thing; and this regex statement can be generalised to any label that contains a question mark, rather than having to type out each label one by one.\n\n\n\n\n\nSearching Labels\nA very nifty feature of variable labels and {labelled} is the ability to search through them with labelled::look_for(). With the whole dataset, look_for() returns a whole codebook (see Data Dictionaries below for more on this), but given a second argument containing a search term, you get back only the variables whose label contains that term.\nFor example, we can use labelled::look_for() to get only the items in this questionnaire that mentioned family. (I’ve piped into tibble::as_tibble() to make the output easier to read.)\n\nlabelled::look_for(mil_data, \"family\") %&gt;%\n  tibble::as_tibble()\n\n\n\n  \n\n\n\n\n\n\nValue Labels\nValue labels contain individual labels associated with unique values within a variable. It’s not necessary to have a label for every value, but for our purposes, it’s important that all values that represent categories have a label.\n\nGetting Labels\nThere are two functions to assist with this. labelled::val_labels() (with an “s”) returns all of the labels, while labelled::val_label() (without an “s”) will return the label for a single specified value.\n\nlabelled::val_labels(mil_data$english_fluency)\n\n Very well       Well   Not well Not at all \n         1          2          3          4 \n\n\n\nlabelled::val_label(mil_data$english_fluency, 3)\n\n[1] \"Not well\"\n\n\n\n\nCreating/Updating Labels\nThese two functions can also be used to update an entire variable or a single value respectively. The structure of this code is the same as we saw with variable labels previously.\nFor example, let’s get all the value labels for the gender variable, then update the last value to “Other”.\nFirst, return the existing labels:\n\nlabelled::val_labels(mil_data$gender)\n\n                      Male                     Female \n                         0                          1 \n                Non-binary Other (please state below) \n                         2                          3 \n\n\nThen, replace the label associated with the value 3:\n\nlabelled::val_label(mil_data$gender, 3) &lt;- \"Other\"\n\n\n\n\nMissing Values\nThis section is included especially for people who may have previous experience with SPSS, and are learning how to adapt their SPSS knowledge to R. Unless you make regular use of SPSS’s alternative options for managing missing values, you can skip this section.\n\n\n\n\n\n\nHaRd Mode: Missing Values\n\n\n\n\n\nLabelled data allows an extra functionality from SPSS, namely to create user-defined “missing” values. These missing values aren’t actually missing, in the sense that the participant didn’t respond at all. Rather, they might be missing in the sense that a participant selected an option like “don’t know”, “doesn’t apply”, “prefer not to say”, etc.\nLet’s look at an example. As we’ve just seen, we can get out all the value labels in variable with labelled::val_labels():\n\nlabelled::val_labels(mil_data$english_fluency)\n\n Very well       Well   Not well Not at all \n         1          2          3          4 \n\n\nThis variable asked participants to indicate their level of English fluency. Even for participants who have in fact responded to this question, we may want to code “Not well” and “Not as all” as “missing” so that they can be excluded easily. To do this, we can use the function labelled::na_values() to indicate which values should be considered as missing.\n\nlabelled::na_values(mil_data$english_fluency) &lt;- 3:4\n\nmil_data$english_fluency\n\n&lt;labelled_spss&lt;double&gt;[164]&gt;: Please select which box best describes your English fluency. - How well can you speak English?\n  [1] 2 2 2 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 2 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 2 2\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 3 2\nMissing values: 3, 4\n\nLabels:\n value      label\n     1  Very well\n     2       Well\n     3   Not well\n     4 Not at all\n\n\nFor the moment, these values are not actually NA in the data - they’re listed under “Missing Values” in the variable attributes. In other words, the actual responses are still retained. However, if we ask R which of the values in this variable are missing…\n\nis.na(mil_data$english_fluency)\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n…we can see one TRUE corresponding to the 3 above.\nIf we wanted to actually remove those values entirely and turn them into NAs for real, we could use labelled::user_na_to_na() for that purpose. Now, the variable has only two remaining values, and any 3s and 4s have been replaced.\n\nlabelled::user_na_to_na(mil_data$english_fluency)\n\n&lt;labelled&lt;double&gt;[164]&gt;: Please select which box best describes your English fluency. - How well can you speak English?\n  [1]  2  2  2  1  1  1  2  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [26]  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [51]  1  1  2  1  1  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  2  1  1  1  1\n [76]  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1\n[101]  1  1  2  1  1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[126]  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  1  1  1  1  1  1  2  1  1  1  1 NA  2\n\nLabels:\n value     label\n     1 Very well\n     2      Well\n\n\n\n\n\n\n\n\nTip\n\n\n\nSee the {labelled} vignette for more help on working with user-defined NAs, including how to deal with them when converting to other types."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#exercises-labels",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#exercises-labels",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Exercises: Labels",
    "text": "Exercises: Labels\nThe following exercises will help you get some hands-on practice with working with labels. You’re strongly recommended to try them yourself before you carry on.\n\n\n\n\n\n\nExercise\n\n\n\nIdentify the item that mentions ‘job’. Then, change the variable label of this item so that it just says ‘Occupational Status’\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Find the variable\nlabelled::look_for(mil_data, \"job\") %&gt;%\n  tibble::as_tibble()\n\n\n\n  \n\n\n## Update the label\nlabelled::var_label(mil_data$occupation) &lt;- \"Occupational Status\"\n\n## Check the new label\nlabelled::var_label(mil_data$occupation)\n\n[1] \"Occupational Status\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFor the income variable, change the value label ‘I prefer not to disclose information about my annual income as part of this research study.’ to ‘Prefer not to say.’\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## View current labels\nlabelled::val_labels(mil_data$income)\n\n                                                                          Less than £15,000 \n                                                                                          1 \n                                                                          £15,000 - £29,999 \n                                                                                          2 \n                                                                          £30,000 - £44,999 \n                                                                                          3 \n                                                                          £45,000 - £59,999 \n                                                                                          4 \n                                                                          £60,000 - £74,999 \n                                                                                          5 \n                                                                          £75,000 - £89,999 \n                                                                                          6 \n                                                                          More than £90,000 \n                                                                                          7 \nI prefer not to disclose information about my annual income as part of this research study. \n                                                                                          8 \n\n\n\n## Replace the correct value\nlabelled::val_label(mil_data$income, 8) &lt;- \"Prefer not to say.\"\n\n## Check this has been done correctly\nlabelled::val_labels(mil_data$income)\n\n Less than £15,000  £15,000 - £29,999  £30,000 - £44,999  £45,000 - £59,999 \n                 1                  2                  3                  4 \n £60,000 - £74,999  £75,000 - £89,999  More than £90,000 Prefer not to say. \n                 5                  6                  7                  8"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#data-dictionaries",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#data-dictionaries",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Data Dictionaries",
    "text": "Data Dictionaries\nOnce our labels have been cleaned and updated, we can finally produce a data dictionary for this dataset.\n\n\n\n\n\n\nWhy a data dictionary?\n\n\n\n\n\nThere’s two key reasons to produce a data dictionary for your dataset.\nFirst, data dictionaries (or “codebooks”) are very useful for understanding datasets, even your own. You may find yourself referring to it frequently when writing your methods and results, to remind yourself what different questions contain, what the text of the question was, etc.\nSecond, data dictionaries are hugely useful for other people. This would be a massive help to, for example, your supervisor who may need to assist you with your data analysis, or to include in your dissertation submission for your markers. If you want to share your data publicly, including a dictionary/codebook is not only a kindness to other users but also helps prevent misuse or misunderstandings.\n\n\n\nIf you primarily need a quick reference as you’re working with your dataset, the delightful sjPlot::view_df() function makes this particularly easy.\nLet’s put mil_data into the sjPlot::view_df() function and see what it does. By default, the document opens in the Viewer, but you can also save the file it creates for further sharing - see the help documentation.\n\nsjPlot::view_df(mil_data)\n\nIf you’re happy with this, this is probably all you need to carry on. If you are keen to create your data dictionary as a dataset that you could further edit - or if you’d like a version of the data dictionary that more closely emulates SPSS’s Variable View - see below.\n\n\n\n\n\n\nHaRd Mode: Editable Data Dictonary\n\n\n\n\n\nUse the generate_dictionary() function from the {labelled} packages to create a data dictionary for mil_data. To have the best look at it, I would recommend using View() to review it.\n\nmil_data %&gt;%\n  labelled::generate_dictionary() %&gt;%\n  View()\n\n\n\n\n\n  \n\n\n\nUnlike the output from sjPlot::view_df(), the output from this function is a dataset that you can work with. This means you can edit it using any of your {dplyr} skills and render it as a table in a document if you like. The sky’s the limit!"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#converting-variables",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#converting-variables",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Converting Variables",
    "text": "Converting Variables\nThe labels have served their purpose helping us navigate and clean up the dataset, and produce a lovely data dictionary for sharing. However, if we want to use the data, we’ll need to convert to other data types that we can use for statistical analysis.\nHow we convert each variable will fall into two main categories:\n\nAny variables containing categorical data, we’ll convert to factors, which will use the value labels as factor levels\nAny variables containing numbers that we want to do maths with, we’ll convert to numeric, which will strip the labels.\n\n\n\n\n\n\n\nImportant\n\n\n\nVariables that will be converted to factor should have labels for all of their levels, whereas variables that will be converted to numeric can have fewer labels, because we will stop using them after the numeric conversion.\n\n\n\nFactors\nFactor variables are R’s way of representing categorical data, which have a fixed and known set of possible values.\nFactors actually contain two pieces of information for each observation: levels and labels. Levels are the (existing or possible) values that the variable contains, whereas labels are very similar to the labels we’ve just been exploring.\nIf you feel confident understanding and working with factors in R, you can skip the box below.\n\n\n\n\n\n\nRevision of Factors\n\n\n\nLet’s start by looking at an example factor to see how it appears. This isn’t in our dataset; instead, we can create factor data using the factor() function.\n\nfactor(c(1, 2, 1, 1, 2),\n       labels = c(\"Male\", \"Female\"))\n\n[1] Male   Female Male   Male   Female\nLevels: Male Female\n\n\nThe underlying values in the factor are numbers, here 1 and 2. The labels are applied to the values in ascending order of those values, so 1 becomes “Male”, “2” becomes “Female”, etc. Here, we don’t need to specify the levels; if you don’t elaborate otherwise, R will assume that they are the same as the unique values.\nYou can also supply additional possible values, even if they haven’t been observed, using the levels argument:\n\nfactor(c(1, 2, 1, 1, 1),\n       levels = c(1, 2, 3),\n       labels = c(\"Male\", \"Female\", \"Non-binary\"))\n\n[1] Male   Female Male   Male   Male  \nLevels: Male Female Non-binary\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFactors are so common and useful in R that they have a whole {tidyverse} package to themselves! You already installed {forcats} with {tidyverse}, but you can check out the help documentation if you’d like to learn more about working with factors.\n\n\n\nConverting to Factors\nLabelled data is very easy to convert into factors, which is what R expects for many different types of analysis and plotting functions. Handy!\nFor an individual variable, we can use labelled::to_factor() to convert to factor.\nFor example, we can convert the gender variable to factor as follows, using the dplyr::mutate() function to make a change to the dataset. Remember that using the same variable name as we have done here means that the existing variable will be replaced (overwritten) in the dataset.\nIf we look at only this particular variable, we can see that its data type is now &lt;fctr&gt;, which is what we wanted.\n\nmil_data %&gt;% \n  dplyr::mutate(\n    gender = labelled::to_factor(gender)\n  ) %&gt;% \n  dplyr::select(gender)\n\n\n\n  \n\n\n\nIf you wanted a specific order of the levels, for plotting or similar, there’s also a sort_levels = argument described in the help documentation for labelled::to_factor().\nThat’s actually it! Whatever the value labels are in the variable, they will be converted into factor labels. Assuming your value labels are correct, no further editing is needed.\n\n\n\nNumeric\nFor continuous variables, we don’t need anything fancy to turn them into numeric data, because they technically already are. Instead, we just need to get rid of the labels using unclass().\nAs an example, we can use unclass() to convert belonging_1 to numeric, using the dplyr::mutate() function to make a change to the dataset again.\nIf we look at only this particular variable, we can see that its data type is now &lt;dbl&gt;, which is again what we wanted.\n\nmil_data %&gt;% \n  dplyr::mutate(\n    belonging_1 = unclass(belonging_1)\n  ) %&gt;% \n  dplyr::select(belonging_1)\n\n\n\n  \n\n\n\nFrom here, you can convert variables one by one as necessary…or, for a (much!) more efficient method, read on.\n\n\nEfficient Conversion\nDepending on the size of your dataset, converting your variables one by one to either factor or numeric might range from mild inconvenience to massive undertaking. In this optional section, we will make use of what we covered previously about selection helpers in combination with a new function, dplyr::across(), to convert multiple variables at once.\nThe general form is:\n\n1dataset_name %&gt;%\n  dplyr::mutate(\n2     dplyr::across(\n3        .cols = variables_to_change,\n4        .fn = function_to_apply\n     )\n  )\n\n\n1\n\nTake the dataset dataset_name, and then make a change to it as follows\n\n2\n\nApply to…\n\n3\n\nThe variables selected to be changed\n\n4\n\nA function to apply to each of the selected variables\n\n\n\n\nIn the first .cols argument, we use &lt;tidyselect&gt; syntax (i.e. selection helpers) to choose which variables we want to change.\nIn the second argument, the function or expression in function_to_apply is applied to each of the variables we’ve chosen.\nAs an example, we can change all of the mattering variables at once as follows:\n\nmil_data %&gt;%\n  dplyr::mutate(\n     dplyr::across(\n        .cols = starts_with(\"mattering\"),\n        .fn = unclass\n     )\n  )\n\n\n\n  \n\n\n\nHere I’ve used the dplyr::starts_with() function to choose which variables I want to change, and then each of those variables will have the unclass() function applied to them, converting them to numeric. This is exactly the same result as:\n\nmil_data %&gt;%\n  dplyr::mutate(\n    mattering_1 = unclass(mattering_1),\n    mattering_2 = unclass(mattering_2),\n    mattering_3 = unclass(mattering_3),\n    mattering_4 = unclass(mattering_4)\n  )\n\n…but with no risk of accidentally replacing variables with the wrong values due to copy/paste or typing mistakes."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#calculating-variables",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#calculating-variables",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Calculating Variables",
    "text": "Calculating Variables\nAs a final topic to get you ahead on your data analysis, this last section is a brief revision/reference of key topics you’ve already covered previously in your core methods modules. First, we’ll revise reverse-coding, to reverse the direction of responses on particular items as necessary. Once that’s done, we can create scores for each group of items that belong to the same subscale to get overall subscale scores to use in analysis.\n\n\n\n\n\n\nTip\n\n\n\nBoth reverse-coding and composite scores, including the underlying concepts and the code, have previously been covered in last year’s QQM Skills Lab.\n\n\n\nReverse Coding\nYou may or not have items in your questionnaire that are reverse-coded. There’s nothing in the data that will tell you this; you have to know what’s in your questionnaire, how the questions were designed, and which item(s) need reverse-coding. Make sure you check this carefully before you go on if working with your own data.\n\n\n\n\n\n\nWhat is reverse-coding?\n\n\n\n\n\nIn many multi-item measures, some items are reversed in the way that they capture a particular construct. For example, items on the State-Trait Inventory of Cognitive and Somatic Anxiety (STICSA, not in this example data) are worded so that a higher numerical response (closer to the “very much so” end of the scale) indicates more anxiety, such as item 4: “I think that others won’t approve of me”.\nHowever, reverse-coded items are intended to capture the same ideas, but in reverse. A reversed version a STICSA item might read, “I can concentrate easily with no intrusive thoughts.” In this case, a higher numerical response (closer to the “very much so” end of the scale) would indicate less anxiety. In order for these reversed items to be aligned with the other items on the scale, so that together they form a cohesive score, the coding of the response scale must be flipped: high becomes low, and low becomes high.\nIf the response scale is a numerical integer sequence, as this one is, then the simplest way to reverse-code the responses is to subtract every response from the maximum possible response plus one. For the STICSA, the response scale ranges from 1 to 4; the maximum possible response is 4, plus one is 5. So, to reverse-code the responses, we would need to subtract each rating on this item from 5. A high score (4) will be become a low score (5 - 4 = 1), and vice versa for a low score (5 - 1 = 4).\n\n\n\nIn order to reverse-code a variable, we will need to make use again of the dplyr::mutate() function for changing variables. For example, let’s reverse-code mattering_32.\nFirst, we need to know the maximum possible value in this variable. Using our data dictionary, we can see that values range from 1 to 7. So, to reverse-code, we should subtract each value from the max value plus one = 7 + 1 = 8.\nThen, we simply overwrite the existing variable with the new scores:\n\nmil_data %&gt;% \n  dplyr::mutate(\n    mattering_3 = 8 - mattering_3\n  )\n\nNote that it is recommended to overwrite the item, rather than create a new variable, so that you don’t accidently include the wrong or multiple versions in the next step.\n\n\nComposite Scores\nOnce all your items are cleaned and reverse-scored, you can finally create a composite score. For example, we have four mattering items, that we can combine into a “composite” score measuring general performance across all items. How this composite is calculated will depend on the questionnaire you’re using, but as many questionnaire subscale scores use mean scores3, we will demonstrate that here.\nTo do this, we need two new functions.\n\nThe first new function, dplyr::c_across(), provides an efficient way to select multiple variables to contribute to the calculation - namely, by using &lt;tidyselect&gt; selection helpers.\nThe second new function is actually a pair of functions, dplyr::rowwise() and dplyr::ungroup(). These two respectively impose and remove an internal structure to the dataset, such that each row is treated like its own group, and any operations are done within those row-wise groups.\n\nLet’s see the combination of these two in action to create a mattering composite score.\n\n\n\n\n\n\nImportant\n\n\n\nThe code below assumes a dataset structured so there is information from each participant on only and exactly one row in the dataset.\nIf your data has observations from the same participants on multiple rows, you will need to reshape your data or otherwise adapt the code to suit your data structure.\n\n\n\n1mil_data |&gt;\n2  dplyr::rowwise() |&gt;\n3  dplyr::mutate(\n    mattering_comp = mean(c_across(starts_with(\"mattering\")),\n                        na.rm = TRUE)\n  ) |&gt;\n4  dplyr::ungroup()\n\n\n1\n\nOverwrite the mil_data dataset with the following output: take the existing mil_data dataset, and then\n\n2\n\nGroup the dataset by row, so any subsequent calculations will be done for each row separately, and then\n\n3\n\nCreate the new mattering_comp variable by taking the mean of all the values in variables that start with the string “mattering” (ignoring any missing values), and then\n\n4\n\nRemove the by-row grouping that was created by rowwise() to output an ungrouped dataset.\n\n\n\n\n\n\n  \n\n\n\nIf you don’t feel comfortable using selection helpers, you can list variables instead inside c_across() using c() to combine them:\n\nmil_data |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate(\n    mattering_comp = mean(c_across(c(mattering_1, mattering_2, mattering_3, mattering_4)),\n                        na.rm = TRUE)\n  ) |&gt;\n  dplyr::ungroup()\n\nHowever, you’re strongly recommended to get the hang of selection helpers, since they are both easy to read and use and extremely versatile!\n\n\n\n\n\n\nRunning Code Out of Order\n\n\n\nUsing selection helpers like this does have a potential issue: it will give you the wrong answer if you run the same code more than once, or out of the order.\nIn the first example above using starts_with(), this command calculates the mean across all of the variables in the data whose names start with the string “mattering”. This will be mattering_1, mattering_2, mattering_3, and mattering_4.\nHowever, if you run the same code a second time, the command will again calculate the mean across all of the variables in the data whose names start with the string “mattering”. This will be mattering_1, mattering_2, mattering_3, mattering_4 - AND mattering_comp, which was created previously.\nAlthough the second example above enumerating individual variable names doesn’t have this danger, it’s still better to use the selection helpers, and simply never run your code out of order."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#exercises-conversion-and-wrangling",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#exercises-conversion-and-wrangling",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Exercises: Conversion and Wrangling",
    "text": "Exercises: Conversion and Wrangling\n\n\n\n\n\n\nExercise\n\n\n\nPrepare the mil_data dataset for analysis.\n\nProduce a final data dictionary and save it.\nConvert all categorical variables to factor, and all scale rating variables to numeric.\nReverse-code global_meaning_2.\nCreate composite scores for all of the subscale variables.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nProduce a final data dictionary and save it.\n\nUsing the help documentation, we can see there is a file argument. Providing a file path/name will save the output of this file into that file. For example, the command below will save the data dictionary as an HTML file to review or share.\n\nsjPlot::view_df(mil_data, file = \"diss_dict.html\")\n\n\nConvert all categorical variables to factor, and all scale rating variables to numeric.\n\nThis can again be accomplished multiple ways. The first way involves listing each variable one by one. Below just a couple of variables are listed, but this would need to be done individually for every variable in the dataset that needs conversion.\n\nmil_data %&gt;% \n  dplyr::mutate(\n    english_fluency = labelled::to_factor(english_fluency),\n    mattering_1 = unclass(mattering_1),\n    ...\n  )\n\nInstead, you are strongly recommended to use dplyr::across() and selection helpers.\n\nmil_data &lt;- mil_data %&gt;% \n  dplyr::mutate(\n    ## Change all grouping variables to factor\n    dplyr::across(c(english_fluency, gender, income, occupation),\n                  labelled::to_factor),\n    ## Change all subscale items to numeric\n    dplyr::across(contains(c(\"mattering\", \"global_meaning\", \"belonging\")),\n                  unclass)\n  )\n\n\nReverse-code global_meaning_2.\n\n\nmil_data &lt;- mil_data %&gt;% \n  dplyr::mutate(\n    global_meaning_2 = 8 - global_meaning_2\n  )\n\n\nCreate composite scores for all of the subscale variables.\n\nThere are three subscales to calculate here for belonging, global_meaning, and mattering.\nAgain, you can type the item names into this command one by one. This does require careful checking to avoid duplicating or leaving out items, especially for subscales with many items. On the other hand, this method is likely to work better if you need specific and nonsequential items for each subscale (for instance, if the subscale is items 2, 3, 7, 10, and 12). For a &lt;tidyselect&gt; way to accompish this, have a look at the selection helper num_range() in the select() help documentation.\nInstead, if all the items with the same prefix belong to the same subscale (as they do here), c_across() + selection helpers are much preferred.\n\nmil_data &lt;- mil_data %&gt;% \n  dplyr::rowwise() %&gt;% \n  dplyr::mutate(\n    belonging_comp = mean(c_across(starts_with(\"belonging\")),\n                          na.rm = TRUE),\n    global_meaning_comp = mean(c_across(starts_with(\"global_meaning\")),\n                          na.rm = TRUE),\n    mattering_comp = mean(c_across(starts_with(\"mattering\")),\n                          na.rm = TRUE),\n  ) %&gt;% \n  dplyr::ungroup()"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#well-done",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#well-done",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Well done!",
    "text": "Well done!\nFrom here you can carry on with your data analysis: further cleaning, visualisation, and analysis. You’ve gained quite a few new skills today, so very well done indeed!"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop_sol.html#footnotes",
    "href": "workshops/dissertations/qualtrics_workshop_sol.html#footnotes",
    "title": "Working with Qualtrics Data (with Solutions)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the purposes of simplicity, we’re going to pretend that Likert and similar rating scales are “continuous”.↩︎\nNote that this item is NOT reversed on the real scale; this is only for practice!↩︎\nNote that averaging Likert data is controversial (h/t Dr Vlad Costin!), but widespread in the literature. We’re going to press boldly onward anyway to not get too deep in the statistical weeds, but if you’re using Likert scales in your own research, it’s something you might want to consider.↩︎"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "This is the home of any materials for one-off workshops outside of the PsychRlogy series. Workshops may focus on particular tasks, outputs, or analyses and will require some core skills in R (typically through the Essentials section of the PsychRlogy series).\nIf you have a suggestion for a workshop topic, you can drop it in the Suggestion Box on Canvas or get in touch."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html",
    "title": "10: Qualtrics and Labelled Data",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to easily produce a data dictionary straight from the dataset itself.\n\n\nThis tutorial was co-conceived and co-created with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. This included collecting commonly asked questions and issues with Qualtrics data analysis; discussing the topics to cover and how best to cover them; and testing code and solutions. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#overview",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#overview",
    "title": "10: Qualtrics and Labelled Data",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to easily produce a data dictionary straight from the dataset itself.\n\n\nThis tutorial was co-conceived and co-created with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. This included collecting commonly asked questions and issues with Qualtrics data analysis; discussing the topics to cover and how best to cover them; and testing code and solutions. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#setup",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#setup",
    "title": "10: Qualtrics and Labelled Data",
    "section": "Setup",
    "text": "Setup\n\nPackages\nAs usual, we will be using {tidyverse}. When {tidyverse} is installed, it also installs the {haven} package, which we will use for data importing. However, {haven} isn’t loaded as part of the core {tidyverse} group of packages, so let’s load it separately. Finally, we will also need the {labelled} package to work with labelled data.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(sjPlot)\n\n\n\n\n\n\n\n\nData\nToday’s dataset focuses on various aspects of meaning in life (MiL), and has been randomly generated based on a real dataset kindly contributed by Hanna Eldarwish and Vlad Costin. All variables have been randomly generated, but they are based on the patterns in the original dataset. The original, bigger dataset will be made available alongside article publication in the future, so keep an eye out for it!\n\n\n\n\n\n\nNew File Type\n\n\n\nYou might notice that instead of the familiar readr::read_csv(), today we have haven::read_sav(). That’s because the file I’ve prepared is a SAV file, associated with the SPSS statistical analysis programme. The next section explains why we are using this data type, but otherwise, there’s nothing new about these commands.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRead in the mil_data.sav object from the data folder, or alternatively from Github via URL, as you prefer.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom a folder:\n\nmil_data &lt;- here::here(\"data/mil_data.sav\") |&gt; haven::read_sav()\n\nFrom URL:\n\nmil_data &lt;- haven::read_sav(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/mil_data.sav\")\n\n\n\n\n\n\n\nCodebook\nThis codebook is intentionally sparse, because we’ll be generating our own from the dataset in just a moment. This table covers only the questionnaire measures to help you understand the variables.\n\n\n\n\n\nVariable Prefix\nScale\nSubscale\n\n\n\n\nglobal_meaning\nMeaning in Life\nGlobal Meaning\n\n\nmattering\nMeaning in Life\nMattering\n\n\ncoherence\nMeaning in Life\nCoherence\n\n\npurpose\nMeaning in Life\nPurpose\n\n\nsym_immortality\nSymbolic Immortality\nSingle scale\n\n\nbelonging\nBelonging\nSingle scale"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#qualtrics-data",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#qualtrics-data",
    "title": "10: Qualtrics and Labelled Data",
    "section": "Qualtrics Data",
    "text": "Qualtrics Data\nQualtrics is a survey-building tool very commonly used for questionnaire-type studies, as well as some experimental work. The University of Sussex has an institutional licence for Qualtrics, so all staff and students can log in with their Sussex details and easily construct and collaborate on surveys.\nFor help using Qualtrics itself, the Qualtrics support pages are generally excellent. This tutorial will only briefly touch on the options within Qualtrics itself.\nOnce the study is complete and responses have been collected, you will need to export your data from Qualtrics so that you can analyse it. Qualtrics offers a variety of export data types, including our familiar CSV type. However, we’re going to instead explore a new option: SAV data.\n\nSAV Data\nThe .sav file type is associated with SPSS, a widely used statistical analysis programme. So, why are we using SPSS files when working in R?\nImporting via .sav has two key advantages. First, it results in a much cleaner import format. If you try importing the same data via .csv file, you’ll find that you need to do some very fiddly and pointless cleanup first. For instance, the .csv version of the same dataset will introduce some empty rows that have to be deleted with dplyr::slice() or similar. The .sav version of the dataset doesn’t have any comparable formatting issues.\nMost importantly, however, importing .sav file types into R with particular packages like {haven} gets us a dataset with a special type of data: namely, labelled data. The labels allow us to preserve important information about the questions asked and response options in Qualtrics, and to (mostly) painlessly create codebooks for datasets. We will explore these features in depth in this tutorial.\n\n\nExporting from Qualtrics\nIf you’d like to work with your own study data, you will need to export your data in SAV format from Qualtrics first. To do this, open your Qualtrics survey and select the “Data & Analysis” tab along the top, just under the name of your survey.\nIn the Data Table view, look to the right-hand side of the screen. Click on the drop-down menu labelled “Export & Import”, then select the first option, “Export Data…”\n\nIn the “Download a data table” menu, choose “SPSS” from the choices along the top. Make sure “Download all fields” is ticked, then click “Download”.\n\nThe dataset will download automatically to your computer’s Downloads folder. From there, you should rename it to something sensible and move it into a data folder within your project folder. From there, you can read it in using the here::here() |&gt; haven::read_sav() combo that we saw in the Data section previously.\n\n\n\n\n\n\nSensible Naming Conventions and Folder Structure\n\n\n\n\n\nI know it may not seem like something anyone should care about, but sensible file and folder names will make your life so much easier for working in R (and generally).\nFor folder structure, make sure you do the following:\n\nAlways always ALWAYS use an R Project for working in R.\nHave a consistent set of folders for each project: for example, images, data, and docs.\nUse sub-folders where necessary, but consider using sensible naming conventions instead.\n\nFor naming conventions, your file name should make it obvious what information it contains and when it was created, especially for datasets like this. Personally, I would prefer longer and more explicit file names over brevity; this is because I prefer to navigate files using R, and that’s much easier using explicit file names than it is with file metadata.\nSo, for a download like this, I’d probably name it something like qtrics_diss_2023_11_08.sav. The qtrics tells me it’s a Qualtrics export, the diss tells me it’s a dissertation project, and the last bit is the full date in easily machine-readable format. Imagine if I continue to recruit participants and download a new dataset later, say a month from now, and name it qtrics_diss_2023_12_08.sav. I could easily distinguish which dataset was which by the date, but also see that they are different versions of the same thing by their shared prefix.\nThis is a much more reliable system than calling them, say, Qualtrics output.sav and Dissertation FINAL REAL.sav. This kind of naming “convention” contains no information about which is which or when they were exported, or even that they’re two versions of the same study dataset! It might seem like a small detail at the time, but Future You trying to figure out which dataset to use weeks or months later will feel the difference."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#the-plan",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#the-plan",
    "title": "10: Qualtrics and Labelled Data",
    "section": "The Plan",
    "text": "The Plan\nOur workflow for this dataset will be slightly different than previously. We’ll start by doing some basic cleanup of the dataset, and produce a codebook, or “data dictionary”, drawing on the label metadata in the SAV file. For the purpose of practice, we’ll also have a look at how to work with those labels, and manage different types of missing values.\nAs useful as labels are, they will get in the way when we want to work with our dataset further. So, we’ll convert the variables in the dataset into either factors, for categorical data, or numeric, for continuous data 1. From that point forward, we can work with the dataset using the techniques and functions we’ve learned thus far."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#cleanup-and-data-dictionary",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#cleanup-and-data-dictionary",
    "title": "10: Qualtrics and Labelled Data",
    "section": "Cleanup and Data Dictionary",
    "text": "Cleanup and Data Dictionary\n\n\n\n\n\n\nTip\n\n\n\nMost of the following examples are drawn from the “Introduction to labelled” vignette from the {labelled} package. If you want to do something with labelled data that isn’t covered here, that’s a good place to start!\n\n\nLet’s start off by having a look at the dataset. As usual, you can call the dataset or use View() on it directly, but we’re going to take advantage of the new data type to get a more helpful summary, that emulates the “Variable View” in SPSS.\n\n\n\n\n\n\nExercise\n\n\n\nUse the generate_dictionary() function from the {labelled} packages to create a data dictionary for mil_data, then pipe it into View().\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmil_data |&gt; \n  labelled::generate_dictionary() |&gt;\n  View()\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nWhat we get is a new summary dataset that contains some useful information about each of the variables in mil_data. Along with the actual variable name in the dataset, which we see under variable, we also get the actual question that participants saw in Qualtrics under label, and the response options - where applicable - in value_labels.\nWhy a data dictionary? There’s two key reasons to do this. First, as we’ve already seen throughout these tutorials, data dictionaries (or “codebooks”) are very useful for understanding datasets, even your own. In this case, we’ve generally named our variables usefully in Qualtrics before the export, but if you forget to (or don’t typically) do this, this reference helps you navigate unhelpful variable names like “Q42”, “Q16” etc. The second reason is for other people: if you want to share your data publicly, including a dictionary/codebook is not only a kindness to other users but also helps prevent misuse or misunderstandings.\nBefore we look at these labels in more depth, we’re first going to address two minor issues that commonly come up with Qualtrics data to make sure our dataset is ready to use.\n\nRenaming Variables\nIf you inspected the dataset closely, you might have noticed that one of the items has a strange name: coherence_42, right between coherence_1 and coherence_3.\nThis wasn’t intentional - in the process of creating the questionnaire in Qualtrics, this variable came out with a weird name. It happens more easily than you think! The best-case option would be to update the Qualtrics questionnaire itself before exporting the data, but you may not be able (or want) to do this, so instead, let’s have a quick look at how to rename variables.\nAs (almost) always, there’s a friendly {dplyr} function to help us with our data wrangling. This time it’s sensibly-named dplyr::rename(), which renames variables using new_name = old_name arguments.\n\n\n\n\n\n\nExercise\n\n\n\nRename coherence_42 to coherence_2. Don’t forget to save this change to the dataset!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmil_data &lt;- mil_data |&gt; \n  dplyr::rename(coherence_2 = coherence_42)\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDo you have lots of variables to rename? Do you like writing functions, or using regular expressions? Check out rename()’s flashier cousin rename_with(), which uses a function to rename variables.\n\n\n\n\nSeparating Columns\nThe second thing I’d like to do doesn’t concern the main dataset, but rather the data dictionary we’ve generated. For the single-item questions, the label column is reasonably helpful. However, the items with a shared prefix all come from the same matrix scale in Qualtrics, and their labels have two parts: the “question text” that usually contains directions about how to respond, and the actual item text for each individual item.\nAs an example, the label for belonging_1 reads:\n\nPlease rate the extent to which these statements apply to you. - When I am with other people, I feel included\n\nWhich corresponds to this in Qualtrics:\n\nTo make the labels more readable, let’s split up the question text, which is repeated for all items on the same subscale and not very useful, and the item text, which contains the specific text of each item. The good news is that the two pieces are defined, or delimited, by the ” - ” symbol that Qualtrics automatically adds to link them.\nSince we want to separate the labels column into two - making the dataset wider - using a delimiter, the separate_wider_delim() function from the {tidyr} package should do the trick!\n\n11mil_data |&gt;\n22  labelled::generate_dictionary() |&gt;\n33  tidyr::separate_wider_delim(\n4    cols = label,\n54    delim = \" - \",\n6    names = c(\"label\", \"item_label\"),\n7    too_few = \"align_start\"\n  )\n\n\n1\n\nTake the data, and then\n\n2\n\nGenerate the data dictionary, and then\n\n3\n\nSeparate wider by delimiter as follows:\n\n4\n\nSeparate the label column\n\n5\n\nAt the ” - ” delimiter\n\n6\n\nInto two new columns called “label” and “item_label” respectively\n\n7\n\nIf there are too few pieces (that is, for the rows where there is no delimiter), fill in values from the start.\n\n\n\n\n\n\n  \n\n\n\nThe result isn’t perfect, but it’ll do for our purposes - namely, to have a quick reference for the variables in our dataset.\n\n\n\n\n\n\nExercise\n\n\n\nSave the final (for now) data dictionary in a new object, mil_dict, so we can refer to it as needed.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmil_dict &lt;- mil_data |&gt; \n  labelled::generate_dictionary() |&gt; \n  tidyr::separate_wider_delim(\n    cols = label,\n    delim = \" - \",\n    names = c(\"label\", \"item_label\"),\n    too_few = \"align_start\"\n  )\n\n\n\n\n\n\n\n\nViewer Data Dictionary\nI personally like labelled::generate_dictionary() because (you will be unsurprised to learn) I like to mess about with regex to make it read just as I like. However, if you primarily need a quick reference as you’re working with your dataset, the delightful sjPlot::view_df() function makes this particularly easy.\n\n\n\n\n\n\nExercise\n\n\n\nPut mil_data into the sjPlot::view_df() function and see what it does!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBy default, the document opens in the Viewer, but you can also save the file it creates for further sharing - see the help documentation.\n\nsjPlot::view_df(mil_data)\n\n\nData frame: mil_data\n\n\n\n\n\n\n\n\n\nID\nName\nLabel\nValues\nValue Labels\n\n\n1\nStartDate\nStart Date\n\n\n\n\n2\nEndDate\nEnd Date\n\n\n\n\n3\nStatus\nResponse Type\n0\n1\n2\n4\n8\n9\n12\n16\n17\n32\n40\n48\nIP Address\nSurvey Preview\nSurvey Test\nImported\nSpam\nSurvey Preview Spam\nImported Spam\nOffline\nOffline Survey Preview\nEX\nEX Spam\nEX Offline\n\n\n4\nFinished\nFinished\n0\n1\nFalse\nTrue\n\n\n5\nRecordedDate\nRecorded Date\n\n\n\n\n6\nResponseId\nResponse ID\n\n&lt;output omitted&gt;\n\n\n7\nDistributionChannel\nDistribution Channel\n\n&lt;output omitted&gt;\n\n\n8\nUserLanguage\nUser Language\n\n&lt;output omitted&gt;\n\n\n9\nenglish_fluency_1\nPlease select which box best describes your\nEnglish fluency. - How well can you speak English?\n1\n2\n3\n4\nVery well\nWell\nNot well\nNot at all\n\n\n10\nage\nHow old are you?\nrange: 15-82\n\n\n11\ngender\nWhat is your gender identity? This question is\noptional. - Selected Choice\n0\n1\n2\n3\nMale\nFemale\nNon-binary\nOther (please state below)\n\n\n12\nglobal_meaning_1\nPlease rate the extent to which you agree or\ndisagree with these statements. - My life as a\nwhole has meaning.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n13\nglobal_meaning_2\nPlease rate the extent to which you agree or\ndisagree with these statements. - My entire\nexistence is full of meaning.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n14\nglobal_meaning_3\nPlease rate the extent to which you agree or\ndisagree with these statements. - My life is\nmeaningless.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n15\nglobal_meaning_4\nPlease rate the extent to which you agree or\ndisagree with these statements. - My existence is\nempty of meaning.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n16\nmattering_1\nPlease rate the extent to which you agree or\ndisagree with these statements. - Whether my life\never existed matters even in the grand scheme of\nthe universe.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n17\nmattering_2\nPlease rate the extent to which you agree or\ndisagree with these statements. - Even considering\nhow big the universe is, I can say that my life\nmatters.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n18\nmattering_3\nPlease rate the extent to which you agree or\ndisagree with these statements. - My existence is\nnot significant in the grand scheme of things.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n19\nmattering_4\nPlease rate the extent to which you agree or\ndisagree with these statements. - Given the\nvastness of the universe, my life does not matter.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n20\ncoherence_1\nPlease rate the extent to which you agree or\ndisagree with these statements. - I can make sense\nof the things that happen in my life.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n21\ncoherence_2\nPlease rate the extent to which you agree or\ndisagree with these statements. - Looking at my\nlife as a whole, things seem clear to me.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n22\ncoherence_3\nPlease rate the extent to which you agree or\ndisagree with these statements. - I can’t make\nsense of events in my life.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n23\ncoherence_4\nPlease rate the extent to which you agree or\ndisagree with these statements. - My life feels\nlike a sequence of unconnected events.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n24\npurpose_1\nPlease rate the extent to which you agree or\ndisagree with these statements. - I have a good\nsense of what I am trying to accomplish in life.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n25\npurpose_2\nPlease rate the extent to which you agree or\ndisagree with these statements. - I have certain\nlife goals that compel me to keep going.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n26\npurpose_3\nPlease rate the extent to which you agree or\ndisagree with these statements. - I don’t know\nwhat I am trying to accomplish in life.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n27\npurpose_4\nPlease rate the extent to which you agree or\ndisagree with these statements. - I don’t have\ncompelling life goals that keep me going.\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n28\nsym_immortality_1\nPlease indicate the extent to which you believe\nthese statements are likely to occur. - After I\ndie, my impact on the world will continue\n1\n2\n3\n4\n5\n6\n7\nExtremely unlikely\nModerately unlikely\nSlightly unlikely\nNeither likely nor unlikely\nSlightly likely\nModerately likely\nExtremely likely\n\n\n29\nsym_immortality_2\nPlease indicate the extent to which you believe\nthese statements are likely to occur. - Some\naspect of myself, such as my name or\naccomplishments, will be remembered long after I\ndie\n1\n2\n3\n4\n5\n6\n7\nExtremely unlikely\nModerately unlikely\nSlightly unlikely\nNeither likely nor unlikely\nSlightly likely\nModerately likely\nExtremely likely\n\n\n30\nbelonging_1\nPlease rate the extent to which these statements\napply to you. - When I am with other people, I\nfeel included\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n31\nbelonging_2\nPlease rate the extent to which these statements\napply to you. - I have close bonds with family and\nfriends\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n32\nbelonging_3\nPlease rate the extent to which these statements\napply to you. - I feel accepted by others\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n33\nbelonging_4\nPlease rate the extent to which these statements\napply to you. - I have a sense of belonging\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n34\nbelonging_5\nPlease rate the extent to which these statements\napply to you. - I have a place at the table with\nothers\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n35\nbelonging_6\nPlease rate the extent to which these statements\napply to you. - I feel connected with others\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n36\nbelonging_7\nPlease rate the extent to which these statements\napply to you. - I feel like an outsider\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n37\nbelonging_8\nPlease rate the extent to which these statements\napply to you. - I feel as if people do not care\nabout me\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n38\nbelonging_9\nPlease rate the extent to which these statements\napply to you. - Because I do not belong, I feel\ndistant during the holiday season\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n39\nbelonging_10\nPlease rate the extent to which these statements\napply to you. - I feel isolated from the rest of\nthe world\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n40\nbelonging_11\nPlease rate the extent to which these statements\napply to you. - When I am with other people, I\nfeel like a stranger\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree\n\n\n41\nbelonging_12\nPlease rate the extent to which these statements\napply to you. - Friends and family do not involve\nme in their plans\n1\n2\n3\n4\n5\n6\n7\nStrongly disagree\nDisagree\nSomewhat Disagree\nNeither Agree nor Disagree\nSomewhat agree\nAgree\nStrongly agree"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#labelled-data",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#labelled-data",
    "title": "10: Qualtrics and Labelled Data",
    "section": "Labelled Data",
    "text": "Labelled Data\nAs we’ve just seen in the data dictionary, the SAV data we’re using has a special property: labels. Labelled data has a number of features, all of which we will explore in depth shortly:\n\nVariable labels. The label associated with a whole variable will contain the text of the item that the participants responded to. This is analogous to the “Label” column of the Variable View in SPSS.\nValue labels. The label associated with individual values within a variable will contain the text associated with individual choices, for instance the points on a Likert scale or the options on a multiple-choice question. This is analogous to the “Values” column of the Variable View in SPSS.\nMissing values. Within value labels, you can designate particular values as indicative of missing responses, refusal to respond, etc. This is analogous to the “Missing” column of the Variable View in SPSS.\n\nWe’re first going to look at how you can work with each of these elements. The reason to do this is that once our dataset has been thoroughly checked, we’re going to generate a final data dictionary, then convert any categorical variables into factors, the levels of which will correspond to the labels for that variable. We’ll also convert any numeric variables into numeric data type, which will discard the labels; that will make it possible to do analyses with them, but that’s why we have to create the data dictionary first.\n\n\n\n\n\n\nImportant\n\n\n\nThese features will work optimally only if you have set up your Qualtrics questionnaire appropriately. Make sure to refer to the Setting Up Qualtrics section of the next tutorial to get the most out of your labelled data and save yourself data cleaning and wrangling headaches later.\n\n\n\nVariable Labels\nVariable labels contain information about the whole variable, and for Qualtrics data, will by default contain either an automatically generated Qualtrics value (like “Start Date”), or the question text that that variable contains the responses to.\n\nGetting Labels\nTo begin, let’s just get out a single variable label to work with using labelled::var_label().\nTo specify the variable we want, we will need to subset it from the dataset, using either $ or dplyr::pull() as previously.\n\nlabelled::var_label(mil_data$gender)\n\n[1] \"What is your gender identity?\\n\\nThis question is optional. - Selected Choice\"\n\n\n\n\nCreating/Updating Labels\nIf you’d like to edit labels, you can do it “manually” - that is, just writing a whole new label from scratch.\nThe structure of this code might look a little unfamiliar in terms of the code structure. For the most part, we’ve seen code that contains longer and more complex instructions on the right-hand side of the &lt;-, and a single object being created or updated on the left-hand side. In the structure below, the left-hand side contains longer and more complex code that identifies the value(s) to be updated or created, and the right-hand side contains the value(s) to create or update. It’s the same logic, just with a different structure.\n\nlabelled::var_label(mil_data$StartDate) &lt;- \"Date and time questionnaire was started\"\n\nlabelled::var_label(mil_data$StartDate)\n\n[1] \"Date and time questionnaire was started\"\n\n\nIf you’re up for it, though, I’d recommend using it as an opportunity to start working with regular expressions. For example, if we want to keep only the first bit of the label for gender, then we can keep everything only up to an including the question mark, and and re-assign that to the variable label. This style is a bit more dynamic and resilient to changes or updates.\n\nlabelled::var_label(mil_data$gender) &lt;- labelled::var_label(mil_data$gender) |&gt; \n  gsub(\"(.*\\\\?).*\", \"\\\\1\", x = _)\n\nlabelled::var_label(mil_data$gender)\n\n[1] \"What is your gender identity?\"\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: How can you read the gsub() command above? Why migh this be “more dynamic and resilient to changes or updates”?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s pick apart this gsub() command a bit at a time. First, gsub() has three arguments:\n\npattern, here \"(.*\\\\?).*\", which is the regex statement representing the string to match.\nreplacement, here \"\\\\1\", which is the string that should replace the match in pattern.\nx, the string to look in.\n\nThe pattern has essentially two parts: the bit in the rounded brackets, and the bit outside. The rounded brackets designate a “capturing group” - a portion of the string that should be grouped together as a unit. The benefit of this grouping is in the second argument of gsub(); \\\\1 isn’t the number 1, but rather is a pronoun referring to the first capturing group. In other words, as a whole, this gsub() command captures a subset of the incoming string, and then replaces the entire string with that captured string, essentially dropping everything outside the capturing group.\nTo understand the regex statement \"(.*\\\\?).*\", we need to look at the incoming text, x. In this case, x is being piped in from above and looks like this:\n\nlabelled::var_label(mil_data$gender)\n\n[1] \"What is your gender identity?\\n\\nThis question is optional. - Selected Choice\"\n\n\nAs discussed in a previous Challenge task, .* is a common regex shorthand that means “match any character, as many times as possible.” It’s essentially an “any number of anything” wildcard. This wildcard appears both inside and outside the brackets. So, how does gsub() know which bit should belong in the capturing group?\nThe answer is \\\\?. This is a “literal” question mark. Some symbols, like . and ?, are regex operators, but we might want to also match the “literal” symbols full-stop “.” and question mark “?” in a string. In this case we need an “escape” character “\", that escapes regex and turns the symbol into a literal one. So, the capturing group ends with a literal question mark - in the target string, that’s the question mark after”identity”, which is the only one in the string.\nAs an aside, if you’re wondering why there are two escape characters instead of one - i.e., why is it \\\\? and not \\?, well, you and me both. There’s an explanation in vignette(\"regular-expressions\") that never completely makes sense to me. Also, this seems to be an R thing - regex outside of R seems to use only a single escape character, so a literal question mark would be \\?. If you are ever trying to adapt regex from e.g. StackOverflow or regex101 and it isn’t working, check whether the escape characters are right!\nAnyway. We can now read \"(.*\\\\?)\" as “capture all characters up to an including a literal question mark” - which matches the substring “What is your gender identity?” in x. However, we don’t just want to replace that portion of the string - instead, we want to replace the whole string with that bit of it. So, the second .* outside the brackets matches the rest of the string. If we didn’t include this last bit, the capturing group would just be replaced with itself, which would result in the same string as we started with, as below:\n\nlabelled::var_label(mil_data$gender) |&gt; \n  gsub(\"(.*\\\\?)\", \"\\\\1\", x = _)\n\n[1] \"What is your gender identity?\\n\\nThis question is optional. - Selected Choice\"\n\n\nSo, altogether, we can read this gsub() command as: “Capture everything up to an including the question mark, and replace the entire string with that capturing group.”\nNow. Why, you might wonder, is all this faff better?\nWell, it might not be. You might find it more frustrating or effortful to generate the right regex pattern than to replace the label “manually”, and in that case, there’s nothing wrong with just writing out the label you want. I said this was “more dynamic and resilient” because this command will always drop everything after the question mark, no matter what that text is. If there is no match, it won’t replace anything. So, unlike the “manual” option, there’s much less danger of accidentally mixing up labels or overwriting the wrong thing; and this regex statement can be generalised to any label that contains a question mark, rather than having to type out each label one by one.\n\n\n\n\n\n\n\nSearching Labels\nA very nifty feature of variable labels and {labelled} is the ability to search through them with labelled::look_for(). With the whole dataset, look_for() returns essentially the same info as generate_dictionary(), but given a second argument containing a search term, you get back only the variables whose label contains that term.\n\n\n\n\n\n\nExercise\n\n\n\nUse labelled::look_for() to get only the items in this questionnaire that mentioned family.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nI’ve piped into tibble::as_tibble() to make the output easier to read.\n\nlabelled::look_for(mil_data, \"family\") |&gt; \n  tibble::as_tibble()\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nValue Labels\nValue labels contain individual labels associated with unique values within a variable. It’s not necessary to have a label for every value, and indeed sometimes it’s advantageous not to.\n\nGetting Labels\nThere are two functions to assist with this. labelled::val_labels() (with an “s”) returns all of the labels, while labelled::val_label() (without an “s”) will return the label for a single specified value.\n\nlabelled::val_labels(mil_data$english_fluency_1)\n\n Very well       Well   Not well Not at all \n         1          2          3          4 \n\n\n\nlabelled::val_label(mil_data$english_fluency_1, 3)\n\n[1] \"Not well\"\n\n\n\n\nCreating/Updating Labels\nThese two functions can also be used to update an entire variable or a single value respectively. The structure of this code is the same as we saw with variable labels previously.\n\n\n\n\n\n\nExercise\n\n\n\nGet all the value labels for the gender variable. Then, update the last value to “Other”.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlabelled::val_labels(mil_data$gender)\n\n                      Male                     Female \n                         0                          1 \n                Non-binary Other (please state below) \n                         2                          3 \n\n\nThe code for replacing this is much simpler manually…\n\nlabelled::val_label(mil_data$gender, 3) &lt;- \"Other\"\n\nBut when has that ever stopped me?\n\nlabelled::val_label(mil_data$gender, 3) &lt;- labelled::val_label(mil_data$gender, 3) |&gt; \n  gsub(\"(.*?) .*\", \"\\\\1\", x = _)\n\n\n\n\n\n\n\n\n\nMissing Values\nLabelled data allows an extra functionality from SPSS, namely to create user-defined “missing” values. These missing values aren’t actually missing, in the sense that the participant didn’t respond at all. Rather, they might be missing in the sense that a participant selected an option like “don’t know”, “doesn’t apply”, “prefer not to say”, etc.\nLet’s look at an example. As we’ve just seen, we can get out all the value labels in variable with labelled::val_labels():\n\nlabelled::val_labels(mil_data$english_fluency_1)\n\n Very well       Well   Not well Not at all \n         1          2          3          4 \n\n\nThis variable asked participants to indicate their level of English fluency. Even for participants who have in fact responded to this question, we may want to code “Not well” and “Not as all” as “missing” so that they can be excluded easily. To do this, we can use the function labelled::na_values() to indicate which values should be considered as missing.\n\nlabelled::na_values(mil_data$english_fluency_1) &lt;- 3:4\n\nmil_data$english_fluency_1\n\n&lt;labelled_spss&lt;double&gt;[164]&gt;: Please select which box best describes your English fluency. - How well can you speak English?\n  [1] 2 2 2 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 2 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 2 2\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 3 2\nMissing values: 3, 4\n\nLabels:\n value      label\n     1  Very well\n     2       Well\n     3   Not well\n     4 Not at all\n\n\nFor the moment, these values are not actually NA in the data - they’re listed under “Missing Values” in the variable attributes. In other words, the actual responses are still retained. However, if we ask R which of the values in this variable are missing…\n\nis.na(mil_data$english_fluency_1)\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n…we can see one TRUE corresponding to the 3 above.\nIf we wanted to actually remove those values entirely and turn them into NAs for real, we could use labelled::user_na_to_na() for that purpose. Now, the variable has only two remaining values, and any 3s and 4s have been replaced.\n\nlabelled::user_na_to_na(mil_data$english_fluency_1)\n\n&lt;labelled&lt;double&gt;[164]&gt;: Please select which box best describes your English fluency. - How well can you speak English?\n  [1]  2  2  2  1  1  1  2  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [26]  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [51]  1  1  2  1  1  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  2  1  1  1  1\n [76]  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1\n[101]  1  1  2  1  1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[126]  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  1  1  1  1  1  1  2  1  1  1  1 NA  2\n\nLabels:\n value     label\n     1 Very well\n     2      Well\n\n\n\n\n\n\n\n\nTip\n\n\n\nSee the {labelled} vignette for more help on working with user-defined NAs, including how to deal with them when converting to other types."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#converting-variables",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#converting-variables",
    "title": "10: Qualtrics and Labelled Data",
    "section": "Converting Variables",
    "text": "Converting Variables\nThe labels have served their purpose helping us navigate and clean up the dataset, and produce a lovely data dictionary for sharing. However, if we want to use the data, we’ll need to convert to other data types that we can use for statistical analysis.\nThis will fall into two main categories. Any variables containing numbers that we want to do maths with, we’ll convert to numeric type, which we’ve encountered a few times before, which will strip the labels. However, any variables that contain categorical data, we’ll instead convert to factors, which we haven’t encountered in this series yet - so we’ll start there. Remember that variables that will be converted to factor should have labels for all of their levels, whereas variables that will be converted to numeric can have fewer labels, because we will stop using them after the numeric conversion.\n\nFactor\nFactor variables are R’s way of representing categorical data, which have a fixed and known set of possible values. Thus far we’ve mostly been using character data for this purpose, but that’s a bit of a cheat - R’s been helping us by treating the same values as the same category, but factors create this structure explicitly.\nAs may be familiar from SPSS, factors actually contain two pieces of information for each observation: levels and labels. Levels are the (existing or possible) values that the variable contains, whereas labels are very similar to the labels we’ve just been exploring.\nAs an example, take an example factor vector:\n\nfactor(c(1, 2, 1, 1, 2),\n       labels = c(\"Male\", \"Female\"))\n\n[1] Male   Female Male   Male   Female\nLevels: Male Female\n\n\nThe underlying values in the factor are numbers, here 1 and 2. The labels are applied to the values in ascending order of those values, so 1 becomes “Male”, “2” becomes “Female”, etc. Here, we haven’t need to specify the levels; if you don’t elaborate otherwise, R will assume that they are the same as the unique values.\nYou can also supply additional possible values, even if they haven’t been observed, using the levels argument:\n\nfactor(c(1, 2, 1, 1, 1),\n       levels = c(1, 2, 3),\n       labels = c(\"Male\", \"Female\", \"Non-binary\"))\n\n[1] Male   Female Male   Male   Male  \nLevels: Male Female Non-binary\n\n\n\n\n\n\n\n\nTip\n\n\n\nFactors are so common and useful in R that they have a whole {tidyverse} package to themselves! You already installed {forcats} with {tidyverse}, but you can check out the help documentation if you’d like to learn more about working with factors.\n\n\nThe useful thing about labelled data that it’s very easy to convert into factors, which is what R expects for many different types of analysis and plotting functions. Handy!\nFor an individual variable, we can use labelled::to_factor() to convert to factor.\n\n\n\n\n\n\nExercise\n\n\n\nConvert the gender variable to factor, although don’t assign this change to the dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmil_data |&gt; \n  dplyr::mutate(\n    gender_fct = labelled::to_factor(gender),\n    .keep = \"used\"\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\nIf you wanted a specific order of the levels, for plotting or similar, there’s also a sort_levels = argument described in the help documentation.\n\n\nNumeric\nFor continuous variables, we don’t need anything fancy to turn them into numeric data, because they technically already are. Instead, we just need to get rid of the labels using unclass().\n\n\n\n\n\n\nExercise\n\n\n\nUse unclass() to convert belonging_1 to numeric, although don’t assign the change to the dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis example shows both the conversion to numeric, and back to labelled with labelled::labelled().\n\nmil_data |&gt; \n  dplyr::mutate(\n    belonging_1_num = unclass(belonging_1),\n    belonging_1_lab = labelled::labelled(belonging_1_num),\n    .keep = \"used\"\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\nThe nice thing about this method is that we can now do maths with the unclassed numeric functions as normal, but the labels are still there if we want to get back - just convert with labelled::labelled().\n\n\nConditional Conversion\nThere are two main ways we could more efficiently convert variables than one by one. The first is offered by the {labelled} package - here I’ve just copied from the vignette describing the setup.\n\n\n\n\n\n\nNote\n\n\n\nIn most of cases, if data documentation was properly done, categorical variables corresponds to vectors where all observed values have a value label while vectors where only few values have a value label should be considered as continuous.\nIn that situation, you could apply the unlabelled() method to an overall data frame. By default, unlabelled() works as follows:\n\nif a column doesn’t inherit the haven_labelled class, it will be not affected;\nif all observed values have a corresponding value label, the column will be converted into a factor (using to_factor());\notherwise, the column will be unclassed (and converted back to a numeric or character vector by applying base::unclass()).\n\n\n\nIf we wanted to do this, we’d have a bit more work to do. That’s because at the moment, our data doesn’t line up with this template. Having a look at our data dictionary again, we can see that our subscale variables have all of their levels labelled, so they won’t be converted as we’d like. Rather than do this now, I’d probably recommend setting up your Qualtrics like this to begin with.\nInstead, we can take the second route and use what we’ve seen in previous Challenge tasks to convert variables conditionally.\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Convert categorical variables with labels to factors, and subscale variables to numeric.\nHint: Have a look back at dplyr::across() for efficient selecting and applying.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmil_data &lt;- mil_data |&gt;\n  dplyr::mutate(\n    across(global_meaning_1:last_col(),\n           unclass),\n    across(c(english_fluency_1, gender),\n           labelled::to_factor)\n  )\n\n\n1\n\nOverwrite mil_data by taking the existing dataset mil_data, and then\n\n2\n\nChange it as follows:\n\n3\n\nAcross all the variables from global_meaning_1 through the last column, convert to numeric\n\n4\n\nAcross the variables english_fluency_1 and gender, convert to factor.\n\n\n\n\n\n\n\n\n\n \nVery well done today. You should now have a data dictionary to refer to, and a complete dataset to work with using the techniques we’ve already covered to clean up responses, create subscales, and so on. Next time we’ll work on solving common issues and avoiding those issues in the first place by setting things up right to begin with."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#footnotes",
    "href": "tutorials/psychrlogy/03_improvRs/10_qtrics.html#footnotes",
    "title": "10: Qualtrics and Labelled Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the purposes of simplicity, we’re going to keep pretending that Likert and similar rating scales are “continuous”.↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/08_analysis.html",
    "href": "tutorials/psychrlogy/02_essentials/08_analysis.html",
    "title": "08: Analysis",
    "section": "",
    "text": "This tutorial is a speedrun of how to run and report a variety of analyses that are commonly used for undergraduate dissertation projects at the University of Sussex, based on a survey of supervisors in summer 2023.\n\n\n\n\n\n\nImportant\n\n\n\nThis is not a statistics tutorial, so interpretation of the following models will be only cursory. For a complete explanation of the statistical concepts and interpretation, refer to the {discovr} tutorials that correspond to the following sections:\n\nCategorical Predictors\n\nComparing Several Means (One-Way ANOVA): discovr_11\nFactorical Designs: discovr_13\nMixed Designs: discovr_16\n\nModeration: discovr_10\nMediation: discovr_10\n\n\n\n\n\n\n\n\n\nUsing the {discovr} tutorials\n\n\n\n\n\nProf Andy Field’s {discovr} tutorials provide detailed walkthroughs of both the R code and the statistical concepts of a variety of statistical analyses. They are a good place to look first to understand what your UG supervisees or advisees have been taught on a particular topic.\nTo install the tutorials, run the following in the Console. Note that this is not necessary for the live workshop Cloud workspaces, which already have the tutorials installed.\nif(!require(remotes)){\n  install.packages('remotes')\n}\n\nremotes::install_github(\"profandyfield/discovr\")\nThe {discovr} tutorials are built in {learnr}, an interactive platform for learning and running R code. So, unlike the tutorial you’re currently reading, they must be run inside an R session.\nTo start a tutorial, open any project and click on the Tutorial tab in the Environment pane. Scroll down to the tutorial you want and click the “Start Tutorial ▶️” button to load the tutorial.\nBecause {discovr} tutorials run within R, you don’t need to use any external documents; you can write and run R code within the tutorial itself. However, I strongly recommend that whenever you work with these tutorials, you write and run your code in a separate document, otherwise you will have no record of the code and output."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/08_analysis.html#overview",
    "href": "tutorials/psychrlogy/02_essentials/08_analysis.html#overview",
    "title": "08: Analysis",
    "section": "",
    "text": "This tutorial is a speedrun of how to run and report a variety of analyses that are commonly used for undergraduate dissertation projects at the University of Sussex, based on a survey of supervisors in summer 2023.\n\n\n\n\n\n\nImportant\n\n\n\nThis is not a statistics tutorial, so interpretation of the following models will be only cursory. For a complete explanation of the statistical concepts and interpretation, refer to the {discovr} tutorials that correspond to the following sections:\n\nCategorical Predictors\n\nComparing Several Means (One-Way ANOVA): discovr_11\nFactorical Designs: discovr_13\nMixed Designs: discovr_16\n\nModeration: discovr_10\nMediation: discovr_10\n\n\n\n\n\n\n\n\n\nUsing the {discovr} tutorials\n\n\n\n\n\nProf Andy Field’s {discovr} tutorials provide detailed walkthroughs of both the R code and the statistical concepts of a variety of statistical analyses. They are a good place to look first to understand what your UG supervisees or advisees have been taught on a particular topic.\nTo install the tutorials, run the following in the Console. Note that this is not necessary for the live workshop Cloud workspaces, which already have the tutorials installed.\nif(!require(remotes)){\n  install.packages('remotes')\n}\n\nremotes::install_github(\"profandyfield/discovr\")\nThe {discovr} tutorials are built in {learnr}, an interactive platform for learning and running R code. So, unlike the tutorial you’re currently reading, they must be run inside an R session.\nTo start a tutorial, open any project and click on the Tutorial tab in the Environment pane. Scroll down to the tutorial you want and click the “Start Tutorial ▶️” button to load the tutorial.\nBecause {discovr} tutorials run within R, you don’t need to use any external documents; you can write and run R code within the tutorial itself. However, I strongly recommend that whenever you work with these tutorials, you write and run your code in a separate document, otherwise you will have no record of the code and output."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/08_analysis.html#setup",
    "href": "tutorials/psychrlogy/02_essentials/08_analysis.html#setup",
    "title": "08: Analysis",
    "section": "Setup",
    "text": "Setup\n\nPackages\nThere are a lot of packages that we will make our way through today. The key ones are, naturally, {tidyverse}; {afex} for factorial designs; {ggrain} for raincloud plots; {lavaan} for mediation; and {papaja} for pretty formatting and reporting.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(afex)\nlibrary(ggrain)\nlibrary(lavaan)\nlibrary(papaja)\n\n\n\n\n\n\n\n\nData\nToday we’re continuing to work with the dataset courtesy of fantastic Sussex colleague Jenny Terry. This dataset contains real data about statistics and maths anxiety. For these latter two tutorials, I’ve created averaged scores for each overall scale and subscale, and dropped the individual items.\n\n\n\n\n\n\nExercise\n\n\n\nRead in the dataset and save it in a new object, anx_data.\nOn the Cloud, you can read in this dataset from the data folder using here::here().\nElsewhere, you can download the dataset, or copy the dataset URL, from the Data and Workbooks page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRead in from file:\n\nanx_data &lt;- readr::read_csv(here::here(\"data/anx_scores_data.csv\"))\n\nRead in from URL:\n\nanx_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/anx_data.csv\")\n\n\n\n\n\n\n\nCodebook\nThere’s quite a bit in this dataset, so you will need to refer to the codebook below for a description of all the variables.\n\n\n\n\n\n\nDataset Info Recap\n\n\n\n\n\nThis study explored the difference between maths and statistics anxiety, widely assumed to be different constructs. Participants completed the Statistics Anxiety Rating Scale (STARS) and Maths Anxiety Rating Scale - Revised (R-MARS), as well as modified versions, the STARS-M and R-MARS-S. In the modified versions of the scales, references to statistics and maths were swapped; for example, the STARS item “Studying for an examination in a statistics course” became the STARS-M item “Studying for an examination in a maths course”; and the R-MARS item “Walking into a maths class” because the R-MARS-S item “Walking into a statistics class”.\nParticipants also completed the State-Trait Inventory for Cognitive and Somatic Anxiety (STICSA). They completed the state anxiety items twice: once before, and once after, answering a set of five MCQ questions. These MCQ questions were either about maths, or about statistics; each participant only saw one of the two MCQ conditions.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor learning purposes, I’ve randomly generated some additional variables to add to the dataset containing info on distribution channel, consent, gender, and age. Especially for the consent variable, don’t worry: all the participants in this dataset did consent to the original study. I’ve simulated and added this variable in later to practice removing participants.\n\n\n\n\n\n\n\nVariable\nType\nDescription\nValues\n\n\n\n\nid\nCategorical\nUnique ID code\nNA\n\n\ndistribution\nCategorical\nChannel through which the study was completed, either as a preview (before real data collection) or anonymous genuine responses. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\"preview\" or \"anonymous\"\n\n\nconsent\nCategorical\nWhether the participant read and consented to participate. Note that this variable has been randomly generated and does NOT reflect genuine responses; all participants in this dataset did originally consent to participate.\n\"Yes\" or \"No\"\n\n\ngender\nCategorical\nGender identity. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\"female\", \"male\", \"non-binary\", or \"other/pnts\". \"pnts\" is an abbreviation for \"Prefer not to say\".\n\n\nage\nNumeric\nAge in years. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n18 - 99\n\n\nmcq\nCategorical\nIndependent variable for MCQ question condition, whether the participant saw MCQ questions about mathematics or statistics.\n\"maths\" or \"stats\"\n\n\nstars_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Statistics Anxiety Rating Scale (STARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_int_score\nNumeric\nAveraged score on the Interpretation Anxiety subscale of the Statistics Anxiety Rating Scale (STARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_help_score\nNumeric\nAveraged score on the Asking for Help subscale of the Statistics Anxiety Rating Scale (STARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_m_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Statistics Anxiety Rating Scale - Maths (STARS-M), a modified version of the STARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_m_int_score\nNumeric\nAveraged score on the Interpretation Anxiety subscale of the Statistics Anxiety Rating Scale - Maths (STARS-M), a modified version of the STARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_m_help_score\nNumeric\nAveraged score on the Asking for Help subscale of the Statistics Anxiety Rating Scale - Maths (STARS-M), a modified version of the STARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Revised Maths Anxiety Rating Scale (R-MARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_num_score\nNumeric\nAveraged score on the Numerical Task Anxiety subscale of the Revised Maths Anxiety Rating Scale (R-MARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_course_score\nNumeric\nAveraged score on the Course Anxiety subscale of the Revised Maths Anxiety Rating Scale (R-MARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_s_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Revised Maths Anxiety Rating Scale - Statistics (R-MARS-S), a modified version of the MARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_s_num_score\nNumeric\nAveraged score on the Numerical Anxiety subscale of the Revised Maths Anxiety Rating Scale - Statistics (R-MARS-S), a modified version of the MARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_s_course_score\nNumeric\nAveraged score on the Course Anxiety subscale of the Revised Maths Anxiety Rating Scale - Statistics (R-MARS-S), a modified version of the MARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nsticsa_trait_score\nNumeric\nAveraged score on the Trait Anxiety subscale of the State-Trait Inventory for Cognitive and Somatic Anxiety.\n1 (not at all) to 4 (very much so)\n\n\nsticsa_pre_state_score\nNumeric\nAveraged score on the State Anxiety subscale of the State-Trait Inventory for Cognitive and Somatic Anxiety, pre-MCQ.\n1 (not at all) to 4 (very much so)\n\n\nsticsa_post_state_score\nNumeric\nAveraged score on the State Anxiety subscale of the State-Trait Inventory for Cognitive and Somatic Anxiety, post-MCQ.\n1 (not at all) to 4 (very much so)\n\n\nmcq_score\nNumeric\nTotal (summed) score on the MCQ questions.\n0 (all incorrect) to 5 (all correct)\n\n\n\n\n\n\n\nIf you have some experience with R, you are welcome to instead use another dataset that you are familiar with or are keen to explore. However, remember that anything you upload to Posit Cloud is visible to all workspace admins, so keep GDPR in mind."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/08_analysis.html#categorical-predictors",
    "href": "tutorials/psychrlogy/02_essentials/08_analysis.html#categorical-predictors",
    "title": "08: Analysis",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\nWe’ll begin by looking at several examples of models with categorical predictors. Traditionally, these are all different types of “ANOVA” (ANalysis Of VAriance), but research methods teaching at Sussex teaches all of these models in the framework of the general linear model.\n\nComparing Several Means\nOur first model will be linear model with a categorical predictor with more than two categories - traditionally a “one-way independent ANOVA”. For our practice today, we’ll look at the differences in the STARS Asking for Help subscore by gender.\n\n\n\n\n\n\nTip\n\n\n\nThis section is derived from discovr_11, which also has more explanations and advanced techniques.\n\n\n\nPlot\nLet’s start by visualising our variables. The discovr tutorial also includes code for a table of means and CIs, which we won’t cover here.\n\n\n\n\n\n\nExercise\n\n\n\nUse what we covered in the last tutorial to create a violin or raincloud plot (your choice) with means and CIs. Optionally, spruce up your plot with labels and a theme.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  ggplot2::ggplot(aes(x = gender, y = stars_help_score)) + \n  geom_rain() + ## or swap out for geom_violin()!\n  stat_summary(fun.data = \"mean_cl_boot\") +\n  ## here's the sprucing\n  labs(x = \"Gender Identity\", y = \"STARS Asking for Help Anxiety Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nFit the Model\nAs mentioned above, this model is taught as a linear model with a categorical predictor. That’s literal: we’re going to use the lm() function to fit the model. It was a bit ago, but we covered the linear model and lm() function in more depth back in Tutorial 04.\nUnfortunately, we’ll need to make a quick change to our data beforehand.\n\n\n\n\n\n\nExercise\n\n\n\nChange out the “/” in the value other/pnts for an underscore, so the value reads other_pnts.\nHint: You can try stringr::str_replace_all() for a {tidyverse} solution, or if you’re basic like me, have a look at gsub().\n\n\n\n\n\n\nSolution\n\n\n\n\n\n{stringr} solution:\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    gender = stringr::str_replace_all(gender, \n                                      pattern = \"/\", \n                                      replacement = \"_\")\n  )\n\ngsub() solution:\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    gender = gsub(\"/\", \"_\", gender)\n  )\n\nThese will do exactly the same thing, so which one you use is completely preference. It’s very useful to know how to do this sort of thing though!\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFollow the steps below to fit and examine the model.\n\n\nNext up, we need to fit the model. There’s absolutely no difference between this and what we did previously in Tutorial 04, at all, so let’s use the same technique to fit a linear model with STARS Asking for Help subscale score as the outcome and gender as the predictor, and save this model as stars_lm.\n\nstars_lm &lt;- lm(stars_help_score ~ gender, data = anx_data)\n\nNext, again just as we did previously, we can get model fit statistics and an F ratio for the model using broom::glance().\n\nbroom::glance(stars_lm)\n\n\n\n  \n\n\n\nIn discovr_11, though, we see that we can use the anova() function as well. We used this previously to compare models, but given only one model, it will instead compare that model to the null model (i.e. the mean of the outcome). The tutorial also provides code to get an omega effect size.\n\nanova(stars_lm) |&gt; \n  parameters::model_parameters(effectsize_type = \"omega\")\n\n\n\n  \n\n\n\nFinally, we can get a look at the model parameters using broom::tidy(), and see what R has done about our categorical predictor.\n\nbroom::tidy(stars_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nTo read this output, we need to know a few things about how R deals with categorical predictors by default.\nUnless we specify comparisons/contracts, lm() will by default fit a model with the first category as the baseline. What do we mean by the “first”? Here, the first alphabetically. Our categories were female, male, non-binary, and other_pnts, so the first is “female”. (You can see in the plot as well that the categories have been automatically ordered this way.) This means that the intercept represents the mean of the outcome in the baseline group - here, the mean Asking for Help anxiety score in female participants.\nThe three other three parameter estimates - gendermale, gendernon-binary, and genderother_pnts - each represent the difference in the mean of the outcome between the baseline category and each other category. For example, the b estimate and the rest of the statistics on the gendermale row represent the difference in mean Asking for Help anxiety between female and male participants. The next, gendernon-binary, represents the difference in mean Asking for Help anxiety between female and non-binary participants, and so on.\n\n\nContrasts\nWhen they cover this week in second year, UGs are taught to manually write orthogonal contrast weights. We are not going to do that but the whole process is described in the {discovr} tutorial if that’s something you want to do.\nInstead, we’re going to use built-in contrasts. The structure generally looks like this:\ncontrasts(dataset_name$variable_name) &lt;- contr.*(...)\nOn the left side of this statement, we’re using the contrasts() function to set the contrasts for a particular variable. Notice this is the variable in the dataset, which we’re specifying using $ notation as we’ve seen before. To do this, we assign a pre-set series of contrasts using one of the contr.*() family of functions. Here, the * represents one of a few options, such as contr.sum() (the one we’ll use now), contr.helmert(), etc., and the ... represents some more arguments you may need to add to this function, depending on which one you want. You can get information about all of them by pulling up the help documentation on any of them.\nFor now, we’re going to use contr.sum(), which requires that we also provide the number of levels (here, four: female, male, non-binary, and other_pnts). There’s a catch, though - we can only set contrasts for factors, so we’ll first need to do something about the gender variable.\n\n\n\n\n\n\nExercise\n\n\n\nFirst, change the character gender variable into a factor. You can do this just for this variable, or optionally use some of the challenge functions from the last tutorials to change all of the character variables into factors together.\nThen, use contr.sum() to set the contrasts for the gender variable.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWithout any fancy extras, we can just change gender into a factor directly - don’t forget to assign the change back to the dataset.\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    gender = factor(gender)\n  )\n\nTo make sure we don’t have to deal with this again, we can change all of the character variables into factors with the across() + where() tag team from the last tutorial.\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    across(where(is.character), factor)\n  )\n\nNow that gender is a factor, we can set the contrasts.\n\ncontrasts(anx_data$gender) &lt;- contr.sum(4)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFit the model again and get the same parameters table as before.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBecause the underlying gender variable has changed, all we need to do is rerun the exact same code as before, which will now produce output with contrasts.\nJust so I can compare them if I want to, I’m going to call this model something different.\n\nstars_contr_lm &lt;- lm(stars_help_score ~ gender, data = anx_data)\n\npapaja::apa_table(papaja::apa_print(stars_contr_lm)$table)\n\n\n(#tab:unnamed-chunk-16)\n\n\n**\n\n\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n3.01\n[2.79, 3.22]\n27.05\n461\n&lt; .001\n\n\nGender1\n-0.24\n[-0.48, -0.01]\n-2.01\n461\n.045\n\n\nGender2\n0.19\n[-0.10, 0.49]\n1.29\n461\n.199\n\n\nGender3\n0.28\n[-0.32, 0.87]\n0.92\n461\n.360\n\n\n\n\n\n\n\n\n\n\n\n\nPost-Hoc Tests\nIf we didn’t have an a priori prediction about the differences between groups, we may decide to instead run post-hoc tests. We can use the function modelbased::estimate_contrasts() to get post-hoc tests by putting in the original model (before we set the contrasts) and telling it which variable to produce the tests for. Confusingly, this argument is also called contrast = !\n\nmodelbased::estimate_contrasts(stars_lm, \n                               contrast = \"gender\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWhat about ANCOVA?\n\n\n\nThe procedures for including a covariate in the linear model aren’t too complex, but the additional work to be done - testing independence of the covariate, changing the type of F-statistic, checking homogeneity of regression slopes, and so on - are just a bit much for this tutorial. If you are Keen to include a covariate, check out the very thorough discovr_12 for a detailed walkthrough.\n\n\n\n\n\nFactorial Designs\nOur next analysis will be factorial design with two categorical predictors, traditionally called a 2x2 independent ANOVA. Although this can certainly be done using lm(), at this point the lm() route becomes a bit burdensome, so we will instead switch to using the {afex} package, which is designed especially for these types of analyses (“afex” stands for “analysis of factorial experiments”). The {discovr} tutorials for this and subsequent similar ANOVA-type analyses demonstrate both {afex} and lm() options, but students are only required to use {afex}.\nFor our practice today, we’ll look at again at gender, but this time with male and female participants only, and we’ll compare whether their post-MCQ state anxiety scores are different depending on whether have high or low trait anxiety.\n\n\n\n\n\n\nTip\n\n\n\nThis section is derived from discovr_13, which also has more explanations and advanced techniques.\n\n\n\nFit the Model\nFirst we need to make a change to our dataset - namely, creating the sticsa_trait_cat variable we looked at last week as well.\n\n\n\n\n\n\nExercise\n\n\n\nRecode the existing sticsa_trait_score variable into a categorical variable, with “high” values of 2.5 or greater, and “low” values below 2.5. Don’t forget to save this change to your dataset!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExpensive and sophisticated dplyr::case_when() option:\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_cat = dplyr::case_when(\n      sticsa_trait_score &gt;= 2.5 ~ \"high\", \n      sticsa_trait_score &lt; 2.5 ~ \"low\",\n      .default = NA)\n  )\n\nCheap and cheerful ifelse() option:\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_cat = ifelse(sticsa_trait_score &gt;= 2.5, \"high\", \"low\")\n  )\n\n\n\n\n\n\nWith our categorical predictor in place, we can now fit our model.\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the code below to fit and interpret the model.\n\n\nThe function we’ll use for this and the next analysis as well is afex::aov_4(). The arguments to this function will look extremely familiar - in fact, it looks just about exactly like the way we used the lm() function. The only new thing is the extra element at the end of the function, + (1|id). We can ignore it for now, although we’ll come back to it in the mixed design analysis. The main thing to notice is that if your dataset doesn’t have one, you’ll need a variable that contains ID codes or numbers of some type to use here. This dataset conveniently has anonymous alphanumeric ID codes in a variable called id, but if yours doesn’t, you’d have to add or generate them.\n\n## Store the model\nmcq_afx &lt;- afex::aov_4(sticsa_post_state_score ~ mcq*sticsa_trait_cat + (1|id), \n                       data = anx_data)\n## Print the model\nmcq_afx\n\nAnova Table (Type 3 tests)\n\nResponse: sticsa_post_state_score\n                Effect     df  MSE          F  ges p.value\n1                  mcq 1, 459 0.28       0.68 .001    .412\n2     sticsa_trait_cat 1, 459 0.28 202.09 *** .306   &lt;.001\n3 mcq:sticsa_trait_cat 1, 459 0.28       0.50 .001    .482\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nSo it seems that there’s a significant main effect of anxiety (no surprise there), but no interaction with MCQ type.\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Imagine you were working with a dataset that didn’t already have ID codes. How could you generate a new id_code variable to use in the {afex} formula, that has a unique value for each row?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are so many ways to do this, but here’s two quick ones.\nFirst, you could simply generate a new variable containing the numbers 1 through as many rows as there are in the dataset.\n\nanx_data |&gt; \n  dplyr::mutate(\n    id_code = 1:dplyr::n(),\n    .before = id\n  )\n\n\n\n  \n\n\n\nSecond, you could use a function to convert the row numbers of the dataset into a variable:\n\nanx_data |&gt; \n  tibble::rowid_to_column(var = \"id_code\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nInteraction Plot\nBesides producing easy-to-read output, one of the handy things about {afex} is that you can quickly generate interaction plots. The afex_plot() function lets you take a model you’ve produced with {afex} and visualise it easily. Since we have one of those to hand (convenient!) let’s have a look.\nMinimally, we need to provide the model object, the variable to put on the x-axis, and the “trace”, the variable on separate lines. Unlike with {ggplot}, though, here we need to provide the names of those variables as strings inside quotes.\n\nafex::afex_plot(mcq_afx,\n                x = \"mcq\",\n                trace = \"sticsa_trait_cat\")\n\n\n\n\nThe plot we get for very little work already has a lot of useful stuff. We get means for each combination of groups, with different values on the “trace” variable with different point shapes and connected by different lines. It needs some work, of course, but especially as a quick glimpse at the relationships of interest, it’s a good start!\n\n\n\n\n\n\nExercise\n\n\n\nGenerate the plot.\nCHALLENGE: Use the afex_plot() help documentation and last week’s tutorial on {ggplot2} to clean up this plot and make it presentable.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere’s what I went for! There’s a couple small bits left, like the labels on the legend, that I’ll leave to you to work out.\n\nafex::afex_plot(mcq_afx, \"mcq\", \"sticsa_trait_cat\",\n                legend_title = \"STICSA Trait\\nAnxiety\",\n                factor_levels = list(\n                  sticsa_cat = c(high = \"High\",\n                                 low = \"Low\")),\n                data_arg = list(\n                  position = position_jitterdodge()\n                )\n                ) +\n  scale_x_discrete(name = \"Type of MCQ\", \n                     labels = c(\"Maths\", \"Stats\")) +\n  scale_y_continuous(name = \"STICSA Post-Test State Anxiety\",\n                     limits = c(1,4),\n                     breaks = 1:4) +\n  papaja::theme_apa()\n\n\n\n\n\n\n\n\n\n\n\nEMMs\nIn addition to the plot, we can get get the actual means for each combination of categories. As with everything, there’s a package for it! Just like with afex_plot(), we provide the model object and the relevant variables to calculate means for.\n\nemmeans::emmeans(mcq_afx, c(\"mcq\", \"sticsa_trait_cat\"))\n\n mcq   sticsa_trait_cat emmean     SE  df lower.CL upper.CL\n maths high               2.34 0.0648 459     2.21     2.47\n stats high               2.26 0.0608 459     2.14     2.38\n maths low                1.54 0.0411 459     1.46     1.62\n stats low                1.54 0.0427 459     1.45     1.62\n\nConfidence level used: 0.95 \n\n\n\n\n\n\n\n\nExercise\n\n\n\nGenerate the table of estimated marginal means.\nCHALLENGE: Turn this estimated marginal means table into a nicely formatted table using kbl() (and, if you feel up for it, pivot_wider()).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe only information I really want out of this table are the means and confidence intervals. Instead of keeping all the other info, I decided to combine the means and CIs into a single variable using paste0(). The rest is just rearranging, relabelling, and making things look good!\n\nemmeans::emmeans(mcq_afx, c(\"mcq\", \"sticsa_trait_cat\")) |&gt; \n  ## convert to tibble to work with more easily\n  tibble::as_tibble() |&gt; \n  ## concatenate all the info into a new variable and reformat mcq\n  dplyr::mutate(\n    mean_ci = paste0(round(emmean, 2), \" \",\n                \"[\", round(lower.CL, 2), \n                \", \", round(upper.CL, 2), \"]\"),\n    mcq = stringr::str_to_title(mcq)\n  ) |&gt; \n  ## keep only categorical vars and new one\n  dplyr::select(mcq, sticsa_trait_cat, mean_ci) |&gt; \n  ## reshape\n  tidyr::pivot_wider(\n    names_from = sticsa_trait_cat,\n    values_from = mean_ci\n  ) |&gt; \n  ## reorder\n  dplyr::select(mcq, low, high) |&gt; \n  kableExtra::kbl(\n    col.names = c(\"MCQ Condition\", \"Low Anxiety\", \"High Anxiety\"),\n    align = \"c\",\n    caption = \"STICSA Post-Test State Anxiety means and 95% confidence intervals\"\n  ) |&gt; \n  kableExtra::kable_classic() |&gt; \n  kableExtra::add_header_above(header = c(\" \" = 1, \n                               \"STICSA Trait Anxiety\" = 2))\n\n\nSTICSA Post-Test State Anxiety means and 95% confidence intervals\n\n\n\n\n\n\n\n\n\nSTICSA Trait Anxiety\n\n\n\nMCQ Condition\nLow Anxiety\nHigh Anxiety\n\n\n\n\nMaths\n1.54 [1.46, 1.62]\n2.34 [2.21, 2.47]\n\n\nStats\n1.54 [1.45, 1.62]\n2.26 [2.14, 2.38]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Effects\nFinally, we can get tests of the effect between the levels of one variable, within each level of the other. Using the emmeans::joint_tests() function, we can use the model object and the name of the variable we want to split by to get tests within each level of that variable.\n\nemmeans::joint_tests(mcq_afx, \"mcq\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProduce the same output but the other way round, i.e. for the trait anxiety predictor.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nemmeans::joint_tests(mcq_afx, \"sticsa_trait_cat\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nMixed Designs\n\n\n\n\n\n\nTip\n\n\n\nThis section is derived from discovr_16, which also has more explanations and advanced techniques.\n\n\nFor our final ANOVA-y analysis for today, we’ll look at mixed designs - that is, models with categorical predictors that include at least one independent-measures variable and at least one repeated-measures variable. This is exactly what we’ll do, using STICSA state anxiety timepoint (pre and post) and MCQ type to predict STICSA state anxiety scores. But before we do, we need to look at something we’ve been dancing around in the challenge tasks: at long last, it’s time to tackle pivoting.\n\nReshaping with pivot_*()\nFor our upcoming analysis, we need to change the way our data are organised in the dataset. We have a repeated measures variable that is currently measured in two variables, namely sticsa_pre_state_score and sticsa_post_state_score. This is how other programmes, like SPSS, also represent repeated measures data, but this is not what we need for R. Instead, we need to reshape our data, so that instead of two separate variables, each with state anxiety scores, we have a single sticsa_state_score variable containing all the scores, with a separate sticsa_time coding variable telling us which timepoint each score comes from, “pre” or “post”. When we’re done, each participant will have two rows in the dataset.\nReshaping in {tidyverse} is typically done with the tidyr::pivot_*() family of functions, which have two star players: pivot_wider() and pivot_longer(). We use generally pivot_wider() when we want to turn rows into columns, and pivot_longer() when we want to turn columns into rows. Here, we want to turn columns into rows, and our dataset will actually get longer (we’ll have twice the rows as before!), so it’s pivot_longer() we need.\n\n\n\n\n\n\nPartying with pivot\n\n\n\nNeed help with pivoting? Me too, like all the time. Run vignette(\"pivot\") in the Console for an absolute indispensable guide through both types of pivoting with a wealth of easily adaptable examples and code.\n\n\nFor this analysis, I’m going to start by select()ing only the variables I actually need for the analysis, just to keep things as simple as possible. Next up is pivot_longer() to reshape the dataset. Minimally we have to specify the columns to pivot (using &lt;tidyselect&gt;, of course!), and where the names and values go to.\nIn this case, the columns I want to pivot are the ones containing “state” - that is, sticsa_pre_state_score and sticsa_post_state_score. The names of these variables are moved to a new variable, sticsa_time, using the names_to = argument - this variable now contains those names as values about timepoint (pre or post). The values that those variables previously contained, which for both variables were STICSA state anxiety scores, are moved to a new variable, sticsa_state_score, using the values_to = argument. Looking at the id and other variables, we can see that, as expected, there are now two rows for each participant, one “pre” and one “post”. Result!\n\nanx_data |&gt; \n  dplyr::select(id, contains(c(\"sticsa\", \"mcq\"))) |&gt; \n  tidyr::pivot_longer(cols = contains(\"state\"),\n                      names_to = \"sticsa_time\",\n                      values_to = \"sticsa_state_score\")\n\n\n\n  \n\n\n\nI’d suggest one final step, if you’re willing to wander dangerously close to regular expressions. The final names_pattern argument allows me to specify a portion of the variable names to be moved in names_to, instead of the entire variable name. We can see that the names sticsa_pre_state_score and sticsa_post_state_score contain only one important element: either “pre” or “post”, between the first and second underscores. The brackets in the regular expression capture this portion of each name and keep only that string in the new sticsa_time variable, which makes the new variable much easier to work with and read.\n\nanx_data |&gt; \n  dplyr::select(id, contains(c(\"sticsa\", \"mcq\"))) |&gt; \n  tidyr::pivot_longer(cols = contains(\"state\"),\n                      names_to = \"sticsa_time\",\n                      values_to = \"sticsa_state_score\",\n                      names_pattern = \".*?_(.*?)_.*\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nReshape the dataset as above and assign to a new dataset name, anx_data_long, lest we overwrite all our data with this much smaller and reshaped dataset!\nHint: It’s fine to copy the code above, especially if you’re short on time, but you’ll learn and understand the code much better if you type it out instead of copying it.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data_long &lt;- anx_data |&gt; \n  dplyr::select(id, contains(c(\"sticsa\", \"mcq\"))) |&gt; \n  tidyr::pivot_longer(cols = contains(\"state\"),\n                      names_to = \"sticsa_time\",\n                      values_to = \"sticsa_state_score\",\n                      names_pattern = \".*?_(.*?)_.*\")\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: How can you read the regular expression .*?_(.*?)_.*? If you’re not sure, try putting into regex101.com for some help (without the quotes).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nDisclaimer: This is an extremely nontechnical explanation, and is meant to capture the gist of what the regex does (as I understand it!) rather than to be technically accurate.\nThis expression really only contains four elements: .*?, .*, (), and _. The last one, the underscore, is just a literal underscore, so all we need to worry about are the other three.\n. is a very handy token in regex that means “any character” (except line terminators). Literally anything: letters, numbers, punctuation, symbols, whitespace, etc. * is a quantifier, expressing how many of the preceding element to match, and means “match between zero and unlimited times”. So, we can read .* as “any character any number of times, including none.” * is a “greedy” quantifier, which means it will match as many times as possible.\nSo, what about .*?? The only new element is the ?, which combines with * to create a “lazy” quantifier, matching as few times as possible. So, we can read .*? as “any character any number of times, including none, but as few as possible.”\nFinally, the brackets () are a capturing group - a portion of the expression we want to single out. In this case, it denotes the portion of the string that we want to actually use as the data in our new sticsa_time variable.\nSo how can we read .*?_(.*?)_.*? Let’s compare to an example string, sticsa_pre_state_score.\n\n.*?_ matches any number of characters as few times as possible until it encounters an underscore. This captures the sticsa_ portion of the example string.\n(.*?)_ matches any number of characters as few times as possible until it encounters the next underscore. This matches the pre_ portion of the example string, with the brackets singling out the capturing group of interest, namely the string “pre”.\n.* matches any number of characters for the rest of the string. This matches the state_score portion of the example string.\n\nSo, why didn’t I type \"sticsa_(.*)_state_score\"? It would have worked just as well (try it!). In a word, generalisability. .*?_(.*?)_.* will get the element between the first and second underscores in any string, whereas \"sticsa_(.*)_state_score\" will only work for these particular variables. That’s a matter of personal choice and style, but I almost always prefer the generalisable way if possible, because I know it’ll be easier to adapt the next time round.\nIf you like this kind of thing, bookmark regex101 and consider reading vignette(\"regular-expressions\")!\n\n\n\n\n\n\n\nFit the Model\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the code below to fit and interpret the model.\n\n\nTo fit our model, we’ll use a very similar formula that was just saw for factorial designs. We’re using the new sticsa_state_score as the outcome, with mcq (the independent-measures variable) and sticsa_time (the repeated-measures variable), and their interaction, as predictors. The key difference is in the last element of our formula, which is now + (sticsa_time|id). In other words, I’ve replaced the 1 in + (1|id) with the repeated element(s) of the model.\n\n# fit the model:\nanx_mcq_afx &lt;- anx_data_long |&gt; \n  afex::aov_4(sticsa_state_score ~ mcq*sticsa_time + (sticsa_time|id),\n              data = _)\n\nanx_mcq_afx\n\nAnova Table (Type 3 tests)\n\nResponse: sticsa_state_score\n           Effect     df  MSE      F   ges p.value\n1             mcq 1, 461 0.68   0.60  .001    .438\n2     sticsa_time 1, 461 0.08   0.01 &lt;.001    .907\n3 mcq:sticsa_time 1, 461 0.08 4.28 *  .001    .039\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\n\n\nMain Effects\nWe can see from the output above that neither main effects are significant. However, that might not always be the case, so there’s a couple ways we can investigate the effect.\nThe first is to get estimated marginal means (EMMs) for all levels of the main effect, which we can do using the emmeans::emmeans() function. It might be useful to save the output in a new object so you can refer to (or report!) it easily.\nSecond, we can visualise the effect easily using afex::afex_plot() and only specifying one variable on the x-axis. I haven’t worried about any additional zhooshing, except to whack on a theme.\n\nmcq_emm &lt;- emmeans::emmeans(anx_mcq_afx, ~mcq, model = \"multivariate\")\nmcq_emm\n\n mcq   emmean     SE  df lower.CL upper.CL\n maths   1.75 0.0383 461     1.68     1.83\n stats   1.79 0.0386 461     1.72     1.87\n\nResults are averaged over the levels of: sticsa_time \nConfidence level used: 0.95 \n\nafex::afex_plot(anx_mcq_afx,\n                x = \"mcq\") +\n  papaja::theme_apa()\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFollowing the example above, get EMMs and a plot for the main effect of sticsa_time.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can almost exactly adapt the code above, but watch out for the warning on the plot:\nWarning: Panel(s) show within-subjects factors, but not within-subjects error bars.\nFor within-subjects error bars use: error = \"within\"\nThis seems very sensible!\n\ntime_emm &lt;- emmeans::emmeans(anx_mcq_afx, ~sticsa_time, model = \"multivariate\")\ntime_emm\n\n sticsa_time emmean     SE  df lower.CL upper.CL\n pre           1.77 0.0281 461     1.72     1.83\n post          1.77 0.0295 461     1.72     1.83\n\nResults are averaged over the levels of: mcq \nConfidence level used: 0.95 \n\nafex::afex_plot(anx_mcq_afx,\n                x = \"sticsa_time\",\n                error = \"within\") +\n  papaja::theme_apa()\n\n\n\n\n\n\n\n\n\n\n\nTwo-Way Interactions\nThe first thing we might want to do with an interaction is break it apart to understand what’s happening. We can start out with this by getting a table of EMMs just like we did above. These are slightly more interesting than those above, but only just (I can’t make much of these without a plot!).\nWe could also use emmeans::estimate_contrast() to break down the interaction…except that there’s only the one 2x2 interaction here, so all this gives us is an identical result to the interaction effect in the overall model. For other designs with more levels, the output compares 2x2 combinations of conditions to better understand what’s happening.\n\nanx_mcq_emm &lt;- emmeans::emmeans(anx_mcq_afx, ~c(mcq, sticsa_time), model = \"multivariate\")\nanx_mcq_emm\n\n mcq   sticsa_time emmean     SE  df lower.CL upper.CL\n maths pre           1.73 0.0395 461     1.65     1.81\n stats pre           1.81 0.0398 461     1.73     1.89\n maths post          1.77 0.0416 461     1.69     1.85\n stats post          1.78 0.0419 461     1.69     1.86\n\nConfidence level used: 0.95 \n\nemmeans::contrast(\n  anx_mcq_emm,\n  interaction = c(mcq = \"trt.vs.ctrl\", sticsa_time = \"trt.vs.ctrl\"),\n  ref = 2,\n  adjust = \"holm\"\n  )\n\n mcq_trt.vs.ctrl sticsa_time_trt.vs.ctrl estimate     SE  df t.ratio p.value\n maths - stats   pre - post               -0.0784 0.0379 461  -2.070  0.0390\n\n\nBut of course, the main thing we want to see is the interaction plot! Just as we did before, we can use afex_plot() to quickly get the bones of the plot and finally see what the hell is going on.\n\nafex::afex_plot(anx_mcq_afx,\n                x = \"sticsa_time\",\n                trace = \"mcq\",\n                error = \"none\") +\n  papaja::theme_apa()\n\n\n\n\nBefore we go too far into zhooshing this plot, which we’ve already had a go at, let’s have a look at {papaja}’s entry into the automatically-making-cool-plots race. To get started, we’ll set up a plot giving the data, ID variable, dependent variable (outcome), and factors (predictors):\n\npapaja::apa_beeplot(\n  data = anx_data_long,\n  id = \"id\",\n  dv = \"sticsa_state_score\",\n  factors = c(\"sticsa_time\", \"mcq\")\n)\n\n\n\n\nNot bad for a first effort! I like that this version shows how the points are dispersed. I don’t like a good few other things: the weird axes, the strange amount of whitespace, the lack of connecting lines to help judge the interaction.\nWhich one to use? Honestly, it depends on personal preference, the data, the journal requirements, and how comfortable you feel with each function.\n\n\n\n\n\n\nExercise\n\n\n\nGenerate the plot with papaja::apa_beeplot().\nCHALLENGE: Zhoosh up this plot to get it at least within spitting distance of nicely formatted.\nHint: Use the {papaja} help documentation!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npapaja::apa_beeplot(\n  data = anx_data_long,\n  id = \"id\",\n  dv = \"sticsa_state_score\",\n  factors = c(\"sticsa_time\", \"mcq\"),\n  dispersion = within_subjects_conf_int,\n  ylab = \"STICSA State Anxiety Score\",\n  xlab = \"Timepoint\",\n  args_x_axis = list(\n    labels = c(\"Post\", \"Pre\")\n  ),\n  args_legend = list(\n    title = \"MCQ\",\n    legend = c(\"Maths\", \"Stats\")\n  )\n)"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/08_analysis.html#continuous-predictors",
    "href": "tutorials/psychrlogy/02_essentials/08_analysis.html#continuous-predictors",
    "title": "08: Analysis",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nRight, had enough of factors? Me too. We’re moving on to models with categorical predictors, which we cover in two broad areas: mediation and moderation.\n\nMediation\n\n\n\n\n\n\nTip\n\n\n\nThis section is derived from discovr_10, which also has more explanations and advanced techniques.\n\n\nMediation models follow a classic structure. A predictor, X, has some relationship with the outcome, Y; this is the direct effect, c. The mediation route is the hypothesised mechanism by way of which X has its relationship in Y, either in whole or in part. The a and b paths in the model below represent the mediation route via the mediator, M.\n\n\n\n\n\nflowchart LR\n      X -- Path a --&gt; M[M: Mediator] -- Path b--&gt; Y\n      X[X: Predictor] -- Direct Effect c --&gt; Y[Y: Outcome]\n\n\nFigure 1: Mediation diagram\n\n\n\n\nFor our example today, we’ll use MCQ score as our outcome of interest, Y. I’ll suggest that how anxious you are about numerical topics impacts MCQ score, so RMARS numerical anxiety score will be our predictor, X. However, it could be that this numerical anxiety makes you more anxious right before you take an MCQ test about numerical topics, which then impacts MCQ score. So, the mediator, M, will be STICSA pre-test state anxiety score.\n\n\n\n\n\nflowchart LR\n      X -- Path a--&gt; M[M: STICSA Pre-Test State Anxiety] -- Path b--&gt; Y\n      X[X: R-MARS Numerical Anxiety] -- Direct Effect c--&gt; Y[Y: MCQ score]\n\n\nFigure 2: Mediation diagram for the MCQ example\n\n\n\n\nAll clear? Brill. That’s a good thing because…\n\nFit the Model\n…now we have to use it to fit the model.\nThe method we’ll use for this is a bit different than everything we’ve done so far. Namely, in order to make the output interpretable, we’ll create some text that will express the relationships that we’re interested in. It’s very easy doing this to lose track of what goes where, so that’s why it’s so useful to have a clear diagram to work with. (You’re welcome!)\nThe general form of this specification looks like this:\n\nmodel &lt;- 'outcome ~ c*predictor + b*mediator\n                   mediator ~ a*predictor\n                   indirect_effect := a*b\n                   total_effect := c + (a*b)\n                   '\n\nSo, all we have to do is replace predictor, mediator, and outcome with the correct variable names!\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new object, mars_model, that contains the text above with the correct variable names from anx_data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmars_model &lt;- 'mcq_score ~ c*rmars_num_score + b*sticsa_pre_state_score\n                   sticsa_pre_state_score ~ a*rmars_num_score\n                   indirect_effect := a*b\n                   total_effect := c + (a*b)\n                   '\n\n\n\n\n\n\nOnce we’ve got that down, we can now use it as the model to give to the lavaan::sem() function to create the model. You might notice that there’s no formula anywhere else - it’s all in the mars_model object!\n\nmars_med &lt;- lavaan::sem(mars_model, data = anx_data,\n                        missing = \"FIML\", estimator = \"MLR\")\n\n\n\n\n\n\n\nExercise\n\n\n\nFit and save the model using the example code.\n\n\n\n\nResults\nWith the horrible part over, from here we can treat our new model object exactly like an lm() model, getting useful fit statistics and parameters with glance() and tidy(). The output might not look quite like we’re used to, though - focus on the “label” column, which points us to the results for the a, b, c, indirect, and total effect.\n\n\n\n\n\n\nExercise\n\n\n\nReview the fit statistics and parameter estimates for the mediation model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo make this easier to read, I’ve filtered the tidy() output to drop the other rows.\n\nbroom::glance(mars_med)\n\n\n\n  \n\n\nbroom::tidy(mars_med, conf.int = TRUE) |&gt; \n  dplyr::filter(label != \"\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nModeration\n\nCentring\nBefore we fit our model, we will first mean-centre our predictors. This doesn’t change the actual model, but it does change the interpretation by setting 0 to be the mean of each variable. All we have to do is subtract each value from the overall mean of the variable.\nIf this sounds like a job for mutate(), you’re absolutely right!\n\n\n\n\n\n\nExercise\n\n\n\nCreate two new variables in the dataset, with the same names as the originals but with “_cent” on the end, that are mean-centered. Don’t forget to assign this change back to the dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_score_cent = sticsa_trait_score - mean(sticsa_trait_score, \n                                                        na.rm = TRUE),\n    stars_int_score_cent = stars_int_score - mean(stars_int_score,\n                                                  na.rm = TRUE)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Instead of doing this “manually”, use the scale() function to mean-centre your variables. Use the help documentation to make sure you use the right arguments.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe help documentation for scale() tells us that we need to give it the variable to centre, then decide whether we want to centre and/or scale. We do want to centre, and center = TRUE is the default setting, so we don’t need to do anything here. However, we don’t want to scale, and scale = TRUE is also the default setting, so we’ll need to specify scale = FALSE.\nThen we just need to mutate() accordingly and give our variables some new names, either manually:\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_score_cent = scale(sticsa_trait_score, scale = FALSE),\n    stars_int_score_cent = scale(stars_int_score, scale = FALSE)\n  )\n\nOr with across(), if you did the challenge tasks in Tutorial 06:\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    across(c(sticsa_trait_score, stars_int_score),\n           ~scale(.x, scale = FALSE),\n           .names = \"{.col}_cent\")\n  )\n\n\n\n\n\n\n\n\nFit the Model\nThe good news is that a moderation model is just a fancy name for your bog-standard linear model, just with an interaction effect, which means we’re back to visit our old friend lm(). Literally the only new thing is how to specify the interaction. The simplest way is to use the multiplication symbol, *, to join together interacting predictors; this will include all the main effects as well as the interaction in the model at once. So, our formula will look like this:\noutcome ~ predictor1*predictor2\nFrom there it’s all exactly the same as what we did before in Tutorial 04 to fit and inspect the model.\n\n\n\n\n\n\nExercise\n\n\n\nFit the model with MCQ score as the outcome, and the centered versions of STARS interpretation anxiety and STICSA trait anxiety as the predictors, including an interaction. Save the model in a new object called stars_mod_lm.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nJust to keep things interesting, here’s how you can do this with the pipe. The main difference it makes is that you can use the tab autocomplete for these long variable names when typing in the code, but it makes no difference to the resulting model.\n\nstars_mod_lm &lt;- anx_data |&gt; \n  lm(mcq_score ~ stars_int_score_cent*sticsa_trait_score_cent,\n     data = _)\n\n\n\n\n\n\n\n\nResults\n\n\n\n\n\n\nExercise\n\n\n\nHave a look at the model results. If you feel so inclined, print them out in a nice auto-formatted table as well.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo just get the parameters table, we can use broom::tidy() as before.\n\nbroom::tidy(stars_mod_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nFor the fancy one, you can do this with {papaja}, {rempsyc}, or other methods of your choice. I’m going for {papaja} this time.\n\npapaja::apa_print(stars_mod_lm)$table |&gt; \n  papaja::apa_table()\n\n\n(#tab:unnamed-chunk-50)\n\n\n**\n\n\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n2.91\n[2.78, 3.04]\n45.45\n373\n&lt; .001\n\n\nStars int score cent\n-0.27\n[-0.42, -0.12]\n-3.47\n373\n.001\n\n\nSticsa trait score cent\n0.20\n[0.00, 0.41]\n1.97\n373\n.050\n\n\nStars int score cent \\(\\times\\) Sticsa trait score cent\n0.22\n[0.01, 0.43]\n2.06\n373\n.040\n\n\n\n\n\n\n\n\n\n\nThe tutorial introduces a few ways to explore the nature of the significant interaction. First, the simple slopes output includes some helpful statistical results as well as a Johnson-Neyman plot. For this we’ll use the sim_slopes() function from the {interactions} package as below.\n\n1interactions::sim_slopes(\n2  stars_mod_lm,\n3  pred = stars_int_score_cent,\n4  modx = sticsa_trait_score_cent,\n5  jnplot = TRUE,\n6  robust = TRUE,\n7  confint = TRUE\n  )\n\n\n1\n\nProduce a simple slopes analysis as follows:\n\n2\n\nUse the model stored in the object stars_mod_lm,\n\n3\n\nwith the centered STARS Interpretation Anxiety score as the predictor,\n\n4\n\nand the centered STICSA Trait Anxiety score as the moderator,\n\n5\n\nincluding a Johnson-Neyman interval plot,\n\n6\n\nrobust estimation of confidence intervals,\n\n7\n\nand CIs instead of SEs.\n\n\n\n\n\n\n\nJOHNSON-NEYMAN INTERVAL \n\nWhen sticsa_trait_score_cent is OUTSIDE the interval [0.48, 13.58], the\nslope of stars_int_score_cent is p &lt; .05.\n\nNote: The range of observed values of sticsa_trait_score_cent is [-1.21,\n1.70]\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of stars_int_score_cent when sticsa_trait_score_cent = -0.60047717 (- 1 SD): \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.40   0.11   -0.60   -0.19    -3.75   0.00\n\nSlope of stars_int_score_cent when sticsa_trait_score_cent =  0.02679718 (Mean): \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.26   0.08   -0.41   -0.11    -3.45   0.00\n\nSlope of stars_int_score_cent when sticsa_trait_score_cent =  0.65407153 (+ 1 SD): \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.12   0.09   -0.30    0.06    -1.34   0.18\n\n\nFor a lovely visualisation of the simple slopes analysis, the interact_plot() function from the same package produces a beautiful plot with minimal effort. This plot output is also a ggplot object, so you can further add to or customise it from here if you prefer.\n\n1interactions::interact_plot(\n2  stars_mod_lm,\n3  pred = stars_int_score_cent,\n4  modx = sticsa_trait_score_cent,\n5  interval = TRUE,\n6  x.label = \"STARS Interpretation Anxiety Score\",\n  y.label = \"Predicted MCQ Score\",\n  legend.main = \"STICSA Trait Anxiety Score\"\n  )\n\n\n1\n\nProduce a simple slopes interaction plot as follows:\n\n2\n\nUse the model stored in the object stars_mod_lm,\n\n3\n\nwith the centered STARS Interpretation Anxiety score as the predictor,\n\n4\n\nand the centered STICSA Trait Anxiety score as the moderator,\n\n5\n\nwith CIs included around the lines,\n\n6\n\nand custom labels for the x, y, and legend.\n\n\n\n\n\n\n\n \nWell, that’s enough statistics for…well, to be honest, most of second year of UG! Aside from factor analysis and some other bits and pieces, we’ve covered a good portion of what you can expect your supervisees to have been taught previously.\nFrom here, it would be a good idea to review the {discovr} tutorial in more depth for the specific analysis you’re thinking of using. However, at this point a lot of the content will be familiar - and you may even know a few tips and tricks that Andy doesn’t cover 🥳\nWell done!"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html",
    "title": "06: Mutate and Summarise",
    "section": "",
    "text": "This tutorial covers three more essential {dplyr} functions: workhorses mutate() and summarise(), and helper group_by().\nVery similar in structure, the first two functions primarily differ in output. mutate() makes changes within a given dataset by creating new variables (columns) whereas summarise() uses the information in a given dataset to create a new, separate summary dataset.\nThe last, group_by(), is a helper function that allows new variables or summaries to be created within subgroups."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html#overview",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html#overview",
    "title": "06: Mutate and Summarise",
    "section": "",
    "text": "This tutorial covers three more essential {dplyr} functions: workhorses mutate() and summarise(), and helper group_by().\nVery similar in structure, the first two functions primarily differ in output. mutate() makes changes within a given dataset by creating new variables (columns) whereas summarise() uses the information in a given dataset to create a new, separate summary dataset.\nThe last, group_by(), is a helper function that allows new variables or summaries to be created within subgroups."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html#setup",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html#setup",
    "title": "06: Mutate and Summarise",
    "section": "Setup",
    "text": "Setup\n\nPackages\nWe will again be focusing on {dplyr} today, which contains all three of our main functions. You can either load {dplyr} alone, or all of {tidyverse} - it won’t make a difference, but you only need one or the other.\nWe’ll also need {GGally} for correlograms, {correlation} for, well, I’ll give you three guesses!, and {kableExtra} for table formatting.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the necessary packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(dplyr)\n## OR\nlibrary(tidyverse)\n\nlibrary(GGally)\nlibrary(correlation)\nlibrary(kableExtra)\n\n\n\n\n\n\n\n\nData\nToday we’re continuing to work with the same dataset as last week. Courtesy of fantastic Sussex colleague Jenny Terry, this dataset contains real data about statistics and maths anxiety.\n\n\n\n\n\n\nExercise\n\n\n\nRead in the dataset and save it in a new object, anx_data.\nOn the Cloud, you can read in this dataset from the data folder using here::here().\nElsewhere, you can download the dataset, or copy the dataset URL, from the Data and Workbooks page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRead in from file:\n\nanx_data &lt;- readr::read_csv(here::here(\"data/anx_data.csv\"))\n\nRead in from URL:\n\nanx_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/anx_data.csv\")\n\n\n\n\n\n\n\nCodebook\nThere’s quite a bit in this dataset, so you will need to refer to the codebook below for a description of all the variables.\n\n\n\n\n\n\nDataset Info Recap\n\n\n\n\n\nThis study explored the difference between maths and statistics anxiety, widely assumed to be different constructs. Participants completed the Statistics Anxiety Rating Scale (STARS) and Maths Anxiety Rating Scale - Revised (R-MARS), as well as modified versions, the STARS-M and R-MARS-S. In the modified versions of the scales, references to statistics and maths were swapped; for example, the STARS item “Studying for an examination in a statistics course” became the STARS-M item “Studying for an examination in a maths course”; and the R-MARS item “Walking into a maths class” because the R-MARS-S item “Walking into a statistics class”.\nParticipants also completed the State-Trait Inventory for Cognitive and Somatic Anxiety (STICSA). They completed the state anxiety items twice: once before, and once after, answering a set of five MCQ questions. These MCQ questions were either about maths, or about statistics; each participant only saw one of the two MCQ conditions.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor learning purposes, I’ve randomly generated some additional variables to add to the dataset containing info on distribution channel, consent, gender, and age. Especially for the consent variable, don’t worry: all the participants in this dataset did consent to the original study. I’ve simulated and added this variable in later to practice removing participants.\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nid\nCategorical\nUnique ID code\n\n\ndistribution\nCategorical\nChannel through which the study was completed, either \"preview\" or \"anonymous\" (the latter representing \"real\" data). Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nconsent\nCategorical\nWhether the participant read and consented to participate (\"Yes\") or not (\"No\"). Note that this variable has been randomly generated and does NOT reflect genuine responses; all participants in this dataset did originally consent to participate.\n\n\ngender\nCategorical\nGender identity, one of \"female\", \"male\", \"non-binary\", or \"other/pnts\". \"pnts\" is an abbreviation for \"Prefer not to say\". Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nage\nNumeric\nAge in years. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nmcq\nCategorical\nIndependent variable for MCQ question condition, whether the participant saw MCQ questions about mathematics (\"maths\") or statistics (\"stats\").\n\n\nstars_[sub][number]\nNumeric\nItem on the Statistics Anxiety Rating Scale. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [help]: Asking for Help\n- [int]: Interpretation Anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (no anxiety) to 5 (a great deal of anxiety), so higher scores reflect higher levels of anxiety.\n\n\nstars_m_[sub][number]\nNumeric\nItem on the Statistics Anxiety Rating Scale - Maths, a modified version of the STARS with all references to statistics replaced with maths. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [help]: Asking for Help\n- [int]: Interpretation Anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (no anxiety) to 5 (a great deal of anxiety), so higher scores reflect higher levels of anxiety.\n\n\nrmars_[sub][number]\nNumeric\nItem on the Revised Maths Anxiety Rating Scale. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [num]: Numerical Task Anxiety\n- [course]: Course anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 5 (very much), so higher scores reflect higher levels of anxiety.\n\n\nrmars_s_[sub][number]\nNumeric\nItem on the Revised Maths Anxiety Rating Scale - Statistics, a modified version of the MARS with all references to maths replaced with statistics. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [num]: Numerical Task Anxiety\n- [course]: Course anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 5 (very much), so higher scores reflect higher levels of anxiety.\n\n\nsticsa_trait_[number]\nNumeric\nItem on the State-Trait Inventory for Cognitive and Somatic Anxiety, Trait subscale. [num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 4 (very much so), so higher scores reflect higher levels of anxiety.\n\n\nsticsa_[time]_state_[number]\nNumeric\nItem on the State-Trait Inventory for Cognitive and Somatic Anxiety, State subscale. [time] denotes one of two times of administration: before completing the MCQ task (\"pre\"), or after (\"post\"). [num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 4 (very much so), so higher scores reflect higher levels of anxiety.\n\n\nmcq_stats_[num]\nCategorical\nCorrect (1) or incorrect (0) response to MCQ questions about statistics, covering mean ([number] = 1), standard deviation (2), confidence intervals (3), beta coefficient (4), and standard error (5).\n\n\nmcq_maths_[num]\nCategorical\nCorrect (1) or incorrect (0) response to MCQ questions about maths, covering mean ([number] = 1), standard deviation (2), confidence intervals (3), beta coefficient (4), and standard error (5)."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html#mutate",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html#mutate",
    "title": "06: Mutate and Summarise",
    "section": "Mutate",
    "text": "Mutate\nThe mutate() function is one of the most essential functions from the {dplyr} package. Its primary job is to easily and transparently make changes within a dataset - in particular, a tibble.\n\nGeneral Format\nTo make a single, straightforward change to a tibble, use the general format:\ndataset_name |&gt;\n  dplyr::mutate(\n    variable_name = instructions_for_creating_the_variable\n  )\nvariable_name is the name of the variable that will be changed by mutate(). This can be any name that follows R’s object naming rules. There are two main options for this name:\n\nIf the dataset does not already contain a variable called variable_name, a new variable will be added to the dataset.\nIf the dataset does already contain a variable called variable_name, the new variable will silently1 replace (i.e. overwrite) the existing variable with the same name.\n\ninstructions_for_creating_the_variable tells the function how to create variable_name. These instructions can be any valid R code, from a single value or constant, to complicated calculations or combinations of other variables. You can think of these instructions exactly the same way as the vector calculations we covered earlier, and they must return a series of values that is the same length as the existing dataset.\n\n\n\n\n\n\nTip\n\n\n\nAlthough creating or modifying variables will likely be the most frequent way you use mutate(), it has other handy features such as:\n\nDeleting variables\nDeciding where newly created variables appear in the dataset\nDeciding which variables appear in the output, depending on which you’ve used\n\nSee the help documentation for more by running help(mutate) or ?mutate in the Console.\n\n\n\n\nAdding and Changing Variables\nLet’s have a look at how these two operations work in mutate().\nFirst, let’s see how to add new variables. Imagine we have found some collaborators to work with and we want to combine our datasets. To keep track of where the data came from, we want to add a lab variable at the start of our existing dataset containing the name of the university before we add more data.\n\n1anx_data |&gt;\n  dplyr::mutate(\n2    lab = \"Sussex\",\n3    .before = 1\n  )\n\n\n1\n\nTake the dataset and make the following changes:\n\n2\n\nCreate a new variable, lab, that contains the value \"Sussex\"\n\n3\n\nPut this variable before the first variable in the existing dataset.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nError Watch: Vector Size\n\n\n\n\n\nNote that in this case, I’ve given a single value, \"Sussex\", as the content of the new variable. R will “recycle” this single value across all of the rows to create a constant. However, if I tried to do this with a longer vector, I’ll get an error:\n\n1anx_data |&gt;\n2  dplyr::mutate(\n    lab = c(\"Sussex\", \"Glasgow\"),\n    .before = 1\n  )\n\nError in `dplyr::mutate()`:\nℹ In argument: `lab = c(\"Sussex\", \"Glasgow\")`.\nCaused by error:\n! `lab` must be size 465 or 1, not 2.\n\n\nIn this case I might need rep() (for creating vectors of repeating values), sample() (for creating random subsamples), or another helper function to generate the vector to add.\n\n\n\nNext, let’s look at changing existing variables. For example, I know that gender and mcq are meant to be factors (also called “categorical data” in SPSS and elsewhere). So, let’s convert each of these two variables into factor data type.\nFor ease of reading the output, I’m using the .keep = \"used\" argument, which only outputs variables that have been used in the preceding operation. This is for demonstration purposes and is great for checking your changes are correct, but make sure you remove this argument if you use this code yourself.\n\n1anx_data |&gt;\n  dplyr::mutate(\n2    gender = factor(gender),\n    mcq = factor(mcq),\n    ## OMIT this line for real analysis\n3    .keep = \"used\"\n  )\n\n\n1\n\nTake the dataset and make the following changes:\n\n2\n\nFor the gender and mcq variables, convert the existing variable into a factor and then replace the existing variable with the new one.\n\n3\n\nOnly output the variables that were used or created in this operation.\n\n\n\n\n\n\n  \n\n\n\nFinally, once I’ve checked the code works by examining the output, if I want to actually change my dataset, I have to assign the output of these commands to the dataset.\n\n\n\n\n\n\nExercise\n\n\n\nMake the above changes to the anx_data dataset and save the output to anx_data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::mutate(\n    lab = \"Sussex\",\n    gender = factor(gender),\n    mcq = factor(mcq),\n    .before = 1\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nImagine that item 17 on the STICSA State subscale is reversed and needs to be reverse-coded. Using the Codebook, replace the existing variable with the reversed version.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nDon’t forget there are pre and post versions of this variable, so BOTH must be reversed!\n\nanx_data |&gt; \n  dplyr::mutate(\n    sticsa_pre_state_17_rev = 5 - sticsa_pre_state_17,\n    sticsa_post_state_17_rev = 5 - sticsa_post_state_17,\n    ## OMIT this line for real analysis\n    .keep = \"used\"\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is reverse-coding?\n\n\n\n\n\nIn many multi-item measures, some items are reversed in the way that they capture a particular construct. In this particular example, items on the STICSA are worded so that a higher numerical response (closer to the “very much so” end of the scale) indicates more anxiety, such as item 4: “I think that others won’t approve of me”.\nHowever, reverse-coded items are intended to capture the same ideas, but in reverse. A reversed version of item 17 might read, “I can concentrate easily with no intrusive thoughts.” In this case, a higher numerical response (closer to the “very much so” end of the scale) would indicate less anxiety. In order for these reversed items to be aligned with the other items on the scale, so that together they form a cohesive score, the coding of the response scale must be flipped: high becomes low, and low becomes high.\nIf the response scale is a numerical integer sequence, as this one is, then the simplest way to reverse-code the responses is to subtract every response from the maximum possible response plus one. Here, the STICSA response scale is from 1 to 4; the maximum possible response is 4, plus one is 5. So, to reverse-code the responses, we need to subtract each rating on this item from 5. A high score (4) will be become a low score (5 - 4 = 1), and vice versa for a low score (5 - 1 = 4).\n\n\n\n\n\nComposite Scores\n\nRow-wise magic is good magic. - Hadley Wickham\n\nA very common mutate() task is to create a composite score from multiple variables - for example, an overall trait anxiety score from our sticsa_trait items. Let’s create an overall score that contains the mean of the ratings2 on each of the STICSA trait anxiety items, for each participant.\nTo do this, we need two new functions.\n\nThe first new function, dplyr::c_across(), provides an efficient way to select multiple variables to contribute to the calculation - namely, by using &lt;tidyselect&gt; semantics.\nThe second new function is actually a pair of functions, dplyr::rowwise() and dplyr::ungroup(). These two respectively impose and remove an internal structure to the dataset, such that each row is treated like its own group, and any operations are done within those row-wise groups.\n\nLet’s see the combination of these two in action.\n\n\n\n\n\n\nImportant\n\n\n\nThe code below assumes a dataset structured so there is information from each participant on only and exactly one row in the dataset.\nIf your data has observations from the same participants on multiple rows, you will need to reshape your data or otherwise adapt the code to suit your data structure.\n\n\n\n1anx_data &lt;- anx_data |&gt;\n2  dplyr::rowwise() |&gt;\n3  dplyr::mutate(\n    sticsa_trait_score = mean(c_across(starts_with(\"sticsa_trait\")),\n                        na.rm = TRUE)\n  ) |&gt;\n4  dplyr::ungroup()\n\n\n1\n\nOverwrite the anx_data dataset with the following output: take the existing anx_data dataset, and then\n\n2\n\nGroup the dataset by row, so any subsequent calculations will be done for each row separately, and then\n\n3\n\nCreate the new sticsa_trait_score variable by taking the mean of all the values in variables that start with the string “sticsa_trait” (ignoring any missing values), and then\n\n4\n\nRemove the by-row grouping that was created by rowwise() to output an ungrouped dataset.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor lots more details and examples on rowwise() and rowwise operations with {dplyr} - including which other scenarios in which a row-wise dataset would be useful - run vignette(\"rowwise\") in the Console.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate mean subscale scores for each of the three STARS subscales and save these changes to the dataset. If you didn’t do it already, make sure you create sticsa_trait_score as above also.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe three new STARS subscales require three separate arguments to mutate(). Remember to change the name of the new variable and the string in starts_with() each time!\n\nanx_data &lt;- anx_data |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate( \n    ## If you didn't create this already!\n    sticsa_trait_score = mean(c_across(starts_with(\"sticsa_trait\")), \n                        na.rm = TRUE),\n    stars_help_score = mean(c_across(starts_with(\"stars_help\")),\n                        na.rm = TRUE),\n    stars_test_score = mean(c_across(starts_with(\"stars_test\")),\n                        na.rm = TRUE),\n    stars_int_score = mean(c_across(starts_with(\"stars_int\")),\n                        na.rm = TRUE)\n  ) |&gt;\n  dplyr::ungroup()\n\nAlthough this is reasonably efficient, I hate copy and pasting. The rule of thumb is: if you have to copy and paste more than once, use a function instead. We might come back to this later…\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat would the above code creating sticsa_trait_score produce without the rowwise()...ungroup() steps (i.e. with only the mutate() command)? Make a prediction, then try it.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can see what happens without rowwise()...ungroup() just by commenting them out of the pipe. To do this, either type # before each line, or highlight them and press CTRL/CMD + SHIFT + C. I’ve also added on an extra select() command at the end to look at only the relevant variable.\n\nanx_data |&gt; \n  # dplyr::rowwise() |&gt; \n  dplyr::mutate(\n    sticsa_trait_score = mean(c_across(starts_with(\"sticsa_trait\")), \n                              na.rm = TRUE),\n  ) |&gt; \n  # dplyr::ungroup() |&gt; \n  dplyr::select(sticsa_trait_score)\n\nThis code still runs successfully, but the result isn’t what we wanted. Have a look at the sticsa_trait_score variable: all the values are the same. Instead of calculating the mean for each person, this code instead calculated the overall mean of all of the anxiety variables, and then assigned that single value to the sticsa_trait_score variable. Not what we wanted in this case - but it could be useful in other scenarios!\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: The rowwise() |&gt; c_across() |&gt; ungroup() code is definitely not the only way to obtain the same output. Try producing the the same sticsa_trait_score variable with the following methods. What are the benefits and drawbacks of each method?\nHint: Use vignette(\"rowwise\") to help if you get stuck.\n\nUsing a dedicated by-row function, rowMeans()\nUsing the basic structure of mutate() only\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we wanted to avoid, or didn’t remember, the rowwise()...ungroup() sequence, there are other options to produce the same result, but neither are easier to read or implement. (They aren’t necessarily harder, either! This really is down to preference.)\n1. Using rowMeans()\nThe {base} function rowMeans() calculates the mean of each row without any additional jiggery pokery to worry about. The problem is specifying which variables to include, especially because we have 21 in this example to work with.\nHowever, rowMeans() is an independent function who don’t need no {dplyr}, and as such does not work the same way as, for instance, mean() does, with no straightforward workaround.\n\n## Reasonable but just doesn't work!\nanx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_score = rowMeans(c(sticsa_trait_1, sticsa_trait_2, sticsa_trait_3, ..., sticsa_trait_21))\n  )\n\nError in `dplyr::mutate()`:\nℹ In argument: `sticsa_trait_score = rowMeans(...)`.\nCaused by error in `is.data.frame()`:\n! '...' used in an incorrect context\n\n\n\nanx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_score = rowMeans(c_across(starts_with(\"sticsa_trait\")))\n  )\n\nError in `dplyr::mutate()`:\nℹ In argument: `sticsa_trait_score =\n  rowMeans(c_across(starts_with(\"sticsa_trait\")))`.\nCaused by error in `rowMeans()`:\n! 'x' must be an array of at least two dimensions\n\n\nThis is because rowMeans() is expecting a whole dataset, not just a subset of columns. You can solve this by select()ing within the rowMeans() function:\n\nanx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_score = rowMeans(\n      dplyr::select(anx_data,\n                    contains(\"sticsa_trait\")\n                    )\n      )\n  )\n\n…which has the major issue that if you update the name of your dataset, you must update it in TWO places - at the start of the pipe and inside rowMeans(). Personally, I avoid this because I am too likely to forget or not notice the dataset name within the command and end up with errors or wrong results.\nAlternatively, you can use dplyr::pick() with &lt;tidyselect&gt; semantics to make this less, well, terrible:\n\nanx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_score = rowMeans(pick(contains(\"sticsa_trait\")))\n  )\n\n…which didn’t seem fair because we haven’t talked about pick(), and also defeats the purpose of using rowMeans() to avoid having to learn new {dplyr} functions. So, {dplyr} wins this one either way.\nIf you’re keen to never have to learn a jot more {dplyr} than absolutely necessary (I bet you are not having a good time so far!), this Stack Overflow post offers some other, non-{dplyr} solutions…that also depend on using the magrittr pipe %&gt;%! Sorry.\n2. Use basic mutate()\nThe most straightforward method - although perhaps not the most obvious - is to express the calculation you want as arithmetic using the relevant variables. In this instance, to calculate a mean, we sum the scores together and then divide by the number of scores:\n\nanx_data |&gt; \n  dplyr::mutate(\n    sticsa_trait_score = (sticsa_trait_1 + sticsa_trait_2 + ... + sticsa_trait_21)/21\n  )\n\nThis method, although very transparent, has some critical downsides.\n\nIt’s clunky and prone to error. This style works best for 2-3 variables contributing to the composite. For more variables, we end up with a lot of repetitive typing of variable names (remember our rule about copy/pasting!), which also means increased likelihood of typos, accidental omissions, or other errors - especially with a large number of variables, as we have here.\nIt’s not robust. Imagine that, on review of the STICSA Trait scale, we find that sticsa_trait_9 is a badly worded/unreliable item and decide to drop it from our analysis. We then either have to (remember to) manually update our code both to remove sticsa_trait_9 and to change the denominator from 21 to 20 (not a good time), or debug the resulting error if we don’t remember.\n\nWe do teach this method to UGs specifically to reduce the number of functions they have to learn, but for real-life usage, in most cases, the rowwise() solution is your best bet for both readability and resilience.\n\n\n\n\n\n\n\nConditionals\nThere are many functions out there for recoding variables (let’s wave cheerfully at dplyr::recode() as we fly by it without stopping), but the following method, using dplyr::case_when(), is recommended because it is so versatile. It can be used to recode the values from one variable into new one, but it can also combine information across variables and handle multiple conditionals. It essentially allows a series of if-else statements without having to actually have lots of if-else statements.\nThe generic format of dplyr::case_when() can be stated as follows:\n\ndataset_name |&gt; \n  dplyr::mutate(\n    new_variable = dplyr::case_when(\n      logical_assertion ~ value,\n      logical_assertion ~ value,\n      .default = value_to_use_for_cases_with_no_matches\n    )\n  )\n\nlogical_assertion is any R code that returns TRUE and FALSE values. These should be very familiar by now!\nvalue is the value to assign to new_variable for the cases for which logical_assertion returns TRUE.\nThe assertions are evaluated sequentially (from first to last in the order they are written in the function), and the first match determines the value. This means that the assertions must be ordered from most specific to least specific.\n\n\n\n\n\n\nTesting Assertions\n\n\n\n\n\nThe assertions for dplyr::case_when() are the same as the ones we used previously in dplyr::filter(). In fact, if you need to test the assertion you are writing to ensure that your code will work as you want, you can try the same assertion in dplyr::filter() to check whether the cases it returns are only and exactly the ones you want to change.\n\n\n\nLet’s look at two examples of how dplyr::case_when() might come in handy.\n\nOne-Variable Input\nWe’ve created our composite sticsa_trait_score variable previously, and now we may want to change this continuous score into a categorical variable indicating whether or not participants display clinical levels of anxiety. So, we can use case_when() to recode sticsa_trait_score into a new sticsa_trait_cat variable.\n\n1anxiety_cutoff &lt;- 2.047619\n\n2anx_data &lt;- anx_data |&gt;\n3  dplyr::mutate(\n4    sticsa_trait_cat = dplyr::case_when(\n5      sticsa_trait_score &gt;= anxiety_cutoff ~ \"clinical\",\n6      sticsa_trait_score &lt; anxiety_cutoff ~ \"non-clinical\",\n7      .default = NA\n    )\n  )\n\n\n1\n\nCreate a new object, anxiety_cutoff, containing the threshold value for separating clinical from non-clinical anxiety. This one is from Van Dam et al., 2013.\n\n2\n\nOverwrite the anx_data object by taking the dataset, and then…\n\n3\n\nMaking a change to it by…\n\n4\n\nCreating a new variable, anxiety_cat, by applying the following rules:\n\n5\n\nFor cases where the value of sticsa_trait_score is greater than or equal to anxiety_cutoff, assign the value “clinical” to sticsa_trait_cat\n\n6\n\nFor cases where the value of sticsa_trait_score is less than anxiety_cutoff, assign the value “non-clinical” to sticsa_trait_cat\n\n7\n\nFor cases that don’t match any of the preceding criteria, assign NA to sticsa_trait_cat\n\n\n\n\n\n\n\n\n\n\nWhy the new anxiety_cutoff object?\n\n\n\n\n\nIn the code above, the cutoff value is stored in a new object, anxiety_cutoff, which is then used in the subsequent case_when() conditions. Why take this extra step?\nThis is a matter of style, since the output of this code would be entirely identical if I wrote the cutoff value into the case_when() assertions directly (e.g. sticsa_trait_score &gt;= 2.047619). I have done it this way for a few reasons:\n\nThe threshold value is easy to find, in case I need to remind myself which one I used, and it’s clearly named, so I know what it represents.\nThe threshold value only needs to be typed in once, rather than copy/pasted or typed out multiple times, which decreases the risk of typos or errors.\nMost importantly, it’s easy to change, in case I need to update it later. I would only have to change the value in the anxiety_cutoff object once, at the beginning of the code chunk, and all of the subsequent code using that object would be similarly updated.\n\nIn short, it makes the code easier to navigate, more resilient to later updates, and more transparent in its meaning.\n\n\n\n\n\nMulti-Variable Input\nWe might also like to create a useful coding variable to help keep track of the number of cases we’ve removed, and for what reasons. We can draw on input from multiple variables to create this single new variable. Here’s the idea to get started:\n\n1anx_data |&gt;\n  dplyr::mutate(\n    remove = dplyr::case_when(\n2      distribution == \"preview\" ~ \"preview\",\n3      consent != \"Yes\" ~ \"no_consent\",\n4      .default = \"keep\"\n    )\n  )\n\n\n1\n\nTake the dataset anx_data and then make a change to it by a creating a new variable, remove, by applying the following rules:\n\n2\n\nFor cases where the distribution variable contains exactly and only \"preview\", assign the value \"preview\" to remove.\n\n3\n\nFor cases where the consent variable does not contain exactly and only \"Yes\", assign the value \"no_consent\" to remove.\n\n4\n\nFor cases that don’t match any of the preceding criteria, assign the value \"keep\" to remove.\n\n\n\n\nNote that for this variable, each assertion is designed to identify the cases that we do NOT want to keep. The .default = \"keep\" line assigns the value \"keep\" for any case that doesn’t match any of the exclusion criteria - i.e., unless there’s a reason to drop a particular case, we keep it by default.\n\n\n\n\n\n\nExercise\n\n\n\nAdapt the code above to finish creating a remove variable that includes the possible reasons for exclusion that we covered in the last tutorial:\n\nBelow ethical age of consent\nAge missing or improbably high (e.g. 100 or above)\n\nThen, assign this change back to your dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStart with the template above, then add more assertions and corresponding values.\n\nanx_data &lt;- anx_data |&gt;\n  dplyr::mutate(\n    remove = dplyr::case_when(\n      distribution == \"preview\" ~ \"preview\",\n      consent != \"Yes\" ~ \"no_consent\",\n      age &lt; 18 ~ \"age_young\",\n      is.na(age) | age &gt;= 100 ~ \"age_bad\",\n      .default = \"keep\"\n    )\n  )\n\n\n\n\n\n\nBecause the first match for each case is the value it is assigned, each case will receive only one value, even if they match multiple criteria. For example, if you had a participant who didn’t consent and their age was 17, they would be coded as \"no_consent\" rather than \"age_young\" because the assertion about consent comes before the assertion about age in the code.\nFrom here, you can easily use this variable to summarise exclusions, and to filter out excluded cases for your final dataset.\n\n1exclusion_summary &lt;- anx_data |&gt;\n  dplyr::count(remove)\n2exclusion_summary\n\n3anx_data_final &lt;- anx_data |&gt;\n  dplyr::filter(remove == \"keep\")\n\n\n1\n\nTake anx_data and count the number of times each unique value occurs in the remove variable, storing the output in a new object, exclusions_summary.\n\n2\n\nPrint out the exclusions_summary object (see below!).\n\n3\n\nCreate a new object, anx_data_final, by taking anx_data and then retaining only the cases for which the remove variable has only and exactly the value \"keep\" - effectively dropping all other cases.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Use exclusion_summary to write a report of all the exclusions from this dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn the previous tutorial, we did this task with individual objects each recording the exclusions for particular reasons. This time round, we instead have a little tibble that contains all these numbers at once. No problem - we know how to work with tibbles!\nInstead of using individual objects in our inline code, we can instead use whole bits of code, including pipes. So, we could use a three-step process: Take the exclusion_summary object, filter it for the specific exclusion criteria we want for that particular sentence, and then pull out the value in the n variable with the counts for that exclusion.\n\nTo begin, `r exclusion_summary |&gt; dplyr::filter(remove == 'no_consent') |&gt; dplyr::pull(n)` cases were removed who did not consent. Next, `r r exclusion_summary |&gt; dplyr::filter(remove == 'age_young') |&gt; dplyr::pull(n)` cases were removed who were younger than 18, which…\n\nThis will work! It will work great. You will get a lovely clean report of exclusions doing this.\nBUT.\nYou know by now that this is not my jam. I’m going to have to copy/paste that long string of code again and edit it manually multiple times3. Instead, I could write a wrapper function to do this for me - a bespoke function that I write only for my particular use, that wraps up some bit of code into a convenient function name to make it easier (and less potentially errorful) to use multiple times.\n\nrep_excl &lt;- function(summary, value){\n  summary |&gt;\n    dplyr::filter(remove == value) |&gt;\n    dplyr::pull(n)\n}\n\n3rep_excl(exclusion_summary, \"no_consent\")\n\n\n1\n\nCreate a new function, rep_excl, with the arguments summary and value that does the following:\n\n2\n\nTake the object summary and then filter it keeping only the cases where the value in the remove variable is only and exactly equal to the value argument, and then pull out the value in the n variable.\n\n3\n\nUse the new rep_excl with the exclusion_summary object as the summary argument, and the string \"no_consent\" as the value argument.\n\n\n\n\n[1] 15\n\n\nShiny right? Now my report might look like this:\n\nTo begin, `r rep_excl(exclusion_summary, 'no_consent')` cases were removed who did not consent. Next, `r rep_excl(exclusion_summary, 'age_young')` cases were removed who were younger than 18, which…\n\nSurely we’re done now! Much easier to read, much easier to update. The only downside is that we have to create the rep_excl() function further up in this document, in a sourced script, or in our own package.\nWe’re done, right?\n…\nLook at that report again. Look at all that repeated text. I bet we could figure out how to write a function that would generate the entire paragraph programmatically, so our report would look like this:\n\n`r report_all_exclusions(exclusion_summary)`\n\nWe may come back to this later…\n\n\n\n\n\n\n\n\nIteration\n\n\n\n\n\n\nWarning\n\n\n\nThis material may not be covered in the live workshops, depending on time. It’s included here for reference because it’s extremely useful in real R analysis workflows, but it won’t be essential for any of the live tasks.\n\n\nMutate is an amazing tool for working with your dataset, but applying the same change to multiple variables quickly becomes tedious. Imagine we wanted to change all of the character variables in this dataset to factors. We could do something like this:\n\nanx_data |&gt; \n  dplyr::mutate(\n    id = factor(id),\n    distribution = factor(distribution),\n    consent = factor(consent),\n    gender = factor(gender),\n    mcq = factor(mcq),\n    remove = factor(remove)\n  )\n\nUgghhh. Did you hate reading that? I hated writing it.\nThe general rule of thumb is: if you have to copy/paste the same code more than once, use (or write!) a function instead. Essentially, we want to spot where our code is identical, and turn the identical bits into a function that does the repetitive part for us.\nLuckily we don’t have to figure out how to do this iteration from scratch4, because {dplyr} already has a built-in method for doing exactly this task. It’s called dplyr::across() and it works like this:\ndataset_name |&gt; \n  dplyr::mutate(\n    dplyr::across(&lt;tidyselect&gt;, function_to_apply)\n  )\nIn the first argument, we use &lt;tidyselect&gt; syntax to choose which variables we want to change.\nIn the second argument, the function or expression in function_to_apply is applied to each of the variables we’ve chosen.\nThe task we wanted to do was convert all character variables to factors. So our repetitive, copy/paste command above becomes:\n\nanx_data |&gt; \n  dplyr::mutate(\n    dplyr::across(where(is.character),\n                  factor),\n    ## For tutorial demo purposes only!\n    .keep = \"used\"\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Use dplyr::across() to choose all the items on the statistics version of the MARS, and add 10 to all the scores.\n(This probably isn’t something you really want to do to your own data, but it’s good for practice.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs we saw in the “Using Custom Functions” section of the last tutorial, we can write an ad-hoc formula instead of using an existing function with the following components:\n\nThe ~ (apparently pronounced “twiddle”!) at the beginning, which is a shortcut for the longer function(x) ... notation for creating functions.\nThe .x, which is a placeholder for each of the variables that the function will be applied to.\n\nSo, “add 10” becomes ~ .x + 10, and the full command looks like:\n\nanx_data |&gt; \n  dplyr::mutate(\n    dplyr::across(contains(\"_s_\"),\n                  ~.x + 10),\n    ## For tutorial demo purposes only!\n    .keep = \"used\"\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: You might notice that across() by default overwrites variables, rather than creating new ones. However, the help documentation for across() includes an argument for creating new variables names. Do the same task as above - adding 10 to the RMARS-S variables - but add _plusten to the end of the new variable names.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUnder “Arguments”, the help documentation describes the .names argument, which allows us to easily create new variable names. This uses a “glue specification” (see the {glue} package for more on that!) but we don’t need much more than what’s in the help documentation for this.\nSo, let’s add the .names argument. Here we’re using {.col} as a stand-in for each variable name, so all of the new variables with 10 added will have the same name as the original, but with _plusten at the end.\n\nanx_data |&gt; \n  dplyr::mutate(\n    across(contains(\"_s_\"),\n           ~ .x + 10,\n           .names = \"{.col}_plusten\"),\n    ## For tutorial demo purposes only!\n    .keep = \"used\"\n  )\n\n\n\n  \n\n\n\nBosh!"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html#summarise",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html#summarise",
    "title": "06: Mutate and Summarise",
    "section": "Summarise",
    "text": "Summarise\nThe summarise() function looks almost exactly like mutate. However, its primary job is to quickly generate summary information in a new, separate tibble.\n\nGeneral Format\ndataset_name |&gt; \n  dplyr::summarise(\n    variable_name = instructions_for_creating_the_variable\n  )\n\n\n\n\n\n\nImportant\n\n\n\nYou may notice that the basic structure of summarise() looks identical to the basic structure of mutate(), above. The difference is that mutate() creates or replaces variables within the same dataset, while summarise() creates a new summary dataset without changing the original.\n\n\nvariable_name is the name of a variable that will be created in the new summary tibble. This can be any name that follows R’s object naming rules.\ninstructions_for_creating_the_variable tells the function how to create variable_name. The instructions can refer to variables in the piped-in dataset, but should output a single value, rather than a vector of values (as we saw in mutate()).\nLet’s have a look at an example of this. Since we’ve previously created the overall trait anxiety variable sticsa_trait_score, we might want to get some info about this variable, such as the mean and standard deviation. To do this, we create a new variable for each descriptive value we want to create on the left side of the =, and instructions for creating that summary on the right.\nI’m also throwing in another new function (yay!). Unassuming little dplyr::n() has one job: counting the number of cases. Right now it’s not telling us much that’s new…but that will change in a bit!\n\nanx_data |&gt; \n  dplyr::summarise(\n    n = dplyr::n(),\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nAdd additional arguments to the summarise() code above to include the min and max in the output.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe’ve seen all of these before!\n\nanx_data |&gt; \n  dplyr::summarise(\n    dplyr::n(),\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_min = min(sticsa_trait_score, na.rm = TRUE),    \n    sticsa_trait_max = max(sticsa_trait_score, na.rm = TRUE)\n    )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Add one more argument to the summarise() code above to include the standard error in the output.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nJust for convenience’s sake, I’m going to drop the other elements for the moment and just try to get the SE.\nFirst, if there’s an sd() function, we might hope there’s a corresponding se() function. No dice.\n\nanx_data |&gt; \n  dplyr::summarise(\n    sticsa_trait_se = se(sticsa_trait_score, na.rm = TRUE)\n  )\n\nError in `dplyr::summarise()`:\nℹ In argument: `sticsa_trait_se = se(sticsa_trait_score, na.rm = TRUE)`.\nCaused by error in `se()`:\n! could not find function \"se\"\n\n\nNext, we can try the help documentation. Besides searching for a particular function in the Console, we can actually search the help documentation generally for a word or phrase. If I go to the “Help” tab and type “standard error”, I’ll get a list of vignettes and functions that might be relevant. The one that looks the most promising (and straightforward) is papaja::se(), which looks to work just like the other {stats} functions. Let’s try it:\n\nanx_data |&gt; \n  dplyr::summarise(\n    sticsa_trait_se = papaja::se(sticsa_trait_score, na.rm = TRUE)\n  )\n\n\n\n  \n\n\n\nHurrah! Job done.\nOkay. Great. BUT. A whole different package? A function with a totally different name? I don’t know about you but I hate that. If you don’t, feel free to stop here, because this is really going to go off the deep end a bit.\nSo, I know that the standard error is the standard deviation divided by the square root of N5. That’s fine then - we’ve just created a variable that contains the SD, and we have the dataset and we know how to count the number of cases so, let’s just do it by hand.\n\nanx_data |&gt; \n  dplyr::summarise(\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    ## Calculate SE taking the SD we just created \n    ## and divide by the square root of the number of cases in anx_data\n    sticsa_trait_se = sticsa_trait_sd/sqrt(nrow(anx_data))\n  )\n\n\n\n  \n\n\n\nSnazzy. BUT. This code is a bit dangerous because if I decide to rename anx_data, or use this code again in the same document with a different dataset, I have remember to update it in two places, one of which (within the sqrt(nrow()) command) is quite hard to spot. I’m piping in the data, can’t I just…..use the dataset itself for this?\nThe reason I might think this is that with the {magrittr} pipe, this sort of thing was easy to do. Let’s have a look:\n\nlibrary(magrittr)\n\n## Notice the magrittr pipe %&gt;% here instead of |&gt;\nanx_data %&gt;%\n  dplyr::summarise(\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_se = sticsa_trait_sd/sqrt(nrow(.))\n  )\n\n\n\n  \n\n\n\nNice, right? We just use the {magrittr} placeholder . to refer to the dataset within the summarise() command. Cool, so let’s do the same, just changing out %&gt;% for |&gt; and . for _:\n\nanx_data |&gt;\n  dplyr::summarise(\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_se = sticsa_trait_sd/sqrt(nrow(_))\n  )\n\nError: invalid use of pipe placeholder\n\n\nWell…shit.\nThis is one of those fringe cases where the {magrittr} pipe and the native pipe don’t work quite the same way. It’s very annoying especially if you’re used to writing code with %&gt;% that does this effortlessly. So, can we get something similar to work with the native pipe instead?\nThe answer is yes, but not with nrow(). Instead we need our new friend dplyr::n() that does the same as nrow() - just counts the number of rows - but is pipe-friendly. We don’t need a placeholder here, and in fact it doesn’t work if you try to add one.\n\nanx_data |&gt;\n  dplyr::summarise(\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_se = sticsa_trait_sd/sqrt(dplyr::n())\n  )\n\n\n\n  \n\n\n\nSo a weird little detour, but at least I have accomplished my primary goal of having as few unusual details remember as possible (no {papaja} and no updating-dataset-names-at-multiple-points), and honestly there’s more than enough weird details to remember already just using R normally so I don’t need that kind of negativity in my life. This comes at the not-insubstantial cost of having to remember to use a different function for getting N, but we were already using dplyr::n() so that’s alright - for me. You’ll find as you go that you develop your own preferences for how to write code that makes the most sense to you and aligns with the kind of things you can remember, spot, or update easily.\n\n\n\n\n\n\n\nBy Group\nNext, let’s combine dplyr::summarise() with the helper function dplyr::group_by() to split up the summary calculations by the values of a grouping variable.\nSimilar to what we saw with rowwise(), group_by() creates internal structure in the dataset - a new group for each unique value in the grouping variable. Any subsequent calculations done with the dataset are done within those groups.\n\n1anx_data |&gt;\n2  dplyr::group_by(mcq) |&gt;\n3  dplyr::summarise(\n4    n = dplyr::n(),\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_min = min(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_max = max(sticsa_trait_score, na.rm = TRUE)\n  )\n\n\n1\n\nTake the dataset, and then\n\n2\n\nGroup by the values in the mcq variable, and then\n\n3\n\nProduce a summary table with the following variables:\n\n4\n\nNumber of cases, and mean, SD, minimum, and maximum values of the sticsa_trait_score variable.\n\n\n\n\n\n\n  \n\n\n\nCompare this to the ungrouped summary in the previous section - it’s the same columns, but a new row for each group. We also see here that little dplyr::n() is a bit more useful now - giving us counts within each group alongside the other summary information.\n\n\n\n\n\n\nExercise\n\n\n\nAdd to the already-grouped summarise() code to further split up the output by gender as well as MCQ group.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAll we have to do is add more grouping variables into group_by(), separated by commas.\n\nanx_data |&gt;\n  dplyr::group_by(mcq, gender) |&gt;\n  dplyr::summarise(\n    n = dplyr::n(),\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_min = min(sticsa_trait_score, na.rm = TRUE), \n    sticsa_trait_max = max(sticsa_trait_score, na.rm = TRUE)\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReshaping Summary Tables\n\n\n\n\n\nThis second summary table, grouped by both MCQ and gender, would likely be easier to read with one variable on separate rows and the other in separate columns. We’ll have a look at reshaping in the last section of this course, but if you want to get a head start, run vignette(\"pivot\") in the Console.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Split up this summarise() output by whether each case scored higher, or lower than/equal to, the median value of the STARS test anxiety score.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou could have solved this in (at least) two ways. The first is to have an intermediate mutate() command to create the grouping variable, which doesn’t yet exist in the dataset.\n\nanx_data |&gt; \n  dplyr::mutate(\n    stars_test_cat = dplyr::case_when(\n      stars_test_score &gt; median(stars_test_score, na.rm = TRUE) ~ \"high\",\n      stars_test_score &lt;= median(stars_test_score, na.rm = TRUE) ~ \"low\",\n      .default = NA\n    )\n  )|&gt; \n  dplyr::group_by(stars_test_cat) |&gt; \n  dplyr::summarise( \n    n = dplyr::n(),\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE), \n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_min = min(sticsa_trait_score, na.rm = TRUE),  \n    sticsa_trait_max = max(sticsa_trait_score, na.rm = TRUE)\n  )\n\n\n\n  \n\n\n\nThe second is to create that grouping variable ad-hoc within group_by(). That is, the variable you want to group by doesn’t have to already exist - you can create it on the fly within group_by()!\n\nanx_data |&gt;\n  dplyr::group_by(stars_test_cat = (stars_test_score &lt;= median(stars_test_score, na.rm = TRUE))) |&gt;\n  dplyr::summarise(\n    n = dplyr::n(),\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_min = min(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_max = max(sticsa_trait_score, na.rm = TRUE)\n  )\n\n\n\n  \n\n\n\nHere I’ve gone with a simplified version of stars_test_cat - just TRUE or FALSE - but if I wanted to I could use the entire dplyr::case_when() bit from the first mutate() command to get exactly the same result.\n\n\n\n\n\n\n\nIteration\n\n\n\n\n\n\nWarning\n\n\n\nThis material may not be covered in the live workshops, depending on time. It’s included here for reference because it’s extremely useful in real R analysis workflows, but it won’t be essential for any of the workshop tasks.\n\n\nDespite the versatility of summarise(), you may have already noticed that the code covered so far is very typing-intensive if you want information about more than one variable. This is neither efficient nor particularly enjoyable:\n\n## Down with this sort of thing!\nanx_data |&gt;\n  dplyr::group_by(mcq) |&gt;\n  dplyr::summarise(\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_min = min(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_max = max(sticsa_trait_score, na.rm = TRUE),\n    stars_test_mean = mean(stars_test_score, na.rm = TRUE),\n    stars_test_sd = sd(stars_test_score, na.rm = TRUE),\n    stars_test_min = min(stars_test_score, na.rm = TRUE),\n    stars_test_max = max(stars_test_score, na.rm = TRUE) \n  )\n\nIf we wanted to also include, for instance, range and CIs, this code would quickly become unmanageably long and difficult to read, not to mention increasingly prone to errors.\nRemember, if you have to copy/paste more than once, use a function instead.\nThere are two main solutions to this issue, and which you choose depends on what you want the output to contain and how much work you want to put into reading the help documentation of various functions.\n\nUse an Existing Function\n\n\n\n\n\n\nChoose this option if:\n\n\n\n\nYou just want the basic descriptives and don’t need grouped summaries\nYou don’t mind reading up in the help documentation to get the right combination of arguments, and/or trying out a few different functions/packages to find the one that works for you.\n\n\n\nAs we saw in Tutorial 03: Datasets, there are existing functions that output pre-made summaries across multiple variables. If you revisit datawizard::describe_distribution(), you will find in the help documentation that it can utilise &lt;tidyselect&gt; syntax to select the variables you want, and the output can even be forced into a tibble for further wrangling.\n\n\nFunction List + across()\n\n\n\n\n\n\nChoose this option if:\n\n\n\n\nYou want custom or complex summary information\nYou want grouped summaries\nLike me, you just want to do everything yourself so you know it’s exactly right.\n\n\n\nThe big, inefficient multi-variable summarise() command above has two main issues to resolve.\n\nWe had to type the same functions over and over (i.e. mean() and sd() are repeated for each variable). Instead, we’ll create a list of functions to use, so we only have to type out each function once.\nWe had to manually type in each variable name we want to use. Instead, we’re going to utilise dplyr::across() to apply the list of functions from the first step to variables selected with &lt;tidyselect&gt;.\n\n\n\n\n\n\n\nTip\n\n\n\nFor more explanation about dplyr::across(), see the section on iteration with mutate() earlier on. For a much more in-depth explanation, run vignette(\"colwise\") in the Console.\n\n\n\n1fxs &lt;- list(\n2  mean = ~ mean(.x, na.rm = TRUE),\n  sd = ~ sd(.x, na.rm = TRUE),\n  min = ~ min(.x, na.rm = TRUE),\n  max = ~ max(.x, na.rm = TRUE)\n)\n\nanx_data |&gt; \n  dplyr::group_by(mcq) |&gt; \n  dplyr::summarise(\n3    across(contains(\"score\"), fxs)\n  )\n\n\n1\n\nTo begin, create a new object containing a list. I’ve called mine fxs, short for “functions”.\n\n2\n\nAdd named elements to the list, with the name on the left and the formula to the right of the =.\n\n3\n\nInstead of using the familiar name = instructions format, we’re instead use dplyr::across() with the list of functions instead of a single function.\n\n\n\n\n\n\n  \n\n\n\nAs described briefly above, the elements in the fxs list have a special format. The first bit, e.g. mean =, gives each element a name. This name will be appended to the relevant column in the summarise() output, so choose something informative and brief. The second bit, e.g. ~ mean(.x, na.rm = TRUE), is the function we want to apply to each variable. The two things to note are the “twiddle” ~, which denotes “this is a function to apply”, and .x, which is a placeholder for each of the variables that the function will be applied to.\nWithin across(), we are building on what we’ve seen before with this function. The first argument selects which variables to use using &lt;tidyselect&gt;. In this case, I’ve selected all of the variables that contain “score”, which will be our subscale composites for the STICSA and the STARS. The second argument provides a list of function(s) to apply to all of the selected variables. So, I’ve put in the list I made in the previous step that contains all the functions I want to use.\nThis function list + dplyr::across() method is extremely versatile. If you are using a lesser-known statistical technique, or even functions of your own making, you can easily add them to your list of functions and apply them with across()."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html#formatting-with-kableextra",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html#formatting-with-kableextra",
    "title": "06: Mutate and Summarise",
    "section": "Formatting with {kableExtra}",
    "text": "Formatting with {kableExtra}\nOnce we have these lovely summary tables, it would be great to include them in a report or paper, but they definitely need some formatting first. We’ll look at kable here, which is what we teach UGs. We’ll be using a wrapper for the knitr::kable() function, kableExtra::kbl(), because {kableExtra} has a bunch of useful tools for customising the output. However, you may also want to check out the {gt} package for powerful table formatting options, and of course {papaja} for some APA-like defaults.\n\n\n\n\n\n\nTip\n\n\n\nFor all things kable, the Create Awesome HTML Table vignette is my go-to for options and examples!\n\n\n\nEssential Formatting\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the instructions to create your own beautiful table.\n\n\nTo begin, let’s take the summarise() code grouped by mcq and save it in a new object. Then, pipe that new summary dataset object into the kableExtra::kbl() function, and then on again into the kableExtra::kable_classic() function. This one-two combo first creates a basic table out of the dataset (kbl()), then apply some APA-ish HTML formatting (kable_classic()).\n\nanx_mcq_sum &lt;- anx_data |&gt;\n  dplyr::group_by(mcq) |&gt;\n  dplyr::summarise(\n    n = dplyr::n(),\n    sticsa_trait_mean = mean(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_sd = sd(sticsa_trait_score, na.rm = TRUE),\n    sticsa_trait_min = min(sticsa_trait_score, na.rm = TRUE), \n    sticsa_trait_max = max(sticsa_trait_score, na.rm = TRUE)\n  )\n\nanx_mcq_sum |&gt; \n  kableExtra::kbl() |&gt; \n  kableExtra::kable_classic()\n\n\n\n\nmcq\nn\nsticsa_trait_mean\nsticsa_trait_sd\nsticsa_trait_min\nsticsa_trait_max\n\n\n\n\nmaths\n233\n2.168437\n0.6462829\n1.000000\n3.904762\n\n\nstats\n232\n2.248214\n0.6103040\n1.095238\n3.714286\n\n\n\n\n\n\n\nSo, already we have some basic formatting. There are a few things to do to make this table Look Nice:\n\nCapitalise the values “maths” and “stats” in the first column\nFormat the column names\nRound the values to two decimal places.\nAlign the columns in the center.\nAdd a caption (optional).\n\nFor the first task, this is a matter of changing the values in the dataset - so we can do this with mutate() before we pass the dataset on to the table.\n\n\n\n\n\n\nExercise\n\n\n\nCapitalise the values “maths” and “stats” in the first column of this summary dataset.\nHint: Check out the {stringr} package for working with character strings.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe {stringr} package has a few handy functions for converting strings into particular capitalisation patterns. Here str_to_sentence() (for sentence case, with the first letter of the string capitalised) or str_to_title() (for title case, with the first letter of each word in the string capitalised) will do the same job.\n\nanx_mcq_sum &lt;- anx_mcq_sum |&gt; \n  dplyr::mutate(\n    mcq = stringr::str_to_title(mcq)\n  )\n\nanx_mcq_sum\n\n\n\n  \n\n\n\n\n\n\n\n\n\nUsing regex\n\n\n\n\n\nI’m a pretty dedicated regex fan, but for quick things like case changes, I find {stringr} easiest. If you happen to want to do this with base R regex instead, here’s an option:\n\nanx_mcq_sum |&gt; \n  dplyr::mutate(\n    ## Substitute the first character in the string with uppercase\n    mcq = gsub(\"^(.)\", \"\\\\U\\\\1\", mcq, perl = TRUE)\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFor the other changes, we can adjust these things in kbl(), which has col.names, digits, align, and caption arguments. For the caption, keep in mind if this table will be in a longer Quarto report, you might want to instead use cross-referencing and table captions via Quarto code chunk options instead of within kbl().\n\n\n\n\n\n\nExercise\n\n\n\nUsing the help documentation, update the kbl() function to have nicely formatted column names, digits rounded to two decimal places, centre-aligned columns, and a caption.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_mcq_sum |&gt; \n  kableExtra::kbl(\n    col.names = c(\"MCQ Type\", \"N\", \"M\", \"SD\", \"Min\", \"Max\"),\n    digits = 2,\n    caption = \"Descriptives for STICSA Trait Anxiety Score by MCQ Type\",\n    align = \"c\"\n  ) |&gt; \n  kableExtra::kable_classic()\n\n\nDescriptives for STICSA Trait Anxiety Score by MCQ Type\n\n\nMCQ Type\nN\nM\nSD\nMin\nMax\n\n\n\n\nMaths\n233\n2.17\n0.65\n1.0\n3.90\n\n\nStats\n232\n2.25\n0.61\n1.1\n3.71\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-warning title: “Error Watch: dimnames not equal to array extent” collapse=“true”}\nThis error is unfortunately both very easy to generate and totally opaque. All it usually means is that the number of names (“dimnames”) you have given col.names isn’t the same as the number of columns in the dataset you’re formatting (the “array extent”).\n\nanx_mcq_sum |&gt; \n  kableExtra::kbl(\n    ## if I forget to include a column name for the MCQ grouping variable\n    col.names = c(\"N\", \"M\", \"SD\", \"Min\", \"Max\"),\n    digits = 2,\n    caption = \"Descriptives for STICSA Trait Anxiety Score by MCQ Type\",\n    align = \"c\"\n  ) |&gt; \n  kableExtra::kable_classic()\n\nError in dimnames(x) &lt;- dn: length of 'dimnames' [2] not equal to array extent\n\n\nTo fix it, just double-check the columns in the dataset and make sure that the col.names names contain a one-to-one match for each. :::\nFinally, render your workbook document to see your hard work in all its glory!\n\n\nDynamic Formatting\n::: {.callout-note appearance=“minimal” title=“Exercise”} CHALLENGE: It will shock you to learn that I didn’t like writing out the column names in the kbl() function one by one. Can you figure out how to generate column names dynamically, instead of writing them out, and then use the “Awesome Tables” vignette to create the table below?\nNote: This is included just to demonstrate that it can be done, but is definitely not within the skills covered in these tutorials!\n\n\n\nDescriptives for STICSA Trait Anxiety Score by MCQ Type\n\n\n\n\n\n\n\n\n\n\n\n\nSTICSA Trait Anxiety Score\n\n\n\nMCQ\nN\nMean\nSD\nMin\nMax\n\n\n\n\nMaths\n233\n2.17\n0.65\n1.0\n3.90\n\n\nStats\n232\n2.25\n0.61\n1.1\n3.71\n\n\n\n\n\n\n\n::: {.callout-note collapse=“true” title=“Solution”} For the first bit - well, this is well beyond anything we’ve covered so far, including a good bit of base R and some jiggery-pokery to get only N and SD to be italicised. (Sorry.) This is the kind of puzzle that I find really useful for making myself learn things like regular expressions, but if this isn’t your bag, don’t worry!\nThe steps are annotated below, which together produce a vector of column names to pass to kbl().\n\n## sub all instances of \"sticsa_trait_\" with an empty string\n## Which just effectively deletes them\nanx_mcq_names &lt;- gsub(\"sticsa_trait_\", \"\", names(anx_mcq_sum)) |&gt; \n  ## Then convert to title case\n  stringr::str_to_title() |&gt; \n  ## substitute all instances of either \"Mcq\" or \"Sd\" to uppercase\n  gsub(pattern = \"(Mcq|Sd)\", replacement = \"\\\\U\\\\1\", perl = TRUE)\n\n## Check where we're at so far\nanx_mcq_names\n\n[1] \"MCQ\"  \"N\"    \"Mean\" \"SD\"   \"Min\"  \"Max\" \n\n## Replace the values in anx_mcq_names that match either N or SD\nanx_mcq_names[which(anx_mcq_names %in% c(\"N\", \"SD\"))] &lt;-\n  ## with HTML italics formatting\n  paste(kableExtra::text_spec(\n         c(\"N\", \"SD\"), italic = TRUE\n       ), sep = \"\")\n\n## View the final vector of column names\nanx_mcq_names\n\n[1] \"MCQ\"                                                \n[2] \"&lt;span style=\\\"  font-style: italic;   \\\" &gt;N&lt;/span&gt;\" \n[3] \"Mean\"                                               \n[4] \"&lt;span style=\\\"  font-style: italic;   \\\" &gt;SD&lt;/span&gt;\"\n[5] \"Min\"                                                \n[6] \"Max\"                                                \n\n\nWe can then replace the vector of names with the new vector we’ve just produced, adding escape = FALSE into the kbl() function so that the HTML formatting appears correctly.\nUsing the “Grouped Columns/Rows” section of the Awesome Tables vignette, we can also find the function add_header_above(), which requires us to specify which header we want above which columns using numbers. So, I have six total columns in the anx_mcq_sum summary dataset; I don’t want any header above the first two (“MCQ” and “N”), but I do want a header above the last four.\n\nanx_mcq_sum |&gt; \n  kableExtra::kbl(\n    col.names = anx_mcq_names,\n    digits = 2,\n    caption = \"Descriptives for STICSA Trait Anxiety Score by MCQ Type\",\n    align = \"c\",\n    escape = FALSE\n  ) |&gt; \n  kableExtra::add_header_above(c(\" \" = 2, \"STICSA Trait Anxiety Score\" = 4)) |&gt; \n  kableExtra::kable_classic()\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: The magic of this approach is that it can be expanded to apply to as many inputs as you give it. Take our iterated summary from the previous section. Turn the code editing the names into a function and then use that within kbl() to generate the column names.\nNote: This is quite a tough challenge that requires function writing, regular expressions/{stringr}, and conditionally manipulating string vectors. This is included just to demonstrate that it can be done, but is definitely not within the skills covered in these tutorials.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Save the summary dataset as a new object \nanx_score_sum &lt;- anx_data |&gt; \n  dplyr::group_by(mcq) |&gt; \n  dplyr::summarise(\n    across(contains(\"score\"), fxs)\n  )\n\n## Write a custom function to automatically format column names\n\nformat_kbl_colnames &lt;- function(summary_data, all_caps = NULL, italics = c(\"N\", \"SD\")){\n  \n## Get initial vector of column names\nkbl_colnames &lt;- gsub(\".*_(.*)\", \"\\\\1\", names(summary_data)) |&gt; \n  stringr::str_to_title() |&gt; \n  gsub(pattern = \"(Sd)\", replacement = \"\\\\U\\\\1\", perl = TRUE) \n\n## Identify which elements to convert to uppercase (if any) and do so\n## Could probably do this with stringr but honestly I just CBA\ncapitalise_these &lt;- kbl_colnames[which(tolower(kbl_colnames) %in% all_caps)]\n\nkbl_colnames[which(kbl_colnames %in% capitalise_these)] &lt;- toupper(capitalise_these)\n\n## Identify which elements to convert to italics (if any) and do so\n\nitalicise_these &lt;- kbl_colnames[which(kbl_colnames %in% italics)]\n\nkbl_colnames[which(kbl_colnames %in% italics)] &lt;-\n  paste(kableExtra::text_spec(\n         italicise_these, italic = TRUE\n       ), sep = \"\")\n\nreturn(kbl_colnames)\n}\n\nAs if all that weren’t bad enough, this is one of those situations where the native pipe just doesn’t cut it. Using the {magrittr} pipe, we can easily use our new custom function inside our kbl() code.\n\nanx_score_sum %&gt;%\n  kableExtra::kbl(\n    col.names = format_kbl_colnames(., all_caps = \"mcq\"),\n    digits = 2,\n    caption = \"Descriptives for STICSA Trait Anxiety Score by MCQ Type\",\n    align = \"c\",\n    escape = FALSE\n  ) |&gt; \n  kableExtra::add_header_above(c(\" \" = 1, \"STICSA Trait Anxiety\" = 4, \"STARS Asking for Help\" = 4, \"STARS Test Anxiety\" = 4, \"STARS Interpretation Anxiety\" = 4)) |&gt; \n  kableExtra::kable_classic()\n\n\nDescriptives for STICSA Trait Anxiety Score by MCQ Type\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTICSA Trait Anxiety\n\n\nSTARS Asking for Help\n\n\nSTARS Test Anxiety\n\n\nSTARS Interpretation Anxiety\n\n\n\nMCQ\nMean\nSD\nMin\nMax\nMean\nSD\nMin\nMax\nMean\nSD\nMin\nMax\nMean\nSD\nMin\nMax\n\n\n\n\nmaths\n2.17\n0.65\n1.0\n3.90\n2.74\n1.14\n1\n5\n3.41\n0.91\n1.12\n5\n2.63\n0.89\n1\n5.00\n\n\nstats\n2.25\n0.61\n1.1\n3.71\n2.93\n1.10\n1\n5\n3.43\n0.82\n1.25\n5\n2.57\n0.83\n1\n4.82\n\n\n\n\n\n\n\nYou know what? I’m still not happy because I had to write out the header names in the add_header_above() function. If you’ve got this far and you’re enjoying this kind of thing, see if you can work out how to automatically produce that as well! If you do, please email me and tell me."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html#quick-test-correlation",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html#quick-test-correlation",
    "title": "06: Mutate and Summarise",
    "section": "Quick Test: Correlation",
    "text": "Quick Test: Correlation\nWhew! Well done so far; let’s cool down with a some snazzy plots and a nice gentle correlation analysis.\nThis bit is meant to be quick, so we’ll only look briefly at what we teach in UG at Sussex. If you want more correlation fun, check out discovr tutorials 07 and 18.\n\nVisualisation\nIn first year, we teach the function GGally::ggscatmat(), which is a quick way to generate a complex plot with lots of useful info, relatively painlessly. However, ggscatmat() (if you’re wondering, that’s G-G-scat-mat, like “scatterplot matrix”) will only work on numeric variables, so we’ll need to select() the ones we want first.\n\n\n\n\n\n\nTip\n\n\n\nFunctions like ggscatmat() output a special kind of plot created with {ggplot2}, another core {tidyverse} package. The lovely thing about ggplot-creating functions like this is that they do a lot of the heavy lifting of plot creation for you - getting a bunch of the complicated structure and setup out of the way - and then you can customise the plot further using {ggplot2}.\nIf you haven’t used {ggplot2} before, we’ll work through it systematically in an upcoming tutorial. If you can’t wait, check out:\n\nThe R Graph Gallery\ndiscovr_05 on data visualisation with {ggplot2}\nR for Data Science chapter 3\nThis list of {ggplot2} resources\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSelect the four “score” variables and pipe into ggscatmat().\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(contains(\"score\")) |&gt; \n  GGally::ggscatmat()\n\n\n\n\n\n\n\n\n\nSo, this single function gets us a pretty complex plot: a matrix containing all of our variables along the top and side, with density plots on the diagonal, scatterplots on one pairwise intersection, and correlation coefficients on the other.\nThis is the only {GGally} function we teach in UG at Sussex, but to go a bit further, there’s another example that might be useful in the future.\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Use GGally::ggpairs() on the same numeric variables, but split up all the plots by gender as well.\nHint: There’s an example of this code in the introduction documentation for the lovely palmerpenguins::penguins dataset!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe “Exploring Correlations” section of the palmerpenguins document gives an example of this code.\nTo adapt it, first, remember to select both the grouping variable sticsa_trait_cat AND the numeric outcome variables.\nSecond, use the aes() function in ggpairs() to assign different colours to the different categories in sticsa_trait_cat. We’ll look at aes() a lot more next week when we get to {ggplot2}.\n\nanx_data |&gt; \n  dplyr::select(sticsa_trait_cat, contains(\"score\")) |&gt; \n  GGally::ggpairs(aes(color = sticsa_trait_cat))\n\n\n\n\n\n\nThe example of this in the {palmerpenguins} intro document also changes the default colours, which is something we’ll look at when we take a tour through {ggplot2} ourselves.\n\n\n\n\n\n\n\nTesting Correlation\n\nSingle Pairwise\nIf we wanted to perform and report a detailed correlation analysis on a single pair of variables, the easiest function to use is cor.test(). Like t.test() (which we encountered in Tutorial 01/02), this is a {stats} package that works in a very similar way. This is what we teach UGs in first year.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the help documentation for cor.test(), perform a correlation analysis between any two “score” variables of your choice in the anx_data dataset. The solution will use the formula option, but if you get it to run, you’re doing good!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun ?cor.test in the Console.\nI chose STARS test and interpretation anxiety, but whatever you chose is fine!\n\ncor.test(~ stars_test_score + stars_int_score, data = anx_data)\n\n\n    Pearson's product-moment correlation\n\ndata:  stars_test_score and stars_int_score\nt = 20.585, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6406041 0.7359424\nsample estimates:\n      cor \n0.6912697 \n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Using what we learned in the last tutorial, report the results of this analysis without typing any of the results out by hand.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nstars_cor &lt;- cor.test(~ stars_test_score + stars_int_score, data = anx_data)\n\nstars_cor_out &lt;- papaja::apa_print(stars_cor)\n\n\nA Pearson’s pairwise correlation between bill length and flipper length indicated a very strong, significant positive correlation between the two measurements (`r stars_cor_out$full_result`).\n\nWhich will render as:\n\nA Pearson’s pairwise correlation between bill length and flipper length indicated a very strong, significant positive correlation between the two measurements (\\(r = .69\\), 95% CI \\([.64, .74]\\), \\(t(463) = 20.58\\), \\(p &lt; .001\\)).\n\n\n\n\n\n\n\n\nMultiple Pairwise Adjusted\nIn second year, UGs are also introduced to the (more {tidyverse}-friendly) function correlation::correlation().\nWhy would you use this one vs cor.test()? On the good side, this function scales up to pairwise tests between as many variables as you give it. This means if you want, for instance, multiple pairwise correlations within a dataset, this is the way to go, since it will apply a familywise error rate correction by default (Holm, to be precise).\nOn the other hand, it’s a right pain to type and doesn’t play ball with {papaja}. The family of packages that {correlation} belongs to, collectively called {easystats}, has its own reporting package, appropriately called {report} - so pick your poison I guess 🤷6\n\n\n\n\n\n\nExercise\n\n\n\nSelect all the “score” variables and get pairwise correlations between all of them with correlation::correlation().\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(contains(\"score\")) |&gt; \n  correlation::correlation()\n\n\n\n  \n\n\n\n\n\n\n\n\n \nUnbelievably, we’re actually at the end of this monstrosity. How are you doing? Having fun yet? I hope so (I am!).\nWe’re now halfway through the Essentials section. In the second half, we’ll be expanding outward from data manipulation to learn {ggplot2} properly, and to speedrun some key analyses for undergraduate dissertations.\nSee you soon!"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html#footnotes",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html#footnotes",
    "title": "06: Mutate and Summarise",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, “silently” means that R overwrites the existing variable without flagging that it is doing this or asking you if you are sure, so it’s important to be aware of this behaviour (and to know what variables already exist in your dataset).↩︎\nNote that averaging Likert data is controversial (h/t Dr Vlad Costin!), but widespread in the literature. We’re going to press boldly onward anyway to not get too deep in the statistical weeds, but if you’re using Likert scales in your own research, it’s something you might want to consider.↩︎\nThe horror!↩︎\n{purrr}, cats, scratch, get it?? I’m hilarious.↩︎\nI know this because I teach first year undergraduate statistics and therefore have to, not because it’s, like, particularly obvious on the face of it.↩︎\nLook, I like my basic {stats} functions. They all look the same and work the same way, don’t require extra installations or loading, and can be reported nicely with {papaja} or custom functions. I love an htest object, me! BUT, you gotta find the functions that do what you need them to do. You won’t always use one or the other - just use the one that makes sense to you and works for the task at hand.↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "",
    "text": "This tutorial covers how to run, inspect, and report a linear model in R. For the report portion, we will cover some key features of dynamic reporting in Quarto and how to write and render professionally formatted documents."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#overview",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#overview",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "",
    "text": "This tutorial covers how to run, inspect, and report a linear model in R. For the report portion, we will cover some key features of dynamic reporting in Quarto and how to write and render professionally formatted documents."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#the-linear-model",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#the-linear-model",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "The Linear Model",
    "text": "The Linear Model\nIn this section, we will walk through the process of fitting, comparing, and reporting hierarchical linear models in R. This is not a statistics tutorial, so there will be minimal detail about how to understand or interpret the output of these commands. Instead, refer to Prof Andy Field’s {discovr} tutorials, which are the primary teaching materials for the same content in UG teaching at Sussex. All of the code and interpretation in the following section is from discovr_08, the GLM.\nThere are two key goals for this linear model walkthrough:\n\nCreate a detailed “cheatsheet” for a linear model analysis for future reference\nGet familiar with the {discovr} tutorials\n\nOf the two, the first is more important. I’d strongly recommend you open the relevant {discovr} tutorial and skim through it as you go so you’re familiar with what it contains. However, the following sections of this tutorial will present the same code and information with very abbreviated text, to serve as a quick reference.\nYou can also have them both open at once and refer to each!\n\n\n\n\n\n\nInstalling {discovr}\n\n\n\n\n\nIf you are working in the Posit Cloud workspace, the tutorials have already been installed. Please do NOT reinstall them!\nIf working elsewhere, use the following code to install the necessary packages in the Console:\n\nif(!require(remotes)){\n  install.packages('remotes')\n}\n\nremotes::install_github(\"profandyfield/discovr\")\n\nNote that there’s no need to call library(discovr) at the start of your document, since the tutorials don’t work in the document, but rather in RStudio itself.\n\n\n\n\n\n\n\n\n\nUsing the {discovr} tutorials\n\n\n\n\n\nProf Andy Field’s {discovr} tutorials provide detailed walkthroughs of both the R code and the statistical concepts of a variety of statistical analyses. They are a good place to look first to understand what your UG supervisees or advisees have been taught on a particular topic.\nThe tutorials are built in {learnr}, an interactive platform for learning and running R code. So, unlike the tutorial you’re currently reading, they must be run inside an R session.\nTo start a tutorial, open any project and click on the Tutorial tab in the Environment pane. You can run any tutorial from here, but the relevant one for the linear modelling we are working on now is discovr_08, “the GLM”. Scroll down to this tutorial and click the “Start Tutorial ▶️” button to load the tutorial.\nBecause {discovr} tutorials run within R, you don’t need to use any external documents; you can write and run R code within the tutorial itself. However, I strongly recommend that whenever you work with these tutorials, you write and run your code in a separate document, otherwise you will have no record of the code and output.\n\n\n\n\nData and Codebook\nToday’s data is the TeachingRatings dataset from the {AER} package. This dataset is built into the package, so you don’t need to read it in from anywhere. Instead, use the code below to get the data and rename it teach_tib (not necessary, just easier to type!).\n\ndata(\"TeachingRatings\", package = \"AER\")\nteach_tib &lt;- TeachingRatings\n\n\n\n\n\n\n\nInstalling {AER}\n\n\n\n\n\nAs usual, no need to install anything on the Cloud, but elsewhere install {AER} first in the Console:\ninstall.packages(\"AER\")\n\n\n\nYou can load the codebook in the Help viewer by running the following in the Console:\n?AER::TeachingRatings\n\n\nOne Predictor\nNow that we have some data, we can fit our first model with a single predictor. We will do this with the very hardworking lm() function in R, standing for “linear m odel”.\n\nGeneral Format\nThe lm() function has a lot of options (as you might expect, given the versatility and ubiquity of linear models!), but its basic format to fit a linear model with a single predictor is very simple:\nlm(outcome ~ predictor, data = dataset_name)\nIn this function, outcome ~ predictor is a formula expressing a (simplified version of) the linear model equation. Here, outcome is the name of the variable in dataset_name that contains the outcome or dependent variable y, and predictor is the name of the variable that contains the predictor or independent variable x.\n\n\n\n\n\n\nExercise\n\n\n\nUse the Codebook and the lm() function to fit a linear model predicting teaching evaluation score from beauty ratings. Save the resulting model in a new object called eval_lm.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\neval_lm &lt;- lm(eval ~ beauty, data = teach_tib)\n\nThat’s it!\n\n\n\n\n\nNow we have a new object that contains all the information about our model. We could simply call the name of this object, but the output doesn’t tell us anything besides the actual value of the b estimates (try it if you like!). Instead, we’ll use some useful functions from the {tidyverse} package {broom} to get the information we need.\n\n\nModel Fit\nTo get some common measures of model fit, we can use the function broom::glance(). The output includes \\(R^2\\), adjusted \\(R^2\\), and F and accompanying statistics in comparison to the null model (the mean of the outcome alone).\n\n\n\n\n\n\nExercise\n\n\n\nPut the eval_lm object into broom::glance() to get model fit statistics.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbroom::glance(eval_lm)\n\n\n\n  \n\n\n\n\n\n\n\n\nHelpfully, broom::glance() (and many other {tidyverse} functions) output tibbles, which means we can work with them as we already know how to do. In future tutorials, we’ll also learn more about changing and filtering tibbles that will make this even more useful. For now, we can simply note the information we get out of this function for future use.\n\n\nModel Parameters\nThe most common technique for getting a look at the details of the model is to use a function we’ve met before: summary(). The output isn’t great, but it does let us get a whole bunch of useful information quickly and directly.\n\n\n\n\n\n\nExercise\n\n\n\nPut the eval_lm object into summary() to produce the model summary output.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(eval_lm)\n\n\nCall:\nlm(formula = eval ~ beauty, data = teach_tib)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.99827    0.02535 157.727  &lt; 2e-16 ***\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05\n\n\n\n\n\n\n\n\n\n\n\n\n\nSignificance Codes\n\n\n\n\n\nThe output from many base-R {stats} functions, like this summary() output, include a line labeled Signif. codes that provide a key for understanding the notation given for significance levels in p-values.\nReading the key from left to right, we can see that a result is given three asterisks (***) when the p-value is between 0 and .001; two asterisks between .001 and .01; and so on.\nThis can be a useful visual check, especially because p-values that are very, very small are frequently expressed in scientific notation, which can make them more difficult to spot.\n\n\n\nTo get information about the b estimates for individual predictors, we can also use the function broom::tidy(). We could run this function without any other information, as we did with glance() above, but we’ll change one argument here in order to get 95% confidence intervals for b in the output as well.\nThe CIs are an improvement over summary(), and this information is a tibble rather than just text output, which will make it very handy for reporting in the future.\n\n\n\n\n\n\nExercise\n\n\n\nPut the eval_lm object into broom::tidy() and use the argument conf.int = TRUE to obtain confidence intervals.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbroom::tidy(eval_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nHierarchial Models\nNext, we may want to test the addition of further predictors in the model. We can then compare the more complex multi-predictor model to the single-predictor model.\n\n\n\n\n\n\nExercise\n\n\n\nFit a linear model with teaching evaluations as the outcome, and both beauty ratings and gender as predictors. Save the model output in a new object called eval_full_lm.\nThen, obtain model fit statistics and parameters as before.\nHint: to add a new predictor, you will need to literally add it (+) to the formula.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\neval_full_lm &lt;- lm(eval ~ beauty + gender, data = teach_tib)\n\n## Simple output with summary\nsummary(eval_full_lm)\n\n\nCall:\nlm(formula = eval ~ beauty + gender, data = teach_tib)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87196 -0.36913  0.03493  0.39919  1.03237 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.08158    0.03293  123.94  &lt; 2e-16 ***\nbeauty        0.14859    0.03195    4.65 4.34e-06 ***\ngenderfemale -0.19781    0.05098   -3.88  0.00012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5373 on 460 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \nF-statistic: 16.33 on 2 and 460 DF,  p-value: 1.407e-07\n\n## Tibble output and CIs with broom\nbroom::glance(eval_full_lm)\n\n\n\n  \n\n\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat About Interactions?\n\n\n\n\n\nThe models we’re describing here contain only additions, not interactions. So, you may be wondering, “What if I want to model more complex relationships between predictors?” If that’s what you’re trying to do now, you may want to jump ahead to discovr_10 for moderation and mediation.\nOtherwise, we will get there in this course eventually!\n\n\n\n\nComparing Models\nNow we have two models, one simpler with only a single predictor, and the other with two predictors. We might next want to test whether the more complex, two-predictor model is a significant improvement over the simpler model, in order to decide which model to retain. We can do this with the anova() function1 to compare the two models.\n\n\n\n\n\n\nWarning\n\n\n\nThe anova() function will only work for model comparison for particular models.\n\nThe models must be hierarchical. That is, the more complex model(s) must contain all of the predictor(s) present in the less complex model(s).\nAll models must be fit to the same dataset. If, for example, your first predictor has no missing values, but your second predictor had one, the model with using the second predictor would have been fit to a dataset of a different size than the model using only the first, and the anova() function will throw an error to this effect.\n\nIf you encounter this issue, you may need to inspect and clean your dataset before you proceed with analysis!\n\n\n\n\n\n\n\n\nExercise\n\n\n\nPut both model objects into the anova() function to find out which model to retain.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanova(eval_lm, eval_full_lm)\n\n\n\n  \n\n\n\n\n\n\n\n\nThe F-test is significant, indicating that the more complex two-predictor model is a significant improvement over the one-predictor model, so we will proceed with the two-predictor model.\n\n\nStandardised Bs\nYou may have noticed that the output we’ve seen so for only contains unstandardised model parameter estimates. If we want standardised Bs expressing the relationship between predictor(s) and outcome in standard deviation units, we can make use of the model_parameters() function from the {parameters} package to standardise our bs.\n\n\n\n\n\n\nExercise\n\n\n\nUse the standarize = \"refit\" argument in the parameters::model_parameters() function to obtain standardised Bs.\n\n\n\n\n\n\nWarning\n\n\n\nNote the spelling of standardize with a “z”! Spelling it with an “s” will not rescale the parameter estimates.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nparameters::model_parameters(eval_full_lm, standardize = \"refit\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAssumptions Checks\nAt Sussex, we teach a range of model diagnostics and robust model sensitivity tests in order to test model assumptions. We will look briefly at each of these in turn.\n\n\n\n\n\n\nTip\n\n\n\nRemember, there are more examples and longer explanations in the discovr_08 tutorial!\n\n\n\nResidual Plots\nTo begin, we can generate nicely formatted, customisable residual plots using the function ggplot2::autoplot(). However, it is essential to load the {ggfortify} package in order for this to work!\n\n\n\n\n\n\nExercise\n\n\n\nLoad the {ggfortify} package and use the autoplot() function to generate residual plots for eval_full_lm. Set the which argument to c(1, 3, 2, 4).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ggfortify)\n\nggplot2::autoplot(eval_full_lm, which = c(1, 3, 2, 4))\n\n\n\n\nIf you’re wondering what’s up with which, the plot() function that autoplot() is based on has a total of six plots available. Here I’ve chosen the two residual plots, the normal Q-Q and the Cook’s distance plot. Plots 5 and 6 have to do with leverage, which we don’t teach at UG level. To see them, simply add them to the which = argument.\n\n\n\n\n\nIf you want to customise the theme or look of these plots further, they are built with {ggplot2} so you can add or change anything about them using that package. We will come round later to a detailed exploration of data visualisations with {ggplot2}.\n\n\nDistribution of Standardised Residuals\nIn the UG core statistics modules at Sussex, we teach that normality is the least important of the assumptions of the linear model - the most important being additivity and linearity - so we do not generally worry too much about normally distributed standardised residuals, especially in large sample sizes. However, we do teach them how to evaluate the proportion of standardised residuals with values above ±1.96 (approximately 5%), ±2.56 (approximately 1%), and above ±3 (likely an outlier). For details on how to use broom::augment() to obtain standardised residuals and other model diagnostic measures like Cook’s distance, see the discovr_08 tutorial.\n\n\nRobust Models\nAt UG level, we teach robust models for two purposes:\n\nAs sensitivity tests to check assumptions. If a robust technique that adjusts for a particular issue, such as heteroscedasticity, results in a model that is substantially different from the unadjusted model, we might conclude that the unadjusted model did in fact have that particular issue.\nAs robust alternatives to the unadjusted model.\n\n\nRobust Parameter Estimates\nOur first robust model re-estimates the parameter estimates using robust techniques with the robust::lmRob() function. We can then compare the robust parameter estimates to the unadjusted ones obtained with lm() to find out if our estimates were biased, and report the robust parameter estimates if they were.\n\n\n\n\n\n\nExercise\n\n\n\nFit the same two-predictor model again with robust::lmRob() and compare the results to the unadjusted two-predictor model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\neval_lm_rob &lt;- robust::lmRob(eval ~ beauty + gender, data = teach_tib)\n\nbroom::tidy(eval_lm_rob, conf.int = TRUE)\n\n\n\n  \n\n\n## Compare to the unadjusted model\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nComparing the values of the two versions of the model, we can see that the parameter estimates have changed very little, so we might conclude that the unadjusted model was fine.\n\n\n\n\n\n\n\nRobust CIs and p-values\nTo test and adjust for heteroscedastic residuals, we can re-estimate the standard error using a robust method. To do this, we’ll use the parameters::model_parameters() function with the argument vcov = \"HC4\" as recommended in the {discovr} tutorial. To do this, use the unadjusted model object eval_full_lm as the first argument.\n\n\n\n\n\n\nExercise\n\n\n\nUse parameters::model_parameters() to re-estimate the SEs, CIs, and p-values, and compare the results to the unadjusted model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nparameters::model_parameters(eval_full_lm, vcov = \"HC4\")\n\n\n\n  \n\n\n## Compare to the unadjusted model\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nThe parameter estimates will not change, but the SEs, CIs, and p-values may. Here, the values are nearly identical and there are no major changes - that is, no predictors have become non-significant that were previously significant - so we might again conclude that the unadjusted model was not unduly biased.\n\n\n\n\n\n\n\nBootstrapping\nIf we had a small sample size, a final option would be to bootstrap the confidence intervals. To do this, we will again use parameters::model_parameters(), but this time with bootstrap = TRUE.\nNote: Sample size is not an issue with this dataset (N = 463).\n\n\n\n\n\n\nExercise\n\n\n\nProduce bootstrapped confidence intervals for the two-predictor model and compare to the unadjusted confidence intervals.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nparameters::model_parameters(eval_full_lm, bootstrap = TRUE)\n\n\n\n  \n\n\n## Compare to the unadjusted model\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nWe could once again note that there are no major changes, as previously, so the evidence of our checks suggests that the original, unadjusted model was not unduly biased."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#quarto",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#quarto",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto documents are a mix of text and code, the next generation of R Markdown documents. For our purposes, we will be using R within Quarto, but Quarto documents support the integration of many different coding languages, including Python, Julia, and Observable. If you’ve previously used Rmd (RMarkdown), Quarto is backwards-compatible and will able to render most documents with no issues.\n\n\n\n\n\n\nHelp with Quarto\n\n\n\nThe official Quarto Guide is extensive, detailed, and extremely helpful. It’s always the best first stop for any questions you have about using Quarto. Quarto also offers detailed tutorials.\nThis quick-reference to Markdown formatting is particularly helpful.\nProf Andy Field has also recorded a series of video guides to using Quarto that are used in UG teaching.\n\n\n\nGetting Started\nTo get some hands-on practice working with Quarto, we will create a new Quarto document from scratch. We will use the analysis code we’ve already written to create a nicely formatted report.\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new Quarto document via File &gt; New File &gt; Quarto Document.\nIf you like, you can give it a title; you will be able to change this later.\nThen, click “Create”.\n\n\nBy default, your new Quarto document will already have some settings and content to demonstrate how it works. Most importantly, you can see that there are three main types of information in this document:\n\nThe YAML header at the top, delineated by ---s, which contains information about how the document will be rendered\nThe body text, which contains regular (i.e. non-code) text\nThe code chunks, which contain code - in this case, specifically, R.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTo complete our setup, take the following steps:\n\nDelete everything in the new Quarto document except for the YAML header (i.e. all the text and code chunks).\nClear your Environment by clicking the broom icon in the Environment tab.\nRestart your R session (via Session &gt; Restart R).\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you have completed and saved your worksheet with all of your code before you do this!\n\n\n\n\n\n\n\n\n\n\nVisual vs Source Mode\n\n\n\n\n\nA new feature with Quarto is Visual mode, which is very like using Word or a similar word processing programme. All formatting can be applied and previewed in the document itself, using keyboard shortcuts or the familiar formatting buttons along the top toolbar.\nVisual mode also has an “insert anything” shortcut, /, that allows you to quickly insert elements into your document. If you type / at the start of a new line (or Ctrl/Cmd + / otherwise), a drop-down list of possible elements will appear, which you can navigate by scrolling or search by typing.\nAlternatively, you can toggle to Source mode by clicking the “Source” button at the very top left of the document pane. In Source mode you can edit the document using Markdown formatting. This allows more fine-tuned control of elements, but automatic formatting and the / shortcut don’t work in this mode.\nWhich you use is entirely personal preference - I toggle regularly between them depending on what I’m trying to do, so pick whichever works for you.\n\n\n\n\n\nCreating a Code Chunk\nTo prepare for our analysis, we will need a place to load packages and read in the data. Any executable code - that is, code that you want to run and do something - must be written in a code chunk and NOT in the body text2.\nThere are several ways to insert a new code chunk.\n\nIn Visual mode, by typing / and then Enter (since “R Code Chunk” is the first option)\nIn either mode, by:\n\nClicking the green “insert code chunk” button in the top right\nTyping out the literal code fencing: ```{r} ```\nUsing a keyboard shortcut.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new code chunk and code that does the following:\n\nLoad all the packages we used in the previous Linear Model section\nRead in the dataset\nCreate the two objects containing the linear models with one predictor and with two predictors.\n\nThen, run the code chunk.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLoad packages:\nIn the previous section, we used the following packages:\n\n{tidyverse}, or specifically {broom}, for tidying up the linear model output\n{ggfortify} for creating diagnostic plots\n{parameters} for standardising bs, re-estimating CIs, and boostrapping\n{robust} for robust parameter estimates\n\nRead in the data:\nUse the code from the Data and Codebook section of this tutorial to read in the dataset and rename it if you want.\nCreate the models:\nCopy and paste the code from your workbook document, or from the previous sections of this tutorial. You can also find all of the commands you have run your History tab (next to Environment).\nAltogether, your new code chunk should look like this:\n```{r}\nlibrary(tidyverse)\nlibrary(ggfortify)\nlibrary(parameters)\nlibrary(robust)\n\ndata(\"TeachingRatings\", package = \"AER\")\nteach_tib &lt;- TeachingRatings\n\neval_lm &lt;- lm(eval ~ beauty, data = teach_tib)\neval_full_lm &lt;- lm(eval ~ beauty + gender, data = teach_tib)\n```\nRun the code chunk by clicking the green “play” arrow, or by pressing Ctrl/Cmd + Shift + Enter while your cursor is inside the chunk.\n\n\n\n\n\n\n\nBody Text\n\nHeadings\nTo begin, we’ll use headings to map out our document. There are several ways to insert a new heading.\n\nIn Visual mode, by:\n\nUsing the text formatting dropdown. Click on “Normal” and select the heading level.\nUsing a keyboard shortcut, Ctrl + Alt + the number of the heading (e.g. 3 for a level 3 heading)\n\nIn either mode, by:\n\nTyping hashes at the start of a new line, followed by a space (e.g. ### creates a level 3 heading)\n\n\n\n\n\n\n\n\nWhy Use Headings?\n\n\n\n\n\nProperly formatted headings are strongly recommended for any documents you write, for a variety of reasons:\n\nThey automatically create the outline (to the right) and navigation menu (to the bottom) of your document for easy navigation\nThey can be automatically converted into a table of contents\nThey are a crucial accessibility feature for navigating the document via keyboard/screenreader, as well as providing clear visual structure.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate 2-3 level 2 headings in your document for the brief report of the linear model. You can choose anything you like, but the three I will refer to will be called “Model Comparison” (constructing and comparing the two models), “Assumptions Checks”, and “The Final Model” (reporting the final model in full).\n\n\n\n\nText\nAny new-line text will automatically be plain text. This can be anything you like, although in our current document, we are writing a mock-formal results section.\nHow to format body text depends on the mode you are in.\n\nIn Visual mode, by:\n\nUsing familiar keyboard shortcuts (e.g. Ctrl/Cmd + B for bold, Ctrl/Cmd + I for italics, etc.)\nUsing the formatting toolbar at the top of the document.\n\nIn Source mode, by using Markdown formatting.\n\n\n\n\n\n\n\nExercise\n\n\n\nUnder the first heading (which I have called “Model Comparison”), write a brief, journal-style description of the two models. Don’t fill in any statistics, but leave placeholders where the statistical reporting will go.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf you aren’t inclined to write your own, here’s a brief sample text to use.\n\nTwo linear models were constructed to investigate the influences on teaching evaluation ratings. The first model contained only instructor beauty ratings as a predictor, and teaching evaluation ratings as the outcome (R2 = ???, F(???, ???) = ???, p = ???). The second model added instructor gender as a second predictor with no interaction, both again predicting teaching evaluation ratings (R2 = ???, F(???, ???) = ???, p = ???). An ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(???, ???) = ???, p = ???).\n\n\n\n\n\n\n\n\n\nDynamic Reporting\nThe paragraph from the previous exercise was very clearly missing some key information: the actual statistical results of the linear model analysis. To start with, the last sentence should report which of the two models was better, based on the result of our F-test, rather than question marks. We could produce the output of this test, read it ourselves with our very own eyes/ears, and then type out the results by hand…but that is definitely not what we are going to do! Instead, we’ll look at a couple options for reporting the results dynamically. (We’ll come back to the R2s and Fs for each model in a moment.)\n\nInline Code\nOur first option is to use inline code to report the numbers. In order to do this, we first need to know what information we have available in the test output, so we can make use of it.\n\n\n\n\n\n\nTip\n\n\n\nThis section will make extensive use of $ subsetting, which was covered in Tutorial 03, and [] subsetting, which was covered in Tutorial 01/02.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIn your report document, in a new code chunk, use the broom::tidy() function to get a nice tibble of the anova() comparison between the two models, and save it in a new object called tidy_f. Then, print out this object in the Console.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe are creating the tidy_f object in a code chunk because we will want to use it again in our report.\nWe are calling this object in the Console because viewing its contents is only for our information/reference, and not something we want to appear or use directly in our finished document.\n\n## In a code chunk\ntidy_f &lt;- broom::tidy(anova(eval_lm, eval_full_lm))\n\n## In the Console\ntidy_f\n\n\n\n  \n\n\n\n\n\n\n\n\nThe tibble we get here is quite different from the “Analysis of Variance Table” text output that we saw earlier when we printed out the results of the anova() function on its own. We now have all the values we need to report these results in a conveniently subsettable tibble with nice R-friendly names.\nSo, let’s have a go getting at some of those values.\n\n\n\n\n\n\nExercise\n\n\n\nGet out the F-ratio of 15.0554899 from this object, and round it to two decimal places.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to get out the statistic variable, which we can do with $.\n\ntidy_f$statistic\n\n[1]       NA 15.05549\n\n\nThis returns a vector of two values, NA and 15.0554899. To index the second of these values, we need [] subsetting and the index number of the correct value.\n\ntidy_f$statistic[2]\n\n[1] 15.05549\n\n\nFinally, we can use the round() function to round to two decimal places.\n\nround(tidy_f$statistic[2], 2)\n\n[1] 15.06\n\n\n\n\n\n\n\nWe now have a bit of R code that produces a single number that we want to report in the text3. The issue is that code chunks contain code, and body text contains text, but we would like the code that we’ve written to print out its value in the text!\nThe solution is inline code, a small bit of R code written in the body text (“inline”) that will, when the document is rendered, automatically insert the right number. Inline code is written as follows, in the text: `r some_r_code`. Note the backticks; these are NOT apostrophes, and are typically located on the top left of a UK keyboard to the lefter of the 1/! key.\nFor our reporting, we will need to insert the inline code in exactly the spot where we would like the output of the code to appear. This particular bit of code produces the F-statistic, so we can replace the “???”s in our reporting where the value of the F-statistic should go:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(???, ???) = `r round(tidy_f$statistic[2], 2)`, p = ???).\n\nYou can check whether this has worked in two ways:\n\nPlace your cursor inside the backticks and press Ctrl/Cmd + Enter, as you would inside a code chunk. This will run the code and a small pop-up will show you what that code will produce when rendered.\nRender your document and see what appears!\n\n\n\n\n\n\n\nExercise\n\n\n\nUse inline code to replace all of the ???s in the F-test reporting to produce a final report in APA style.\nHint: Rounding p-values is a bit tricky. Check out the {papaja} package to see if you can find a function besides round() that will produce the p-value with the correct formatting.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAll of the other numbers should be straightforward, except for the p-value. Rounding to three decimal places with round() will result in a value of 0, which is not what we want. Instead, since the p-value is below .001, we want “&lt; .001”.\n{papaja} has the printp() function, which will do this exactly was we like (as well as containing a lot of other useful functions for rounding and printing in APA style!)\nYour final text may look like this:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(`r tidy_f$df[2]`, `r tidy_f$df.residual[2]`) = `r round(tidy_f$statistic[2], 2)`, p `r papaja::printp(tidy_f$p.value[2]`).\n\nNote that there’s no need for a “&lt;” symbol because papaja::printp() includes it automatically.\nThis will render as:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(1, 460) = 15.06, p &lt; .001).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Create a bit of inline code that will either report a significant or non-significant result depending on the value of p.\nHint: You may need to check out the ifelse() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we really want our reporting to be resilient, we want to remove or replace all the places where we have to manually remember to update the code if our results change. In this case, our reporting reads:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit…\n\nBut if we wanted to reuse this code for another report, we would have to remember to update this depending on the actual value of p. OR, we can have R do it for us.\nFirst, we need to write a bit of R code that will evaluate whether the value of p is above or below a particular threshold, and then output the correct text. We could do this with ifelse(), a handy little base R function with three arguments. The first is a test returning a logical value (either TRUE or FALSE). If the test returns TRUE, the second argument is executed; if the test returns FALSE, the final argument is executed.\n\nifelse(\n1  tidy_f$p.value[2] &lt; .05,\n2  \"significant\",\n3  \"non-significant\"\n)\n\n\n1\n\nThe test: is the p-value for our F-test less than .05? (If you have a different \\(\\alpha\\)-threshold, you could hard-code it here or use an object you’ve defined previously for this comparison.)\n\n2\n\nWhat to do if the test returns TRUE: print “significant”.\n\n3\n\nWhat to do if the test returns FALSE: print “non-significant”.\n\n\n\n\n[1] \"significant\"\n\n\nWe can then take this command and replace the word “significant” in our report with this inline code:\n\nAn ANOVA comparing the models indicated a `r ifelse(tidy_f$p.value[2] &lt; .05, \"significant\", \"non-significant\")` improvement in model fit…\n\nNow we don’t have to worry about getting this right: our document, when rendered, will automatically insert the right word depending on the data.\n\n\n\n\n\n\n\nAutomatic Reporting\nAs helpful as inline code is (and I would recommend reporting all values dynamically/automatically wherever possible, so it is very useful!), you may have noticed that there was a lot of repetitive typing that also made the text itself quite difficult to read, as well lots of opportunities make typos or mistakes. Surely there’s a simpler way to do this sort of thing!4\nThere are, in fact, many simpler ways to do common tasks like this, which take advantage of the fact that an object created by a particular function will always have the same structure. One option is to make further use of the {papaja} package, which is designed for just this purpose.\n\n\n\n\n\n\nExercise\n\n\n\nUse the {papaja} documentation to fill in the statistical reporting for each of the linear models (i.e., R2, F, and p) using only one piece of inline code for each.\nWhen you’re done, render your document to see the results!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe {papaja} documentation illustrates the process using t.test(), which works the same way as lm(). The key here is to use the original objects containing the models you want to report.\nLet’s have a look at the first of the two models and see what the papaja::apa_print() function gives us.\n\npapaja::apa_print(eval_lm)\n\n$estimate\n$estimate$Intercept\n[1] \"$b = 4.00$, 95\\\\% CI $[3.95, 4.05]$\"\n\n$estimate$beauty\n[1] \"$b = 0.13$, 95\\\\% CI $[0.07, 0.20]$\"\n\n$estimate$modelfit\n$estimate$modelfit$r2\n[1] \"$R^2 = .04$\"\n\n$estimate$modelfit$r2_adj\n[1] \"$R^2_{adj} = .03$\"\n\n$estimate$modelfit$aic\n[1] \"$\\\\mathrm{AIC} = 756.65$\"\n\n$estimate$modelfit$bic\n[1] \"$\\\\mathrm{BIC} = 769.06$\"\n\n\n\n$statistic\n$statistic$Intercept\n[1] \"$t(461) = 157.73$, $p &lt; .001$\"\n\n$statistic$beauty\n[1] \"$t(461) = 4.13$, $p &lt; .001$\"\n\n$statistic$modelfit\n$statistic$modelfit$r2\n[1] \"$F(1, 461) = 17.08$, $p &lt; .001$\"\n\n\n\n$full_result\n$full_result$Intercept\n[1] \"$b = 4.00$, 95\\\\% CI $[3.95, 4.05]$, $t(461) = 157.73$, $p &lt; .001$\"\n\n$full_result$beauty\n[1] \"$b = 0.13$, 95\\\\% CI $[0.07, 0.20]$, $t(461) = 4.13$, $p &lt; .001$\"\n\n$full_result$modelfit\n$full_result$modelfit$r2\n[1] \"$R^2 = .04$, $F(1, 461) = 17.08$, $p &lt; .001$\"\n\n\n\n$table\nA data.frame with 6 labelled columns:\n\n       term estimate     conf.int statistic  df p.value\n1 Intercept     4.00 [3.95, 4.05]    157.73 461  &lt; .001\n2    Beauty     0.13 [0.07, 0.20]      4.13 461  &lt; .001\n\nterm     : Predictor \nestimate : $b$ \nconf.int : 95\\\\% CI \nstatistic: $t$ \ndf       : $\\\\mathit{df}$ \np.value  : $p$ \nattr(,\"class\")\n[1] \"apa_results\" \"list\"       \n\n\nWe’ve got a huge number of options here, but for this exercise we wanted R2, F, and p. All three are given under $full_result$modelfit$r2. We will need to save the output from apa_print() into an object, then we can subset it using inline code:\n\neval_lm_out &lt;- papaja::apa_print(eval_lm)\neval_full_lm_out &lt;- papaja::apa_print(eval_full_lm)\n\n\nTwo linear models were constructed to investigate the influences on teaching evaluation ratings. The first model contained only instructor beauty ratings as a predictor, and teaching evaluation ratings as the outcome (`r eval_lm_out$full_result$modelfit$r2`). The second model added instructor gender as a second predictor with no interaction, both again predicting teaching evaluation ratings (`r eval_full_lm_out$full_result$modelfit$r2`).\n\nWhich will render as:\n\nTwo linear models were constructed to investigate the influences on teaching evaluation ratings. The first model contained only instructor beauty ratings as a predictor, and teaching evaluation ratings as the outcome (\\(R^2 = .04\\), \\(F(1, 461) = 17.08\\), \\(p &lt; .001\\)). The second model added instructor gender as a second predictor with no interaction, both again predicting teaching evaluation ratings (\\(R^2 = .07\\), \\(F(2, 460) = 16.33\\), \\(p &lt; .001\\)).\n\n\n\n\n\n\n\n\nTable Formatting\nNext, we will jump to the “Final Model” heading and have a look at how to turn our final model output into a nicely formatted table. Once again, {papaja} provides a quick and beautiful solution for reporting, so let’s use it again.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the {papaja} help documentation, produce a nicely formatted table of the final model, presenting the parameter estimates, p-values etc. for each predictor under the third (“Final Model”) heading.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe already have the necessary object, eval_full_lm_out, from the previous task. We just need to subset it as described in the help documentation.\nThis command should go in a new code chunk, wherever you want the table to appear in your document.\n\npapaja::apa_table(eval_full_lm_out$table)\n\n\n(#tab:unnamed-chunk-21)\n\n\n**\n\n\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n4.08\n[4.02, 4.15]\n123.94\n460\n&lt; .001\n\n\nBeauty\n0.15\n[0.09, 0.21]\n4.65\n460\n&lt; .001\n\n\nGenderfemale\n-0.20\n[-0.30, -0.10]\n-3.88\n460\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: {papaja} isn’t the only package to provide easy formatting for commonly reported tests. Have a go creating this table again using the nice_table() function from the {rempsyc} package.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe nice_table() function can be for tables generally, but it can apply specialised formatting for model tables created with broom, if we use the broom = argument to tell the function what formatting template to apply.\n\nrempsyc::nice_table(broom::tidy(eval_full_lm, conf.int = TRUE), broom = \"lm\")\n\n\nTermbSEtp95% CI(Intercept)4.080.03123.94&lt; .001***[4.02, 4.15]beauty0.150.034.65&lt; .001***[0.09, 0.21]genderfemale-0.200.05-3.88&lt; .001***[-0.30, -0.10]\n\n\n\n\n\n\n\n\n\nCross-Referencing\nAs anyone who has had to create a long document with lots of tables and figures knows, keeping track of the numbering is a huge pain, especially when, for instance, a reviewer asks you to add something partway through and then everything has to be renumbered.\nThe good news is that Quarto can take care of figure and table numbering automatically. There are two steps to this:\n\nInclude a label in the relevant code chunk, using the prefix fig- for figures and tbl- for tables.\nRefer to the figure or table in the text using @.\n\n\n\n\n\n\n\nExercises\n\n\n\nUsing the Quarto help documentation, write a short introductory sentence under the “Final Model” heading and refer to the final model table with a cross-reference.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, add a label and caption to the code chunk from the previous task that produces the model table.\n```{r}\n#| label: tbl-final-model\n#| tbl-cap: \"The final model predicting teaching evaluation ratings from instructor beauty and gender.\"\n\npapaja::apa_table(eval_full_lm_out$table)\n```\nYou can of course write whatever you like, or borrow the text below, but use whatever label you gave the table code chunk to refer to the table.\n\nThe final model with two predictors is presented in full in @tbl-final-model.\n\nAltogether, it should render as follows:\n\nThe Final Model\nThe final model with two predictors is presented in full in Table 1.\n\n\npapaja::apa_table(eval_full_lm_out$table)\n\n\n(#tab:tbl-final-model)\n\n\n**\n\n\n\nTable 1: The final model predicting teaching evaluation ratings from instructor beauty and gender.\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n4.08\n[4.02, 4.15]\n123.94\n460\n&lt; .001\n\n\nBeauty\n0.15\n[0.09, 0.21]\n4.65\n460\n&lt; .001\n\n\nGenderfemale\n-0.20\n[-0.30, -0.10]\n-3.88\n460\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Complete the final “Assumptions Checks” section summarising the checks and using figure cross-referencing to insert and refer to the diagnostic plots.\nHint: To report the exact maximum value of Cook’s distance, you will also need to refer to discovr_08 for how to use broom::augment().\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can write whatever you like, but here’s a suggestion with the plot included.\n\nAssumptions Checks\nWe next assessed the model with two predictors for any evidence of bias. Residual plots (@fig-diag-plots) did not indicate any outstanding issues with normality, linearity, or heteroscedasticity. There was also no evidence of influential cases, as the max value of Cook’s distance was `r round(max(broom::augment(eval_full_lm)$.cooksd), 2)`. Robust models were also fitted as sensitivity checks. Robust parameter estimates estimated using the {robust} package were minimally different from the unadjusted parameter estimates. Similarly, robust HC4 standard errors estimated using the {parameters} package yielded confidence intervals and p-values very similar to the unadjusted values. Therefore, we will proceed with the unadjusted two-predictor model as our final model.\n\n```{r}\n#| label: fig-diag-plots\n#| fig-cap: \"Diagnostic plots for the two-predictor model.\"\n\nggplot2::autoplot(eval_full_lm, which = c(1, 3, 2, 4))\n```\n\n\n\n\n\n\n\n\nRendering\nAfter all this work to analyse, interpret, and report, it’s finally time to produce the final document. The process of turning a Quarto document into some output format - including running all the code and applying all the formatting - is called rendering (previously knitting with RMarkdown).\n\n\n\n\n\n\nExercise\n\n\n\nRender your report document using the “Render” button at the top of the document, or by using the keyboard shortcut Ctrl/Cmd + Shift + K5.\n\n\n\nGlobal Options\nAt the moment, your report may not be as clean as we’d like it to be: there are likely messages from R floating around and code all over the place, whereas for a formal report, we of course only want to show the final output. This is where the YAML header comes in.\nThe default YAML header contains only a few explicit settings, and if you haven’t changed anything, probably looks like this:\n---\ntitle: \"Untitled\"\nformat: html\neditor: visual\n---\nBy adding options to the YAML header, we can determine how the document as a whole is rendered. Here are the common ones that I use on the regular:\n\n---\ntitle: \"Linear Model Report\"\nformat:\n  html:\n1    toc: true\neditor: visual\n2self-contained: true\n3execute:\n4  echo: false\n5  warning: false\n6  message: false\n---\n\n\n1\n\nAutomatically produce a table of contents (ToC) from the document headings.\n\n2\n\nCombine all the files, images, stylesheets etc. into a single output document. Necessary if you want to send an HTML file to someone else and have it look as it should!\n\n3\n\nSet default behaviour for all code chunks as follows\n\n4\n\nRun code and show output, but do not show the code itself.\n\n5\n\nDo not show any warnings produced by code.\n\n6\n\nDo not show any messages produced by code.\n\n\n\n\nThe requirements for each document will change depending on its purpose.\n\n\n\n\n\n\nTip\n\n\n\nThe Quarto help documentation, as usual, has a complete list of YAML options, including how to set default behaviour for figures and other settings.\n\n\n\n\nCode Chunk Options\nGlobal options apply to the entire document, but you may want to change these settings for individual code chunks to override the default settings in the YAML.\nFor example, you may have a code chunk containing some processing code that you used to view and clean your data. If your global execution option is set to echo: false, the code output would still appear, although the code itself would be hidden. If, for this particular code chunk, you don’t want the output to appear either, you can override the global option with a local option for that chunk only.\nLocal code chunk options appear as “hashpipe” (#|) comments, as we have seen earlier with labels. They use the same syntax as YAML, but the settings only apply to individual code chunks. Any settings that aren’t explicitly changed within the chunk are inherited from the YAML settings.\nFor this example, we could set a local code chunk option include: false which will prevent the output from appearing in the document.6\n```{r}\n#| include: false\n\nsome_code_doing_cleaning_and_processing\n```\n\n\nOutput Formats\nFor these tutorials, we will generally stick to HTML, as it’s the most painless of the rendering options. However, you will likely find yourself wanting to produce some other type of document, which you can easily7 do from the same Quarto document.\nTo render to a different format, change the YAML format: setting to a different output.\n\n\n\n\n\n\nTip\n\n\n\nAs per, the Quarto guide on output formats has all the information you need!\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRender your linear model report to a Word document.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSimply update the format: html YAML option to format: docx and render your document. Note that the format options are usually named after the file extension rather than the name of the programme necessarily (e.g. docx rather than word.)\n\n\n\n\n\n \nAnd there we have it! You now have a complete example of a linear model report, rendered into both HTML and Word, to refer to. It’s amazing how much you’re able to do after just a few weeks!\nThis is the end of the FundRmentals section of the course. If you’re so inclined, we’ll see you in the Essentials section, which will cover data wrangling and cleaning, and running and reporting many more statistical analyses."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#footnotes",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#footnotes",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSomewhat confusingly, this is not the function we teach UGs to perform ANOVAs! See discovr tutorials 11, 12, 15, and 16 for a detailed guide through the afex package for running linear models with categorical predictors.↩︎\nMostly true, except for inline code!↩︎\nGenerally, inline code should only ever produce a single value, otherwise the formatting can get interesting. This value, however, could longer than a single number, if you want to get creative with your dynamic reporting!↩︎\nHere we’re going to briefly encounter a principle that will come up again and again, namely: If you have to copy and paste more than once, use a function instead.↩︎\nIt seems obvious that this should be R instead of K, but remember this made perfect sense when it was called “knitting”!↩︎\nFor writing tutorials, I make extensive use of eval: false, which includes the code in the output but does not attempt to run the code. This allows me to write all kinds of nonsense code without R getting stroppy and throwing errors all over the place!↩︎\nDepends crucially on what you consider to be “easy”, especially when dealing with PDFs!↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#orientation",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#orientation",
    "title": "01/02: IntRoduction",
    "section": "Orientation",
    "text": "Orientation\nWelcome to the first PsychRlogy tutorial! Let’s jump right in and get started.\nThis tutorial is designed to accompany a “Workbook” document, which is already available for you on the Posit Cloud workspace for this course. To access it, navigate to Posit Cloud and open the course workspace. If you haven’t joined the workspace yet, use the join link on the Posit Cloud page on Canvas.\nIn the workspace, you will see a list of projects available. These will have an “ASSIGNMENT” banner next to them. When you click on these projects, a new copy of the project will be generated for you to work in.\n\n\n\n\n\n\nClick on the project with the same name as this tutorial: “01/02 IntRoduction”.\n\n\n\n\nThe RStudio Interface\nYou are now looking at the RStudio IDE itself. It is possible to use R directly with minimal interface, but using an integrated development environment like RStudio comes with a lot of additional convenience to make working with R smoother, easier, and more efficient.\n\n\n\n\n\n\nImportant\n\n\n\nIt’s beyond the scope of this tutorial to cover all of the options and tools available in RStudio. Here we’ll focus only on the minimum to get started and build outward from there.\nFor a more complete tour, try this playlist of Andy Field’s RStudio tutorials.\n\n\nBefore anything else, open up the workbook for this tutorial. If you’re a little shaky on where this is, skip down to the screenshot below!\n\n\n\n\n\n\nFind the document named “01_02_intro_workbook.qmd” in the Files window. Click on it to open it.\n\n\n\nYou should now be looking at a dashboard-like interface with four main windows, each with a bunch of tabs across the top, like pictured below. We’ll refer to each window by these names, which come from the most important or commonly used tab in each window.\n\nWe will eventually work with all four of these windows, but if you’ve opened the workbook, you can ignore the Environment and Files windows for now. We’ll be focusing only on the other two: Source and Console.\n\nSource\nThe Source window is where any documents you want to create or work on will open up. What you have open now is a Quarto document, a type of document that integrates regular text and code. A Quarto document has three main elements: the YAML header, body text, and code chunks.\n\n\n\n\n\nIgnore the YAML header for now; we’ll come back to Quarto documents, including the YAML, in depth in Tutorial 04.\nYou can use the body text portion of a Quarto document more or less like you would a document in Word (or your word processor of choice). In the body of the document, you can write and format any text you want - notes, questions, thoughts, ideas, comments, etc. This workbook already contains all the headings from this tutorial to help you get organised, but please do delete or edit as you see fit.\nThe last element is the code chunk, which is where all R code should be written. Code chunks have a contrasting background colour, an {r} in the upper left corner, and two green buttons in the upper right. The one that looks like a green “play” button will run all the code in that chunk. Code chunks will NOT handle any non-code text, unless it’s a comment (i.e. preceded by one or more #s.)\n\n\nConsole\nThe Console is deceptively simple: just some stuff about the R version and acknowledgements, and the &gt; symbol with a flashing cursor after it, waiting for you to type something. However, the Console is the heart of R, where anything you want to do actually happens. Every command that you type, anything you want R to do, goes through here.\nSo, already we have two places we could write R code: in a code chunk or in the Console. How do we know where to start?\n\n\n\n\n\n\nCode Chunk or Console?\n\n\n\nFor the purposes of learning, by default, it’s best that you write all code into the workbook code chunks, so you have a detailed record of everything you’ve tried - even if it doesn’t work!\nOutside of these sessions, whether to write a bit of R code in a code chunk in Quarto, or in the Console, largely depends on a single question: do you want to use this same line of code again in the future?\nIf yes, write the code in a code chunk. By adding the code to a document like Quarto, we are creating a record of all the steps we’ve taken in whatever task we are working on. Assuming we want to be able to use and refer to that code again in the future, it should go into a document.\nIf no, write the code in the Console. Code written directly in the Console isn’t saved or documented anywhere1. Some common uses of the Console are:\n\nInstalling/updating packages\nOpening help documentation\nDrafting or testing code to go into a document.\n\nSo, I often use the Console to test my code, building it up bit by bit, until it does what I want it to do. Then, when I’ve puzzled out the solution, I add it into a code chunk.\n\n\n\n\n\n\nStill Confused? How About a Convoluted Analogy?\n\n\n\n\n\nImagine that working in R is like cooking, and writing a sequence of commands to, for example, clean a new dataset is like developing a new recipe.\nIf you’re developing a recipe, you likely wouldn’t just sit down and write down the final version if you’ve never tried the recipe before. Instead, you might experiment a bit with each step to see what works and what doesn’t.\nAlong the way, you may write notes to yourself: “Maybe try cumin?”, “Buy more kefir”, “This time was 2 tsp salt, too salty!” Those notes are a part of the development process, relevant to what you’re doing now and helpful to try out or note down ideas, but they wouldn’t go in your final recipe. Those behind-the-scenes and under-development bits are the code you’d write in the Console.\nWhen you find a technique or temperature or seasoning that works, you might add it as a step in your recipe. That final recipe, the steps that actually work the way you want, are the code in your code chunks.\n\n\n\n\n\nIf that seems like a lot to remember, don’t worry - we’ll practice both and let you know clearly if it should be one or the other.\nRight, enough orientation - let’s get cracking!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#errors",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#errors",
    "title": "01/02: IntRoduction",
    "section": "Errors",
    "text": "Errors\nBefore we go any further, an affirmation: you will, inevitably, make typos and errors using R. You will write commands that make sense to you that R doesn’t understand; and you will write commands that don’t make sense to you, that R does understand. Errors are an essential and unavoidable part of learning R, so let’s start there.\n\n\n\n\n\n\nExercise\n\n\n\nType literally any gibberish, words, keysmashes etc. into the code chunk in the workbook and press Run (or Ctrl/Cmd + Shift + Enter).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Keysmash!\naslavb;lj aew aljvb\n\nError: &lt;text&gt;:2:11: unexpected symbol\n1: ## Keysmash!\n2: aslavb;lj aew\n             ^\n\n\n\n## Words!\nAm I a coward? Who calls me villain?\n\nError: &lt;text&gt;:2:4: unexpected symbol\n1: ## Words!\n2: Am I\n      ^\n\n\n\n## Emojis! \n¯\\_(ツ)_/¯\n\nError: &lt;text&gt;:2:1: unexpected input\n1: ## Emojis! \n2: ¯\n   ^\n\n\n\n\n\n\n\nWell, that went about as well as expected.\nIf you haven’t tried this yet, and your pristine document is just ominously staring at you, I’m serious - punch your keyboard if you have to, or let your cat walk on it, or play it as if it were a piano, and press Enter. There’s two important things to learn from this:\n\nTo ask R to do something, you must write commands out somewhere (in a code chunk, in the Console) and then run them.\nEventually, inevitably, something that you type WILL produce an error.\n\nFrom our keysmashing above, you will have seen that aslavb;lj aew aljvb, Am I a coward? Who calls me villain?, and ¯\\_(ツ)_/¯ are not valid commands in R. Although each of these has a communicative function for humans, R can’t understand them. In order to get the answer that we want, we have to ask R to do something in a way it can understand, by writing commands it can parse using the R language.\nJust like learning any other language, learning to communicate with R takes time and practice, and it can be very frustrating when you and R can’t seem to understand each other. However, one advantage of learning to talk to R vs learning to speak a human language is that R always works the same way. Even if the response it gives doesn’t make sense to you, there’s always a logical reason for what it does.\n\n\n\n\n\n\nGlossoRlia: the Language of Errors\n\n\n\nVery often, R communicates with you via errors. Unlike many other computer programmes you might be familiar with, an “error” in R doesn’t (usually!) mean a catastrophic failure and/or potential loss of hours of work2. Rather, whatever you’ve asked R to do, it’s essentially replied, “Sorry, I can’t do that.” Two of the most important skills you can develop early on with R is to treat errors as feedback, and to learn to read and recognise errors.\nTreat errors as feedback: Errors aren’t (just) an annoyance, although running into lots of errors, usually just when you don’t want them the most!, can be incredibly frustrating. However, errors are just R’s way of telling you that it can’t do what you’ve asked it to do. If you’re trying to work out how to get R to do something, then the “no” of an error rules out whatever you’ve just tried. Rather than setting out to avoid errors, and thinking of an error as a “failure” to “do it right”, it’s much better to expect errors, and make use of them as part of the code-writing process.\nLearn to read and recognise errors: Errors often contain useful information about what’s gone wrong and how to fix it. At minimum, errors usually contain the following information:\n\nWhere in the document the error occurred, or which command/bit of code produced the error\nSome sort of message about what went wrong.\n\nErrors vary wildly in understandability and helpfulness, from highly technical jargon to friendly and conversational with suggestions for fixing common problems. Even the obtuse ones, though, will become familiar with time.\n\n\nLet’s take a look at how we might interpret the error we produced above.\n\naslavb;lj aew aljvb\n\nError: &lt;text&gt;:1:11: unexpected symbol\n1: aslavb;lj aew\n              ^\n\n\nHere R doesn’t need to tell us where the error is, because there’s only one thing it’s trying to run. We do have the text of the error, though: object 'aslavb' not found. We’ll come to objects later on in this tutorial, but in short R is looking for some information labeled aslavb and can’t find it (because it’s just a keysmash!). This is an example of an error that comes up all the time - often when you’ve made a typo, or forgotten to create or store information in an object properly. For me at this point, as someone who forgets or mistypes things on the reg, this error is a familiar friend!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#types-of-data",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#types-of-data",
    "title": "01/02: IntRoduction",
    "section": "Types of Data",
    "text": "Types of Data\nOne key concept for using R is the different ways it categorises data. “Data” here means any piece of information you put into R - a word, a number, the result of a command or calculation, a dataset, etc. Depending on the type of data you have, R will treat it differently, and some operations only work on certain types of data. So, let’s have a look at how R encodes and deals with different types of data. Here we’ll cover three of the most common and important: numeric, character, and logical. As we do so, we’ll practice some core skills in R.\n\nNumeric Data\nThe first, and perhaps most obvious, type of data in R is numbers. We’ll start by doing some calculations with common mathematical operators.\n\n\n\n\n\n\nExercise\n\n\n\nType any single number and run the code.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Pick any number at random\n\n3958\n\n[1] 3958\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that you can run all the code in a code chunk by pressing Ctrl/Cmd + Shift + Enter on your keyboard, or by clicking the green “play” arrow in the top right corner of the code chunk.\nYou can also run only a particular line of code, or something that you’ve highlighted, by pressing Ctrl/Cmd + Enter.\n\n\nThis might be what you’d expect. We’ve essentially asked R, “Give me 3958” (or whatever number you put in) and R obliges. The only thing that might be a surprise is the [1] marker, called an index. Basically, R has replied, “The first thing ([1]) that you asked me for is 3958.” We’ll come back to indices in a moment.\n\n\n\n\n\n\nExercise\n\n\n\nHow does R handle commas within a number (e.g. to separate the thousands place from the hundreds)? How about full stops for decimals?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n3,958\n\nError: &lt;text&gt;:1:2: unexpected ','\n1: 3,\n     ^\n\n\n\n3.958\n\n[1] 3.958\n\n\n\n\n\n\n\nSo, commas within numbers throw an error. This is because commas have an important role to play in the syntax of R, so long numbers must be inputted into R without any punctuation. However, full stops to mark decimal places are just fine.\n\n\n\n\n\n\nGrammar Check\n\n\n\n\n\nTry for a moment switching to Source mode by clicking the Source button in the upper left hand of your Quarto document. You can see that RStudio helpfully marks out the part of the code that isn’t parsable (not in “grammatical” R) with a red ❌ next to the line number, and squiggly red underlining, likely familiar from word processing programmes, under the part of the code that’s causing the issue. It won’t do this for every error, but it’s very helpful for finding “grammatical” errors like extra or missing brackets or misplaced commas.\n\n\n\nNext, let’s try doing some maths.\n\n\n\n\n\n\nExercise\n\n\n\nAdd together your shoe size and the number of windows in the room you’re currently in.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n40 + 8\n\n[1] 48\n\n\n\n\n\n\n\nImportant to note here is that we don’t need to type an = to get the answer, just the equation we want to solve and then run the code. Again, we’ve asked R, “Give me 40 + 8” (or whatever numbers you chose) and R replies with the answer.\nYou will not be surprised to learn that you can use R as a calculator to subtract, divide, and multiply as well.\n\n\n\n\n\n\nExercise\n\n\n\nTry subtracting, dividing, and multiplying the same two numbers.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n40 - 8\n\n[1] 32\n\n40 * 8\n\n[1] 320\n\n40 / 8\n\n[1] 5\n\n\n\n\n\n\n\nThis exercise also shows us something useful: you can run multiple commands within the same code chunk. While spaces are not meaningful to R (that is, 40 - 8 and 40-8 and 40      -8 are all read the same way), new lines have an important role to play separating out commands. Each command must have its own new line.\n\nVectors\nLet’s imagine I am running a study, and I want to generate some simple participant ID numbers to keep track of the order that they completed my study. I had 50 participants in total. I could do this by typing every number out one by one, but this is exactly the kind of tedious nonsense that R is great at. Instead, we’ll use the operator :, which means “every whole number between”.\n\n\n\n\n\n\nExercise\n\n\n\nPrint out every whole number between 1 and 50.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n1:50\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n\n\n\n\n\n\nNotice that the indices mentioned earlier have come up again. The first element after the [n] index is the nth element. Let’s have a look at this some more.\n\n\n\n\n\n\nExercise\n\n\n\nPrint out all the numbers 12 through 30; all of the numbers 23 through 55; and 36, all in one command.\n\n\nYou may have tried something like this:\n\n12:30\n\n [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n23:55\n\n [1] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n[26] 48 49 50 51 52 53 54 55\n\n36\n\n[1] 36\n\n\nAs you can see from the indices, this is three separate commands, because the numbered indices start over from [1] each time. However, we want all those numbers in a single command. To do this, we’ll use a function called c().\nThis is our first contact with functions in R, and we’ll explore how they work more later on. To use this one, type it out, then inside the brackets, put the numbers you want to collect (or concatenate, or combine), with different groups separated by commas.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc(12:30, 23:55, 36)\n\n [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 23 24 25 26 27 28\n[26] 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n[51] 54 55 36\n\n\n\n\n\nAs you can see from the numbered indices this time, when I put the numbers I want inside the function c(), separated by commas, R collects all of the numbers into a single series of elements, called a vector.\nActually, we’ve been looking at vectors this whole time. Any series of pieces of information in R is a vector (but see Tip box on vectors and elements). When we were looking at single numbers (like 3958 above), we were still getting a vector back from R, but it was a vector with only one element, and thus only [1].\nIf I want the nth element in the vector we’ve just created, (say, the 33rd), I can get it out by indexing with square brackets.\n\nc(12:30, 23:55, 36)[33]\n\n[1] 36\n\n\nWhat I’ve essentially asked R is, “Put all of these numbers into a single vector, and then give me the 33rd element in that vector.” As it turns out, the 33rd element in that vector of numbers is 36.\n\n\n\n\n\n\nDefinition: Vectors\n\n\n\nA vector is essentially a series of pieces of data, or elements. It is a key basic piece of how data is stored in R. When R returns a vector as the output from a command, each element is numbered in square brackets. These square brackets can also be used to index the vector to get the nth element.\nFor atomic vectors created with c() or similar operations, there are some important rules:\n\nEach element must be scalar (i.e. of length 1)\nAll of the elements must have the same data type (or will be coerced)\n\nFor a complete explanation of vectors (and their more versatile siblings, lists) that’s beyond the scope of this tutorial, see:\n\nThis excellent explainer on vectors and lists\nR for Data Science chapter 20\n\n\n\n\n\nVector Calculations\n\n\n\n\n\n\nExercise\n\n\n\nCreate a vector of every whole number between 37 and 63, and subtract 7 from each element.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc(37:63) - 7\n\n [1] 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n[26] 55 56\n\n\n\n\n\n\n\nThis could be a very tedious process, but here we have an example of a vectorised operation. By default, the operation “subtract 7” is automatically applied to each individual element of the vector.\nWe can do a lot more than this with numbers and data in R, but this is an excellent start. Just one note before we move on about the order in which R performs its calculations.\n\n\n\n\n\n\nOrder of Operations\n\n\n\nMathematical expressions are evaluated in a certain order of priority. You can use brackets to tell R which part of a longer calculation to do first, e.g.:\n\n59 * (401 + 821)\n\n[1] 72098\n\n\nWithout the brackets, the expression is evaluated from left to right, which in this case would give a different answer:\n\n59 * 401 + 821\n\n[1] 24480\n\n\nWhenever there’s any chance for ambiguity, always use brackets to make sure the calculation is performed correctly.\n\n\n\n\n\nCharacter Data\nCharacters are a more general data category that also includes letters and words. In R, strings of letters or words must be enclosed in either ‘single’ or “double” quotes, otherwise R will try to read them as code:\n\n\nHello world!\n\nError: &lt;text&gt;:1:7: unexpected symbol\n1: Hello world\n          ^\n\n\n\n\"Hello world!\"\n\n[1] \"Hello world!\"\n\n\nAs you can see here, the first command without quotes throws an error, whereas the second prints out our input just like it did with the single numbers before.\nAn important thing to note is that R sees everything inside a pair of quotes as a single element, regardless of how long it is. You can see this in the indices we saw before:\n\n\n\"Hi!\"\n\n[1] \"Hi!\"\n\n\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\"\n\n[1] \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\"\n\n\nThe [1] markers also tell us that each of the two strings above already constitute vectors, each of length 1. Just like we saw with numbers, above, any number of character strings can be combined into a vector. You can also use the numbered markers to extract the nth element in that vector.\n\n\n\n\n\n\nExercise\n\n\n\nCreate a vector containing the first five animals you think of, then print the 3rd one.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc(\"bumblebee\", \"squid\", \"falcon\", \"flea\", \"seagull\")[3]\n\n[1] \"falcon\"\n\n\n\n\n\n\n\nThe placement of the quotes is very important - they can’t include the commas. As we saw before, R uses commas to separate different elements. So, if you didn’t enclose each word in quotes separately with commas in between, you would have had this odd message:\n\nc(\"bumblebee, squid, falcon, flea, seagull\")[3]\n\n[1] NA\n\n\nNA is a special value in R. It indicates that something is not available, and it usually represents missing data, or that a calculation has gone wrong or can’t be performed properly.\nHere, we asked R for the third element in a vector that, as far as R can tell, only contained one. This is because there’s only one pair of quotes, so all five animals and the commas between them are considered to be one element. Since there isn’t a third element, R has informed us so accordingly - the answer to our query is NA, doesn’t exist.\n\n\nLogical Data\nThe final type of data that we’ll look at for now is logical data. In addition to performing calculations and printing out words, R can also tell you whether a particular statement is TRUE or FALSE. To do this, we can use logical operators to form an assertion, and then R will tell us the result.\n\n\n\n\n\n\nExercise\n\n\n\nWrite the following assertions in R:\n\n5 is greater than 10\n6 is less than 12\n27 is less than or equal to 27\n49 does not equal 93\n420 equals 42\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n5 &gt; 10\n\n[1] FALSE\n\n6 &lt; 12\n\n[1] TRUE\n\n27 &lt;= 27\n\n[1] TRUE\n\n49 != 93\n\n[1] TRUE\n\n420 == 42\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsserting Equivalence\n\n\n\nThe last few statements above may have caused you some trouble if the notation is unfamiliar.\nFor “less than or equal to”, R won’t recognise the \\(\\le\\) symbol. Instead, we combine two operators, “less than” &lt; and “equal to” =, in the same order we’d normally read them aloud. The same goes for “greater than or equal to”, &gt;=. (It does have to be this way round; try =&lt; and =&gt; to see what happens.)\nFor “does not equal”, ! is common notation in R for “not”, or the reverse of something. So != can be read as “not-equals”.\nFor “equals”, if you tried this with a single equals sign, you would have had a strange error:\n\n420 = 42\n\nError in 420 = 42: invalid (do_set) left-hand side to assignment\n\n\nThe problem is that the single equals sign =, like the comma, has some very specialised syntactic uses, including one equivalent to the assignment operator &lt;-, which we’ll look at shortly. Single equals = also has an important and specific role to play in function arguments. Essentially, = is a special operator that doesn’t assert equivalence. Instead, “exactly equals” in R is “double-equals” (or “exactly and only”), ==.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse a single command to ask R whether the numbers 2 through 10 are less than or equal to 6.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n2:10 &lt;= 6\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n\nHere R prints out a value of TRUE or FALSE for each comparison it’s asked to make. So, the first element in the output (TRUE) corresponds to the statement 2 &lt;= 6, the second to 3 &lt;= 6, and so on. This is a vectorised calculation again, as we saw with numeric data before. These vectorised assertions will be absolutely essential to selecting and filtering data that meet particular requirements, or checking our data to find problems.\n\n\n\n\n\n\nSPSS Data Types\n\n\n\n\n\nIf you’re a regular SPSS user, you might recognise many of these data types from SPSS. See page 7 of this SPSS user guide for a reminder of these types.\nSo far, what we’ve called “numeric” in R is also (broadly) “scale/numeric” in SPSS. What we’ve called “character” in R is “string” in SPSS. As far as I know, SPSS doesn’t have an equivalent of “logical”, but would probably be a “nominal/string” type.\nWe haven’t thus far talked about a few common data types that you might be used to using in SPSS. Ordinal and Nominal data types in SPSS correspond (more or less) to factors in R, which we will come to in a later tutorial. R also has date-time data, which is not covered in this series, but if you need to use it, check out the {lubridate} package."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#class-and-coercion",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#class-and-coercion",
    "title": "01/02: IntRoduction",
    "section": "Class and Coercion",
    "text": "Class and Coercion\nWith these short examples, it may be obvious just by looking that 25 is a number and porcupine is a word. However, this isn’t always so straightforward, and there are some situations - such as data checking/cleaning, or debugging - where we might want to check what type of data a certain thing is. To do this, we’ll need another new function, class(). This function will print out, as a character, the name of the data type of whatever is put into the brackets.\n\n\n\n\n\n\nExercise\n\n\n\nUse the class() function to get R to print the values \"numeric\", \"logical\", and \"character\".\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Any numeric vector will do\nclass(216907)\n\n[1] \"numeric\"\n\n## You can also use a longer vector of numbers\n## as long as they are all numbers!\nclass(c(4:291, -1, 38.7, 100000000))\n\n[1] \"numeric\"\n\n## Logical has two options\n## Create a vector of TRUEs and FALSEs\nclass(TRUE)\n\n[1] \"logical\"\n\n## Create a vector that outputs logical values\n## (now you're thinking with functions!)\nclass(c(6 &gt; 4, 10 == 37, 3 != 8))\n\n[1] \"logical\"\n\n## Character\nclass(\"antidisestablishmentarianism\")\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat data type does R give you if you combine numbers and characters in c()?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Again, anything will do\n\nclass(c(93, -1905, \"avocado\"))\n\n[1] \"character\"\n\n\n\n\n\n\n\nSomething interesting has happened here. Recall that atomic vectors created with c() must all have the same data type. Here, we combined two types of data: numeric and character. We didn’t get an error - instead, without warning or telling us, R quietly converted the entire vector to character type. This forcible conversion is called coersion.\n\n\n\n\n\n\nDefinition: Coersion\n\n\n\nCoersion is when a piece of data is forcibly changed from one data type to another. This is sometimes intentional, but it can happen unintentionally (and without any warning or fanfare!), so is a common source of errors.\nCoersion follows a hierarchy; data types on the left can be coerced into the types further along to the right.\nlogical ==&gt; integer ==&gt; double (numeric) ==&gt; character\nAs we saw previously, you can check the data type of a vector with class(). You can also check if a vector is a particular type (and receive a logical vector in response) with the is.*() family of functions. (The * notation refers to a placeholder for many different options, such as is.numeric, is.character, etc.)\nYou can similarly (try to) coerce a vector into a particular data type with the as.*() family of functions.\n\n\nThis explains why our vector from the last exercise was a character vector - since the vector contained at least one character element, everything else in the vector was coerced to the same type. This can cause problems when, for example, numeric data is coerced into character data, even though it still looks like numbers.\nEven though we can do mathematical operations on numbers, we can’t do them on characters; it should be clear that asking e.g. what is \"tomato\" - 7 is nonsense. However, this is the case even if all of the data are numerals! For example:\n\n## No problem here; all numbers\nc(2:20, 45) - 7\n\n [1] -5 -4 -3 -2 -1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 38\n\n## Doesn't work\nc(2:20, \"45\") - 7\n\nError in c(2:20, \"45\") - 7: non-numeric argument to binary operator\n\n\nEven though “45” looks like a number, because it’s in quotes, R thinks that it’s a character, and will refuse to do the calculation, in the same way that it would refuse to do it with “tomato”.\n\n\n\n\n\n\nExercise\n\n\n\nUse an as.*() function to convert the following vector of participant ages into numeric data: c(20, \"42\", \"36 years old\"). What do you think will happen to each element?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nas.numeric(c(20, \"42\", \"36 years old\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 20 42 NA\n\n\n\n\n\n\n\nHere for the first time we see an example of a warning. Warnings are not errors, even though they get printed out in the same (usually alarming) colour and the same (often unfriendly) curt tone. The key difference is whether the code runs or not. With errors, the code cannot be executed as written, and the error is returned at the point where the execution failed. With warnings, the code CAN be (and has been) executed as written, but R is telling you that it has done something that you might not have expected or wanted along the way.\nWhat we have asked in this command is for three pieces of data to be coerced to numeric type. The first, 20, is already numeric type, so presents no issue. The second, \"42\", is character type (because of the quotes), but is also parsable as a number so similarly presents no issue. The problem is \"36 years old\", which cannot be turned into a number3. Instead of throwing an error, though, R instead replaces \"36 years old\" in the output vector with NA, and prints a warning to let you know it’s done this. If this is what you wanted (and sometimes it might be), you can ignore it, but if you thought that all these ages were already numbers, this warning would be an important flag to investigate your data a bit more thoroughly."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#objects",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#objects",
    "title": "01/02: IntRoduction",
    "section": "Objects",
    "text": "Objects\nR is a programming language, but (being created by speakers of natural language) it has many features similar or analogous to natural languages. In this section, we’ll cover the basic “grammar” of R, including how R understands what you ask it to do.\nIn a similar way that the basic unit of many languages is the word4, the basic unit of the R programming language is the object. This section will explore the basics of what an object is and some of their key features in R.\n\n\n\n\n\n\nDefinition: Objects\n\n\n\nObjects are the basic elements that R is built around - the equivalent of words. An “object” in R is any bit of information that is stored with a particular name. Objects can hold anything, from a single number or word to huge datasets with thousands of data points or complex graphs. These named objects are the main way you, the programmer, can store, retrieve, and interact with information in R.\n\n\n\nCreating an Object\nAlthough we have done quite a bit in R so far - creating vectors, doing calculations, etc. - you may notice that we haven’t stored this information anywhere. To store the output of code for further use, it needs to be assigned to an object using the assignment operator, &lt;-. Once an object is created, it will appear in the Environment pane.\n\n\n\n\n\n\nClear Your Environment\n\n\n\nAt the moment your Environment should be empty. As a reminder, Environment is by default the first (leftmost) tab in one of your four main windows in RStudio, probably the one on the top right.\nIf this window is blank except for “Environment is empty”, you’re ready to go. If for some reason it isn’t empty, click the broom icon to clear everything from your Environment before you get started, as indicted in the image below. (There will be a very ominous-sounding “Are you sure?” pop-up, but just click “Yes”.)\n\n\n\nFirst, let’s look at the foundational structure of almost everything you will do in R:\nobject &lt;- instructions\nThis is “pseudo-code”, or a “general format” for a command in R. It isn’t valid R code, but is rather intended as a midpoint between natural language and R to help make it clear how the code works. You can read this code as, “An object is created from (&lt;-) some instructions.”\n\nobject: Objects can be named almost anything (although see Naming Objects, below). The object name is a label so you, the analyst, can find, refer to, and use the information you need.\n&lt;-: The assignment operator &lt;- has single job: to assign output to names, or in other words, to create objects.\ninstructions: Any amount of code that produces some output, which is what object will contain.\n\n\nNaming Objects\nGenerally, you can name objects pretty freely in R. Object names must be a single sequence of symbols, so can’t include spaces or special operators (like =, &lt;-, ,, etc.). The best idea is to come up with naming conventions that work for you, so you can easily remember what objects contain. R will let you know if you try to name an object something that it doesn’t like:\n\n## This doesn't work because it would frankly be bonkers if it did\n1285 &lt;- \"a\"\n\nError in 1285 &lt;- \"a\": invalid (do_set) left-hand side to assignment\n\n\n\n## Can't start with a number...\n1stletter &lt;- \"a\"\n\nError: &lt;text&gt;:2:2: unexpected symbol\n1: ## Can't start with a number...\n2: 1stletter\n    ^\n\n\n\n## But numbers inside are okay\nletter1 &lt;- \"a\"\nletter1\n\n[1] \"a\"\n\n\n\n## You can really go wild if you want!\nthisis_TheFirstLETTER.of.the.alphabetWOW &lt;- \"a\"\nthisis_TheFirstLETTER.of.the.alphabetWOW\n\n[1] \"a\"\n\n\nIn these tutorials, we will typically stick to so-called “snake case” - lowercase names with underscores. This is generally the style of {tidyverse} as well. However, there’s nothing to stop you from using different conventions such as camelCase, PascalCase, whatever.this.is, or mixing them all at random, except maybe for the fact that your future self, and anyone else who might want to read or use your code, will almost certainly despair.\n\n\n\n\n\n\nNongrammatical Names\n\n\n\n\n\nIt is actually possible to use unallowable symbols, like spaces and punctuation, in some names, by using backticks. You must use the backticks when you create AND every time you use/call the object. It is generally a very bad idea to do this (just use underscores like a reasonable person), but it occasionally comes in handy for formatting tables or figures when the names don’t need to be machine-readable/good R code/easy to work with anymore.\n\nnope this is bad &lt;- \"a\"\n\nError: &lt;text&gt;:1:6: unexpected symbol\n1: nope this\n         ^\n\n\n\n`but this one works!` &lt;- \"a\"\n`but this one works!`\n\n[1] \"a\"\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThink of a research scenario familiar to you with two independent groups. You’re welcome to draw from your own research or expertise, but you should choose something with numerical scores. Some ideas include:\n\nReaction times on a button-pressing task from a control and an experimental group\nStatistics anxiety scores from first and second year UG students\nQuiz marks from students with practicals scheduled 9am and students with practicals at 6pm\n\nMake a note of the scenario you chose. Then, create two new objects: one that contains a vector of six scores from the first of the two groups, and the second that has six different scores from the second group.\nHint: Just make up some numbers that sound plausible!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nChoosing scenario 3, this vector contains some hypothetical quiz marks from each class.\n\nquiz_9am &lt;- c(75, 58, 62, 16, 33, 67)\nquiz_6pm &lt;- c(45, 90, 27, 65, 39, 77)\n\nLet’s have a look at these two commands. On the left side I’ve written the name I want my new object to have, which I’ve called quiz_9am5. Next, the assignment operator &lt;- assigns whatever comes after it to the object label quiz_9am. Finally, I’ve written instructions for what I want this object to contain: in this case, a vector of numbers that I’ve made up, but that reasonably look like quiz scores. The second command is the same as the first, but with a different object name and different numbers.\nIf you haven’t done this yet, do so now, even if you’ve looked at the solution rather than trying it for yourself first. Once you’ve typed the command, there’s a final step to actually create the object: you have to run the command in order for it to take effect. As a reminder, you can do this by clicking the green ▶️ button in the upper right corner of the code chunk, or by pressing Ctrl/Cmd + Enter when your cursor is blinking on the same line as the code you want to run.\n\n\n\n\n\nAssuming your code is valid, you should see a green bar appear along the left-hand side of the code chunk when you run the code, but you might notice that there’s no printout that appears under the code chunk, as there was previously. In fact, if the code ran successfully, it might look like nothing happened at all. To find out what did happen, look your Environment pane. You should now see a new section, “Values”, and underneath the name of your new objects and what they contain. Success!\n\n\n\nCalling an Object\nFor any object, from the most simple to the most complex, you can always see what’s in it by calling the object. This simply means that you type the name of the object and run the code. R will print out whatever is stored in the object.\n\n\n\n\n\n\nExercise\n\n\n\nCall both of the objects you just created.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nReplace with the name of the object you created, if you did something different.\n\nquiz_9am\n\n[1] 75 58 62 16 33 67\n\nquiz_6pm\n\n[1] 45 90 27 65 39 77\n\n\n\n\n\n\n\nThis output looks just like what we saw earlier, when we just asked R to print out a vector of numbers. In essence, the object names are just labels for storing and referring to the information they contain.\n\n\n\n\n\n\nCreating vs Calling\n\n\n\nThese two actions are the essential basis of everything you will do in R. All of your code will, at base, either create an object, or call an object. (Changing an existing object, as we’ll see shortly, is the exact same procedure as creating one from scratch.)\nWhen you create an object using the assignment operator (&lt;-), the object is created but is not printed out. This is because R always does only and exactly what you ask it to do, and using the assignment operator only tells R to assign something to an object, not to print it out6.\nWhen you call an object, the current contents of that object are printed out, but that object is not changed - you only reproduce a copy of its contents for review. To create or change an object, you must use the assignment operator to assign the output to a new (or existing) object name.\n\n\nLet’s make all of this a bit more concrete by seeing how we can use objects effectively.\n\n\nUsing Objects\nSince objects are convenient reference labels for the information they contain, we can work with them as if they were the information they contain. In this case, our objects contain numbers, so we can use them for numerical calculations.\nFor instance, we might want to know what the mean mark was for this sample of quiz marks. To do this, we could make use of a very handy function, mean(), as follows:\n\nmean(quiz_9am)\n\n[1] 51.83333\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the mean of each of the two sets of scores you created.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhether you save the output of the mean() command is up to you!\n\nmean(quiz_9am)\n\n[1] 51.83333\n\nmean(quiz_6pm)\n\n[1] 57.16667\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the difference in the mean of each of the two sets of scores, and save this difference in a new object called quiz_diff.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere are two options for accomplishing this.\nThe first option is to save each mean value in a new intermediate object (if you didn’t do that already), then subtract one mean from the other. This is very easy to read, but a bit inefficient.\n\nmean_9am &lt;- mean(quiz_9am)\nmean_6pm &lt;- mean(quiz_6pm)\n\nquiz_diff &lt;- mean_9am - mean_6pm\n\nThe second option is to do everything in one command, which takes a bit more effort to parse but is more succinct.\n\nquiz_diff &lt;- mean(quiz_9am) - mean(quiz_6pm)\n\nEither way, you will get the same result:\n\nquiz_diff\n\n[1] -5.333333\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the class of these objects?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEither one will do.\n\nclass(quiz_9am)\n\n[1] \"numeric\"\n\n\nSo, an object has the class of the data it contains.\n\n\n\n\n\nYou may have been surprised to see that the class of these objects is numeric, rather than character - even though the name of the object is a character string. To find out the class of the object, R looks at what that object contains, not at the name of the object itself. We already saw that quiz_9am (or whatever your object is called) contains only numbers; so, R tells us that it’s a numeric vector.\nOne more example to emphasize this point, because it’s often a source of confusion when starting out with R. If we want to ask R the class of the string “quiz_9am”, we would need to put it in quotes, and we’d get a different answer:\n\nclass(\"quiz_9am\")\n\n[1] \"character\"\n\n\nThe key thing here is that objects have the class of the data they contain, and are not character data; and whenever you want to use an object, you must not use quotation marks. On the other hand, if you want to input character data into R, you must use quotation marks. Otherwise, R will look for an object or function with that name, which will likely produce a “cannot find object” error.\n\n\nOverwriting Objects\nOur last major point to cover with objects - for now - is how to change what an object contains.\n\n\n\n\n\n\nExercise\n\n\n\nFirst, let’s imagine we get three new participants in each condition of our previous study. Update the same two objects you created previously with three new scores each.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne way - the longer way round - is to type in all the same numbers again from before, and then include three more.\n\n## Example with just one\nquiz_9am &lt;- c(75, 58, 62, 16, 33, 67, 89, 100, 79)\n\nHowever, this is tedious, repetitive, and prone to error. Wherever possible, it’s better to rely on R to do calculations or repetitions for you. So, we could instead just embed the previous quiz_9am object - which already contains the first six numbers - into a c() along with the three new numbers.\n\nquiz_9am &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am\n\n[1]  75  58  62  16  33  67  89 100  79\n\nquiz_6pm &lt;- c(quiz_6pm, 38, 42, 53)\nquiz_6pm\n\n[1] 45 90 27 65 39 77 38 42 53\n\n\n\n\n\n\n\n \nThe command we’ve just written for the task above demonstrates some extremely important properties of how assignment and functions work in R.\nOverwriting objects is accomplished by assigning new output to an existing object name. If you have a look in your Environment, you will see that the previous version of quiz_9am, containing only six values, has been replaced with the new one containing nine values.\nOverwriting objects is silent. Unlike, say, a word processor, that will give you a warning if you try to save two documents in the same folder with the same name, R won’t ask you if you’re sure you want to overwrite an existing object with new information - it will just do it. This can be a good thing, because you can easily update the information stored in an object with changes, edits, or new information. However, it also means that you can overwrite or replace data when you don’t want to, if you use the same object name.\nThis is why it is so important to keep track of all of the commands and changes you make to your data. If you accidentally replace your dataset with, say, a single word, or number with an error in your code, you can easily retrace your steps and avoid redoing work.\nOverwriting objects can be done recursively. In the command we saw above, we took the current quiz_9am object, combined it with some new values, and then overwrote the quiz_9am object with the new values. If we were to run this exact same code again, this means that each time we would add three new values to quiz_9am, over and over and over:\n\nquiz_9am &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am\n\n [1]  75  58  62  16  33  67  89 100  79  89 100  79\n\nquiz_9am &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am\n\n [1]  75  58  62  16  33  67  89 100  79  89 100  79  89 100  79\n\n\nThis is one reason why the decision to overwrite an existing object, vs creating a new one, can make a big difference to your code. This behaviour only happens because the input and output objects are the same. If we name the output object something different, we don’t get the same recursion - the new object quiz_9am_full is recreated from the same input in the same way every time, so it always contains the same thing.\n\n## Recreating the original version of quiz_9am so things don't get out of hand!\nquiz_9am &lt;- c(75, 58, 62, 16, 33, 67)\n\nquiz_9am_full &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am_full\n\n[1]  75  58  62  16  33  67  89 100  79\n\nquiz_9am_full &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am_full\n\n[1]  75  58  62  16  33  67  89 100  79\n\n\nThere isn’t a right or wrong way to do this - sometimes this recursive property is exactly what you want. (It’s very useful, for example, in loops.) But it is important to be aware of.\nFinally, overwriting objects only changes the overwritten objects, and NOT any other objects created from them. To see this in action, recall that earlier we calculated the mean of the two quiz objects and saved it as quiz_diff. If we do the same calculation now with our updated objects, we can see that the difference in the means is no longer the same.\n\n## Value calculated previously\nquiz_diff\n\n[1] -5.333333\n\n## Value using the updated objects\nmean(quiz_9am) - mean(quiz_6pm)\n\n[1] 11.44444\n\n## Ask R if the two are the same\nquiz_diff == (mean(quiz_9am) - mean(quiz_6pm))\n\n[1] FALSE\n\n\nThis illustrates the importance of writing and running code sequentially, from beginning to end. If you go back and change values created earlier on in your code, the value you currently have in your Environment may not match the value that your code will produce when run.\nIf you are interested in understanding this process of assigning and replacing the contents of objects better, the aside below explains it in more depth.\n\n\n\n\n\n\nCan you actually change an object?\n\n\n\n\n\nThe majority of this aside was originally written by Milan Valášek\nThink of objects as boxes. The names of the objects are only labels, and you can store anything you like inside them. However, unlike in the physical world, objects in R cannot truly change. You can put stuff in and take stuff out, and that’s pretty much it. Unlike boxes, though, when you take stuff out of objects, you only take out a copy of its contents. The original contents of the box remain intact. Of course, you can do whatever you want (within limits) to the stuff once you’ve taken it out of the box, but you are only modifying the copy. The key thing to remember is that unless you put that modified stuff into a box, R will forget about it as soon as it’s done with it. In other words, if you want to “save” any changes you make, you must assign them to an object in order to keep them.\nNow, as you probably know, you can call your boxes (objects) whatever you want (again, within certain limits). This means that that you can call the new box the same as the old one, as we saw with quiz_9am above. When that happens, R basically takes the label off the old box, pastes it on the new one, and burns the old box. So even though some operations in R may look like they change objects, what’s actually happening is that R copies their content, modifies it, stores the result in a different object, puts the same label on it, and discards the original object. Understanding this mechanism will make things much easier!\nPutting the above into practice, this is how you “change” an R object:\n\n# put 1 into an object (box) called a\na &lt;- 1\n\n# copy the content of a, add 1 to it and store it in an object b\nb &lt;- a + 1\n\n# copy what's inside b and put it in a new object called a\n# discarding (\"overwriting\") the old object a\na &lt;- b\n\n# now see what's inside of a\n# (by copying its content and pasting it in the console)\na\n\n[1] 2\n\n\nOf course, you can just cut out the middleman (creating an object b). So to increment a by another 1, we can do:\n\na &lt;- a + 1\n\na\n\n[1] 3"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#functions",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#functions",
    "title": "01/02: IntRoduction",
    "section": "Functions",
    "text": "Functions\nFunctions are like verbs in the R language. We’ve already started using a few functions: c(), class(), mean(), and as.numeric(). As we’ve seen, these functions perform some operation using the input inside their brackets, and produce the output of that operation. So, functions are the main way that R does anything.\nIn this section, we’ll take a systematic look at the process of using a new function - in particular, functions that take multiple inputs, or arguments. As we go, we’ll look at how to “translate” the command you want to give R into a verb (function) it can understand.\n\nBasics and Help\nLet’s look at an example of how this translation might work. For this example, I’m going to use a number I generated earlier: the mean of the quiz_9am group, 64.3333333, which I’d like to round to two decimal places - a common task for reporting results in APA style.\nIf we want R to do this for us, we have to write this command in a way that R can understand. First, we need to know what function corresponds to the English verb “round” - that is, what function will do the same action that we want R to perform. We’re lucky in this case: the function in R is also called round().\nWe know that we’re looking at a function in R because functions often have a name followed by brackets (and nothing else in R does). That is, they have the general form function_name(). Inside the brackets, we can add more information to the function to complete our command, although not all functions require any more information.\n\n\n\n\n\n\nExercise\n\n\n\nTry running the round() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nround()\n\nError in eval(expr, envir, enclos): 0 arguments passed to 'round' which requires 1 or 2 arguments\n\n\nUnsurprisingly, R has given us an error. This is an informative error, though - that is, the error gives of some sort of intelligible clue about what’s gone wrong. Namely, it tells us that round() can’t just work without additional information (i.e. “required arguments”).\n\n\n\n\n\nWhat we want to do, “Round the number 64.3333333 to two decimal places”, has two more important pieces of information that we need to tell R: what number we want to round (64.3333333) and how many decimal places we want to round it to (2). So, how do we say this in R? To find out, let’s look at the help documentation.\n\n\n\n\n\n\nExercise\n\n\n\nOpen the help documentation for the round() function by running ?round() or help(round) in the Console.\n\n\n\n\n\n\n\n\nHelp Documentation\n\n\n\nHelp documentation is information, like instruction manuals, built into R about how individual functions work. Function documentation varies wildly in helpfulness and completeness, but it’s a useful place to check first if you want to find out what a function does. You can access the help documentation in a few different ways: by running ?function_name or help(function_name) in the Console, or by clicking on the “Help” tab in the Files section of RStudio and using the Find box to search for the function.\n\n\nThe first section, “Description”, varies quite a bit in intelligibility, depending on how complex the function is. Here, if we ignore the information about the other function included in this document, we can see that we have a useful description of round() that tells us that it rounds numbers (that’s a good sign) to a certain number of decimal places. That’s exactly what we want, so how do we use it?\nLet’s scroll down to “Usage”, which gives examples of what the function looks like. You can see that the basic structure of this function is round(x, digits = 0). It seems like we need to add some more information in the brackets of our function - but how do we interpret x and digits = 0?\n\n\nArguments\nThe information inside a function’s brackets, which give it the information it needs to work, are called arguments. Each argument in a function is separated by a comma, so we can see from round(x, digits = 0) that the round() function can take two arguments. How many arguments a function has depends on the function; some (like Sys.Date()) don’t need any arguments to run. One of the most useful parts of a function’s help documentation is the “Arguments” section, which tells you what each of the function’s arguments are and how to use them.\nWhen referring to arguments, you will hear the terms “named” and “unnamed” arguments. This can be a bit confusing, because all arguments have a name - they have to, otherwise we couldn’t refer to them!7 The named vs unnamed distinction doesn’t refer to the arguments themselves, but rather how the person using the function chooses to write them out. There are some conventions around which arguments are named or not, so let’s have a look at that now.\nThe first argument to round() is simply x. Just like in maths, x is a placeholder for some number or numbers (a “numeric vector”, which should sound familiar now) that the function will work on. This is common notation in many functions: x, often the first argument in a function, typically denotes the placeholder for the information you want to use the function on. In our case, we just have one number we want to round, so that’s what we should replace with x.\nThis argument has no default, so it must be provided or the function won’t run. Because we always have to provide some information here, x and similar arguments containing the values or data to work on are frequently unnamed when we use them. That means that instead of round(x = 64.333333...) we can just write round(64.33333...). They are also frequently the first argument in the function8. So, when you see reference to the “first unnamed argument” - especially important in {tidyverse} functions designed to work with the pipe operator, which we’ll meet next week - that simply means, “the first argument in the function for which the programmer hasn’t specifed a name”, which is usually, but not always or necessarily, the “data” or “information to work on” argument.\nThe second argument of round() is digits. You can think of arguments like this as settings that change the way a function works, often with only certain allowable values.\nThe help documentation tells us that digits should be an “integer indicating the number of decimal places…to be used.” We can also see in “Usage” that this argument has a default value, digits = 0. That means that if we don’t explicitly include the argument digits when we use the function, by default the round() function will round the number you give it to 0 decimal places. Arguments frequently, but not always, have a default, and it’s important to check so the function doesn’t quietly do something unexpected.\nDefault values of arguments are really useful, because the default is often the most frequently used or safest9 setting. It means you don’t have to specify every single aspect of a function every time you use it, as long as you want the function to work according to its defaults. In our case, we actually wanted round() to round to two decimal places, not 0. So, in our command, we should change the digits setting from the default, 0, to 2.\n\n\nUsing Functions\nNow that we know what both of these arguments mean, we can change them to actually translate the sentence “Round the number 64.3333333 to two decimal places” into a command that R can work with. We’ll explicitly write out each argument so we know what they are doing.\n\n\n\n\n\n\nExercise\n\n\n\nUse the round() function to round 64.3333333 to two decimal places.\nIf you prefer, you can do this with one of the means you calculated for your own scores earlier.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Using the actual value from my earlier calculation\nround(x = 64.3333333, digits = 2)\n\n[1] 64.33\n\n## Using a nested function - that is, calculating the mean and then rounding it!\nround(x = mean(quiz_9am), digits = 2)\n\n[1] 64.33\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrder and Naming of Arguments\n\n\n\n\n\nIf you want to, you can achieve the same result by changing the order of the arguments, as long as you pay careful attention to which argument(s) you have named.\nIf we have written the names of both arguments, R can still do what we want it to do with the order of arguments reversed:\n\nround(digits = 2, x = 64.3333333)\n\n[1] 64.33\n\n\nWe can also, to some degree, drop the names of the arguments, as long as R can still understand what we’re trying to do. In this case, the “first unnamed argument” is still x! Even though it’s not the first argument we’ve written in the function, it’s the first one that doesn’t have an explicit name.\n\nround(digits = 2, 64.3333333)\n\n[1] 64.33\n\n\nAlthough I left out the x =, R can still understand this because round() only takes two arguments, and we explicitly told it what value belongs to digits, so it assumes the second number must be x.\nIf more than one, or all, of the arguments are unnamed, then order becomes critical:\n\nround(64.3333333, 2)\n\n[1] 64.33\n\n\nThis time I dropped both argument names. R can still understand this because when you don’t specify which input goes with which argument, R will assume they should go in the default order given in the help documentation. So, R has automatically assigned 64.3333333 to x and 2 to digits.\nAs I use R more and more, I find that I name arguments more consistently, even though I know how the function works and dropping them is more efficient (at least in terms of typing). That’s because when I come back from lunch, or the next day, or six months later to revisit the same code, it’s much easier to recall what it all means when it’s well-annotated. So, I strongly recommend getting in the habit of including argument names in your code as a favour to your future self, and to avoid situations like this:\n\n## uh oh!\nround(2, 64.3333333)\n\n[1] 2\n\n\nHere, since we left all the arguments unnamed, R assumed that 2 was the number we wanted to round. This isn’t what we wanted - but R has no way of knowing this. It always assumes that what we typed was precisely what we intended to ask R to do.\n\n\n\n\nPassing Multiple Values to Arguments\nA last important aspect of using functions is that each argument in a function can only take a single object or value as input. For example, we saw above that we put the single value 64.3333333 into the x argument of round(). But what if we wanted to round more than one number? We don’t want to have to write a new round() command for every number, even though we could do this if we particularly enjoyed doing a lot of tedious and repetitive typing:\n\nround(64.3333333, digits = 2)\n\n[1] 64.33\n\n## ughhhh\nround(59.5452, digits = 2)\n\n[1] 59.55\n\n## noooooo :(\nround(0.198, digits = 2)\n\n[1] 0.2\n\n## thanks I hate it\n\n\n\n\n\n\n\nExercise\n\n\n\nBefore you go on, have a go using a single round() command to round 64.3333333, 59.5452, and 0.198 at once.\nHint: Refer to Vectors.\n\n\nSo what happens if we try to put all of those numbers into round()? We might first try this:\n\nround(64.3333333, 59.5452, 0.198, 2)\n\nError in eval(expr, envir, enclos): 4 arguments passed to 'round' which requires 1 or 2 arguments\n\n\nOnce again, R tells us that this doesn’t work by throwing an error. R has tried to do what we wanted, but the round() function only allows a max of two arguments, and we’ve given it four. Behind the scenes, R has tried to run round(x = 64.3333333, digits = 59.5452... and can’t proceed from there because it doesn’t know what to do with the last two numbers. So, what we need to do is find a way to put all three numbers that we want to round into the first x argument together. If only there was a way to concatenate them together…\nYou may have guessed where this is going: one method we could use would be to put the three numbers we want to round into a single object, and then pass that object to round() as the x argument. We already saw that we can combine any number of things together into a single vector using the c() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Create an intermediate object to contain the numbers\nnumbers &lt;- c(64.3333333, 59.5452, 0.198)\nround(numbers, digits = 2)\n\n[1] 64.33 59.55  0.20\n\n## Put the vector of numbers into round() directly\nround(c(64.3333333, 59.5452, 0.198), digits = 2)\n\n[1] 64.33 59.55  0.20\n\n\n\n\n\nHere we can see a good example of a function inside another function. You can stack, or “nest”, functions inside each other like this as much as you like, although it can become difficult to read the code or keep track of what it’s doing. (There’s a great solution to this problem that we’ll encounter in the next tutorial: the pipe operator.)\nThat’s looking like some proper R code! Very nicely done.\n\n\n\n\n\n\nHelp Documentation, Revisited\n\n\n\nBefore we leave the round() function altogether, let’s take a look at two more useful sections of the help documentation. Depending on what you are trying to do, the “Details” section can tell you more about how exactly the function works - how it behaves in certain situations, or how it handles unusual or difficult cases. If a function isn’t doing what you expect it to, this is a good place to look for an explanation.\nFinally, at the end of the documentation you can find the “Examples” section. If you are learning to use a new function, this section can give you a template for writing your own commands. You can also click the “Run examples” link, which will run the code in the Examples section for you so you can see what the function will do."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#quick-test-t-test",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#quick-test-t-test",
    "title": "01/02: IntRoduction",
    "section": "Quick Test: t-test",
    "text": "Quick Test: t-test\nLet’s put all of this together and have a look at what we can already do with the skills in this tutorial. R has many, many uses, but one of its core purposes is statistical analysis - and we already know more than enough to do this.\n\n\n\n\n\n\nIf we run out of time in the live session, you can attempt this last bit as an optional Challenge task. Don’t worry if you get stuck - we will come back to t-tests later on in the Essentials section of the course.\nIn case you’re not familiar with t-tests, you can find a lecture recording on the Analysing Data 22/23 Canvas site.\n\n\n\nWe’ve created two objects that contain scores from two different groups - scores we made up, but we will get to real data soon (in the next tutorial!). For now, one common statistical test we could run on data like this is a independent-samples t-test, which is a hypothesis test essentially evaluating the probability that two sets of scores come from the same population.\nHelpfully, the function we want is called t.test().\n\n\n\n\n\n\nExercise\n\n\n\nBring up the help documentation for t.test() and use it to run a t-test comparing your two sets of scores.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCall up the help documentation in the Console:\n\n?t.test\nhelp(t.test)\n\nRun the test:\n\nt.test(quiz_9am, quiz_6pm)\n\n\n    Welch Two Sample t-test\n\ndata:  quiz_9am and quiz_6pm\nt = 1.0264, df = 15.081, p-value = 0.3209\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12.30962  35.19851\nsample estimates:\nmean of x mean of y \n 64.33333  52.88889 \n\n\n\n\n\n\n\nThere are a lot of options in the t.test() function, which can be used, through different arguments, to run almost any variety of t-test you can think of. In this case, though, the code is quite simple, because we want all the default settings (for a two-sample, independent test), so we only need to provide x and y, our two numeric vectors.\nNote that the output mentions “Welch Two Sample t-test”, which is a version of the test that does not assume equal variances. This is the version that is taught to undergraduates, because we have not at this point introduced the process of assumption testing. If you definitely know that the variances are equal and you definitely want Student’s t-test, you can instead change the default setting.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the help documentation, re-run the t-test with equal variances assumed.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nt.test(quiz_9am, quiz_6pm, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  quiz_9am and quiz_6pm\nt = 1.0264, df = 16, p-value = 0.32\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12.19206  35.08095\nsample estimates:\nmean of x mean of y \n 64.33333  52.88889 \n\n\n\n\n\n\n\nIn future tutorial, we’ll see how to turn this rather ugly R output automatically into beautifully formatted reporting like this:\n\nWe compared mean scores between two groups, one who took the quiz in a 9am practical session (M = 64.33) and the other who took the quiz in a 6pm practical session (M =52.89, Mdiff = 11.44). There was no statistically significant difference in scores between practical groups (t(16) = 1.03, p = 0.32, 95% CI [-12.19, 35.08]).\n\n \n \n\n\n\n\n\n\nWell Done!\n\n\n\nThat’s the end of the first tutorial. Very well done on all your hard work!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#footnotes",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#footnotes",
    "title": "01/02: IntRoduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis isn’t technically true - have a look at the “History” tab in the Environment window. However, the commands stored here can’t be run or used - they have to be copied into the Console or Source windows in order to run. The History tab provides an exhaustive record of the things you’ve typed, not a cohesive or meaningful series of steps.↩︎\nDon’t get me wrong - crashes do happen! But they often look like a “fatal error” popup message, the programme freezing, or other obvious breakdowns of the programme itself. “Errors” in code as we’re seeing here are just a part of the normal functioning of R and don’t usually mean anything particularly horrific is occurring.↩︎\nWell, not in the single command we’re using here. We can certainly get out the age 36, but it will take a bit more work. We’ll come back to this problem in the Essentials section of the course.↩︎\nAs a linguist I have to note, one, words don’t exist, and two, the closest linguistic term for what an object is is probably “lexeme”. “Word” will get you in the right vicinity, though, conceptually. If you’d like to dive down this rabbit hole (rabbit-hole?) this Crash Course video on morphology is a good place to start.↩︎\nAgain, I could have called this object anything, like the_first_example_of_an_object_InThisSection.so.far or made_upQuizScores.fornineamclass or anything else that follows R’s naming conventions. However, it’s a good idea to name your objects something brief and obvious, so you can remember what they contain and work with them easily.↩︎\nThere is a way to do this - you can enclose the entire expression in round brackets, e.g. (object &lt;- instructions), which will BOTH create the object AND print out what that object contains at the same time. I’m not using this method in these tutorials because I think it will be confusing, since it’s primarily for demonstration purposes and not necessary the sort of thing you’d want to use in your own analysis code.↩︎\nIn fact, a previous version of this tutorial very confidently gave the wrong definitions! Sorry…↩︎\nAgain, not necessarily - the base-R string-manipulation functions grep() and friends, for example, have x as their third argument. I know all the irregularities can be confusing, but remember that R is a massive collaborative project across decades and millions of users, so some quirks are inevitable!↩︎\nBy “safest” setting, I mean that the function makes the fewest assumptions about what you intended.↩︎"
  },
  {
    "objectID": "resources/faqs/ug_faqs.html",
    "href": "resources/faqs/ug_faqs.html",
    "title": "UG Analysis FAQs",
    "section": "",
    "text": "This FAQ is co-authored with current Sussex PhD researchers, Hanna Eldarwish and Josh Francis. Their input and insight has been invaluable throughout.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are a UG student reading this, do not email Hanna or Josh for help with your dissertation! You should talk to your supervisor in the first instance if you need specific help with your own work.\n\n\nMany UG dissertation projects at Sussex use Qualtrics, so involve the wrangling and analysis of Qualtrics datasets. The recommended method for dealing with Qualtrics data is using SAV files with the {haven} and {labelled} packages, as described in Tutorials 10 and 11 (see Tutorials). However, this method is not (presently) taught at UG level, as there isn’t space in the curriculum for it, and not all students end up using Qualtrics data. This page compiles some frequently encountered problems and solutions that have come up in previous years."
  },
  {
    "objectID": "resources/faqs/ug_faqs.html#acknowledgements",
    "href": "resources/faqs/ug_faqs.html#acknowledgements",
    "title": "UG Analysis FAQs",
    "section": "",
    "text": "This FAQ is co-authored with current Sussex PhD researchers, Hanna Eldarwish and Josh Francis. Their input and insight has been invaluable throughout.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are a UG student reading this, do not email Hanna or Josh for help with your dissertation! You should talk to your supervisor in the first instance if you need specific help with your own work.\n\n\nMany UG dissertation projects at Sussex use Qualtrics, so involve the wrangling and analysis of Qualtrics datasets. The recommended method for dealing with Qualtrics data is using SAV files with the {haven} and {labelled} packages, as described in Tutorials 10 and 11 (see Tutorials). However, this method is not (presently) taught at UG level, as there isn’t space in the curriculum for it, and not all students end up using Qualtrics data. This page compiles some frequently encountered problems and solutions that have come up in previous years."
  },
  {
    "objectID": "resources/faqs/ug_faqs.html#data-cleaning",
    "href": "resources/faqs/ug_faqs.html#data-cleaning",
    "title": "UG Analysis FAQs",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nRemoving extra rows\nWhen data is read in from CSV, as it often is by students who have been primarily taught using CSV files, Qualtrics adds some extra empty rows to the file to make it easier to view in Excel or similar, but that introduce NAs in R. These rows should be removed.\n\nSolution\nNote that Qualtrics CSVs typically have the blank rows inserted as the 1st and 2nd rows in the dataset, which is what the code below assumes. You should check your own dataset carefully to make sure you remove the correct rows.\nThe function dplyr::slice() takes out a horizontal slice of the data, keeping only particular rows. It’s like dplyr::filter()’s evil twin; they both create a subset of rows, but filter() does this based on the values within the variables (see Tutorial 05), whereas slice() does it using row numbers or position. We can drop the blank rows either by removing those rows, or keeping only the non-empty rows.\n\n## Choose ONE of the below - if you use both you might delete real data!!\n\n## Drop empty rows\n## c(1:2) refers to the row NUMBERS of the blank rows\ndata &lt;- data |&gt; \n  dplyr::slice(-c(1:2))\n\n## Keep non-empty rows\ndata &lt;- data |&gt; \n  dplyr::slice(3:nrow(data))\n\n\n\n\nRemoving previews\nAs discussed in Tutorial 11, Qualtrics automatically adds a variable to its exported data called “DistributionChannel”. The main values are “preview” and “anonymous”, with “preview” corresponding to preview runs through the study by the researcher(s), and “anonymous” corresponding to genuine responses from participants. Any “preview” data is not real data, and should be removed before analysis.\n\nSolution\nRemove preview rows by filtering.\n\ndata &lt;- data |&gt; \n  dplyr::filter(\n    DistributionChannel == \"anonymous\"\n  )\n\nAlternatively, create a keep coding variable, as described in Tutorial 06.\n\n\n\nChanging variable types\nWith CSV files especially, it may be the case that variables containing numbers have been read in as character data, and attempting to do e.g. summary statistics may result in strange errors, as below.\n\nnumbers &lt;- c(\"10\", \"5\", \"8\") # some numbers\nclass(numbers) # R says the numbers object is character \n\n[1] \"character\"\n\nmean(numbers) # throws an informative error that the values aren't in the correct form\n\n[1] NA\n\n\n\nSolution\nOnce the data have been checked to ensure there are no unexpected character values that need to be replaced (e.g. “Twenty” instead of 20), the variables will need to be converted to numeric (or whatever the relevant data type is). This is covered in depth in Tutorial 06 and revised here.\nOften students try to change each variable one by one using dplyr::mutate(), which is perfectly reasonable as this is the method they have been taught. However, especially for large datasets, this is not only tedious but also prone to error, such as missing out or misspelling a variable.\n\ndata |&gt; \n  dplyr::mutate(\n    item_01 = as.numeric(item_01),\n    item_02 = as.numeric(item_02) # and so on... \n  )\n\nThe better option is to use dplyr::across() to convert variables en masse.\n\n# more efficient alternative: \ndata &lt;- data |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      c(item_01, item_2, item_27), # pick the variables you want to change \n      as.numeric # coerce them to be numeric vars \n    )\n  )\n\n## if your variables are all directly next to each other (like item_01 to item_10), just type in item_01:item_10 instead of c(item_01, item_2, item_27)\n\n\n\n\n\n\n\nTip\n\n\n\nThe help documentation for the dplyr::select() function contains a friendly description of the options for efficiently selecting variables by both position and name.\n\n\n\n\n\nDealing with NAs\nNAs (missing values) will cause different problems depending on the task you are trying to complete. By default, for example, lm() will remove rows with missing data, while other functions like mean() and sd() will return NA if there are any NAs present.\n\nSolution\nOn the one hand, you can include the na.rm = TRUE argument in functions like mean() and sd() to ignore NAs when doing the calculations.\nHowever, it’s a bit risky to not deal explicitly with NAs. Among other issues, the sample size you would calculate from using a method like nrow(data) will not be the same as the actual sample size that is used in the analysis or calculations. It’s strongly recommend to develop clear rules for dealing with NAs as part of your analysis plan, in discussion with your supervisor, and then implementing those rules consistently to remove or replace NA values so that your final dataset doesn’t make any assumptions about which data to include.\n\n\n\nReformatting for repeated measures\nAs described in Tutorial 09, repeated measures data must be in long rather than wide format. Before analysis, the dataset must be restructured.\n\nSolution\nRestructuring can be straightforward or complex depending on how the information is stored and in which variables. Refer to the example of tidyr::pivot_longer() in Tutorial 09, or run vignette(\"pivot\") in the Console for detailed help."
  },
  {
    "objectID": "resources/faqs/ug_faqs.html#troubleshooting-errors",
    "href": "resources/faqs/ug_faqs.html#troubleshooting-errors",
    "title": "UG Analysis FAQs",
    "section": "Troubleshooting Errors",
    "text": "Troubleshooting Errors\n\nError: Can’t subset columns that don’t exist\nThis indicates there is no column (variable) with the name as it has been written.\n\nSolution\nThe cause is very typically typos in the column name. To avoid this, make use of tab-autocomplete when typing variable names, and keep variable names short, succinct, and consistent.\n\n\n\nError: object/function not found\nSimilar to the above, this indicates that R has looked in the Environment (in the case of objects) or in the loaded packages (in the case of functions) and has not found the object or function you have tried to use.\n\n## Misspelling the object name\nmean(numbrs)\n\nError in mean(numbrs): object 'numbrs' not found\n\n\n\n## Misspelling the function name\nmena(numbers)\n\nError in mena(numbers): could not find function \"mena\"\n\n\n\nSolution\nAgain, typos are a common cause, and can be avoided with autocomplete.\nFor objects, this error may appear when you have written the code that creates an object, but not run it. This could happen if you restart R, or clear your Environment, so objects you have previously created are removed. Find and run the code that creates the object (but beware of running code out of order!).\nFor functions, this error frequently appears when you are using a function without a package call (e.g. filter() instead of dplyr::filter()) and have not loaded the relevant package.\nThe solution is to add the package call and/or load the package. If the error persists, you may need to install the package.\n\n\n\nAdapting code from {discovr}\nIt’s completely fine to use code examples from the {discovR} tutorials for analysis purposes. However, some “object not found” or “column doesn’t exist” errors may arise because the object/dataset/variable names from the tutorial example haven’t been replaced.\n\nSolution\nCheck the code for leftover objects or names from {discovr} and replace with the corresponding objects or names from your dataset.\n\n\n\nError: mapping must be created by aes()\nMuscle memory often gets you into trouble in {ggplot2}! Using the pipe |&gt; instead of + to link together layers in a plot will return one of the friendliest errors you’ll see.\n\nlibrary(ggplot2)\n\ndata &lt;- tibble::tibble(\n  x = 1:10,\n  y = 1:10\n)\n\n# this will not work \ndata |&gt; \n  ggplot2::ggplot(aes(x = x, y = y)) |&gt; \n  ggplot2::geom_point()\n\nError in `ggplot2::geom_point()`:\n! `mapping` must be created by `aes()`\nℹ Did you use `%&gt;%` or `|&gt;` instead of `+`?\n\n\n\nSolution\nUse + instead.\n\ndata |&gt; \n  ggplot2::ggplot(aes(x = x, y = y)) +\n  ggplot2::geom_point()"
  },
  {
    "objectID": "resources/faqs/ug_faqs.html#miscellaneous",
    "href": "resources/faqs/ug_faqs.html#miscellaneous",
    "title": "UG Analysis FAQs",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nDuplicate code\nAs we saw in the very first tutorial, duplicating code can result in accidentally reversing or overwriting values. This can frequently happen with reverse-coding and mean-centring. For example, if you write code to reverse-code a variable, and then duplicate that code, the values will be reversed again, resulting as the same values as the originals.\n\n## Original values\nnumbers &lt;- as.numeric(numbers)\nnumbers\n\n[1] 10  5  8\n\n## Apply the same transformation to all the numbers\nnumbers &lt;- 8 - numbers\nnumbers\n\n[1] -2  3  0\n\n## Duplicate code - applies the same transformation again\nnumbers &lt;- 8 - numbers\nnumbers # same as numbers we originally had \n\n[1] 10  5  8\n\n\nRe-running code out of order can have this same effect!\n\nSolution\nAlways run code in order, once through, for the “final” effect. Check copy/pasted code carefully and only run bits of code out of order if you know exactly what it does and check the results very thoroughly."
  },
  {
    "objectID": "quick_ref.html",
    "href": "quick_ref.html",
    "title": "Quick Reference",
    "section": "",
    "text": "Looking for a function you can’t quite remember how to use? You’re in the right place! The table below is arranged alphabetically by function name, and the linked full name (including relevant package calls) will take you to the help documentation.\n\n\n\n\n\nFunction Name\n\n\nLink to Help Documentation\n\n\nUsed In…\n\n\n\n\n\n\nacross()\n\n\ndplyr::across()\n\n\n06: Mutate and Summarise08: Analysis\n\n\n\n\nall()\n\n\nall()\n\n\n05: Filter and Select07: Visualisations\n\n\n\n\nanova()\n\n\nanova()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nany()\n\n\nany()\n\n\n05: Filter and Select\n\n\n\n\napa_print()\n\n\npapaja::apa_print()\n\n\n04: Reporting Linear Models with Quarto05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis\n\n\n\n\napa_table()\n\n\npapaja::apa_table()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\narrange()\n\n\ndplyr::arrange()\n\n\n05: Filter and Select\n\n\n\n\nas.numeric()\n\n\nas.numeric()\n\n\n01/02: IntRoduction\n\n\n\n\nas_tibble()\n\n\ntibble::as_tibble()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nautoplot()\n\n\nggplot2::autoplot()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nbetween()\n\n\ndplyr::between()\n\n\n05: Filter and Select\n\n\n\n\nboxplot()\n\n\nboxplot()\n\n\n03: Datasets\n\n\n\n\nc()\n\n\nc()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nc_across()\n\n\ndplyr::c_across()\n\n\n06: Mutate and Summarise\n\n\n\n\nchisq.test()\n\n\nchisq.test()\n\n\n07: Visualisations\n\n\n\n\nclass()\n\n\nclass()\n\n\n01/02: IntRoduction\n\n\n\n\ncontains()\n\n\ndplyr::contains()\n\n\n05: Filter and Select06: Mutate and Summarise\n\n\n\n\ncontr.sum()\n\n\ncontr.sum()\n\n\n08: Analysis\n\n\n\n\ncontrasts()\n\n\ncontrasts()\n\n\n08: Analysis\n\n\n\n\ncor.test()\n\n\ncor.test()\n\n\n06: Mutate and Summarise\n\n\n\n\ncorrelation()\n\n\ncorrelation::correlation()\n\n\n06: Mutate and Summarise\n\n\n\n\ncount()\n\n\ndplyr::count()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise\n\n\n\n\ndata()\n\n\ndata()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\ndescribe_distribution()\n\n\ndatawizard::describe_distribution()\n\n\n03: Datasets\n\n\n\n\ndrop_na()\n\n\ntidyr::drop_na()\n\n\n05: Filter and Select\n\n\n\n\nemmeans()\n\n\nemmeans::emmeans()\n\n\n08: Analysis\n\n\n\n\neverything()\n\n\ndplyr::everything()\n\n\n05: Filter and Select\n\n\n\n\nfilter()\n\n\ndplyr::filter()\n\n\n05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis\n\n\n\n\ngeom_histogram()\n\n\nggplot2::geom_histogram()\n\n\n07: Visualisations\n\n\n\n\nggpairs()\n\n\nGGally::ggpairs()\n\n\n06: Mutate and Summarise\n\n\n\n\nggscatmat()\n\n\nGGally::ggscatmat()\n\n\n06: Mutate and Summarise\n\n\n\n\nglance()\n\n\nbroom::glance()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\nglimpse()\n\n\ndplyr::glimpse()\n\n\n03: Datasets\n\n\n\n\ngsub()\n\n\ngsub()\n\n\n06: Mutate and Summarise10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nhere()\n\n\nhere::here()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nhist()\n\n\nhist()\n\n\n03: Datasets\n\n\n\n\nifelse()\n\n\nifelse()\n\n\n07: Visualisations\n\n\n\n\nis.character()\n\n\nis.character()\n\n\n05: Filter and Select\n\n\n\n\nis.na()\n\n\nis.na()\n\n\n05: Filter and Select10: Qualtrics and Labelled Data\n\n\n\n\nis.null()\n\n\nis.null()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\njoint_tests()\n\n\nemmeans::joint_tests()\n\n\n08: Analysis\n\n\n\n\nkable_classic()\n\n\nkableExtra::kable_classic()\n\n\n06: Mutate and Summarise\n\n\n\n\nkable_styling()\n\n\nkableExtra::kable_styling()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nlibrary()\n\n\nlibrary()\n\n\n03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nlm()\n\n\nlm()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\nlmRob()\n\n\nrobust::lmRob()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nmax()\n\n\nmax()\n\n\n06: Mutate and Summarise\n\n\n\n\nmean()\n\n\nmean()\n\n\n01/02: IntRoduction03: Datasets\n\n\n\n\nmedian()\n\n\nmedian()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nmodel_parameters()\n\n\nparameters::model_parameters()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\nn()\n\n\ndplyr::n()\n\n\n06: Mutate and Summarise\n\n\n\n\nna_values()\n\n\nlabelled::na_values()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nnames()\n\n\nnames()\n\n\n03: Datasets10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nncol()\n\n\nncol()\n\n\n03: Datasets\n\n\n\n\nnice_table()\n\n\nrempsyc::nice_table()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nnrow()\n\n\nnrow()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations\n\n\n\n\noptions()\n\n\noptions()\n\n\n07: Visualisations\n\n\n\n\npaste0()\n\n\npaste0()\n\n\n08: Analysis10: Qualtrics and Labelled Data\n\n\n\n\npick()\n\n\ndplyr::pick()\n\n\n05: Filter and Select\n\n\n\n\nplot()\n\n\nplot()\n\n\n03: Datasets\n\n\n\n\nposition_jitterdodge()\n\n\nggplot2::position_jitterdodge()\n\n\n08: Analysis\n\n\n\n\nprint()\n\n\nprint()\n\n\n06: Mutate and Summarise\n\n\n\n\npull()\n\n\ndplyr::pull()\n\n\n03: Datasets06: Mutate and Summarise10: Qualtrics and Labelled Data\n\n\n\n\nrange()\n\n\nrange()\n\n\n03: Datasets\n\n\n\n\nread_csv()\n\n\nreadr::read_csv()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight\n\n\n\n\nread_sav()\n\n\nhaven::read_sav()\n\n\n10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nrename()\n\n\ndplyr::rename()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nrep()\n\n\nrep()\n\n\n10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nreturn()\n\n\nreturn()\n\n\n06: Mutate and Summarise\n\n\n\n\nrglwidget()\n\n\nrgl::rglwidget()\n\n\n07: Visualisations\n\n\n\n\nround()\n\n\nround()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto\n\n\n\n\nrowMeans()\n\n\nrowMeans()\n\n\n06: Mutate and Summarise\n\n\n\n\nrowSums()\n\n\nrowSums()\n\n\n05: Filter and Select\n\n\n\n\nrowid_to_column()\n\n\ntibble::rowid_to_column()\n\n\n08: Analysis\n\n\n\n\nscale()\n\n\nscale()\n\n\n08: Analysis\n\n\n\n\nsd()\n\n\nsd()\n\n\n03: Datasets\n\n\n\n\nse()\n\n\npapaja::se()\n\n\n06: Mutate and Summarise06: Mutate and Summarise\n\n\n\n\nsqrt()\n\n\nsqrt()\n\n\n06: Mutate and Summarise\n\n\n\n\nstarts_with()\n\n\ndplyr::starts_with()\n\n\n05: Filter and Select06: Mutate and Summarise\n\n\n\n\nstr_to_title()\n\n\nstringr::str_to_title()\n\n\n06: Mutate and Summarise07: Visualisations08: Analysis10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nsub()\n\n\nsub()\n\n\n08: Analysis\n\n\n\n\nsum()\n\n\nsum()\n\n\n05: Filter and Select\n\n\n\n\nsummary()\n\n\nsummary()\n\n\n03: Datasets04: Reporting Linear Models with Quarto\n\n\n\n\nt.test()\n\n\nt.test()\n\n\n01/02: IntRoduction05: Filter and Select\n\n\n\n\ntheme_apa()\n\n\npapaja::theme_apa()\n\n\n07: Visualisations08: Analysis\n\n\n\n\ntheme_minimal()\n\n\nggplot2::theme_minimal()\n\n\n08: Analysis\n\n\n\n\ntidy()\n\n\nbroom::tidy()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\ntolower()\n\n\ntolower()\n\n\n06: Mutate and Summarise\n\n\n\n\ntoupper()\n\n\ntoupper()\n\n\n06: Mutate and Summarise\n\n\n\n\nungroup()\n\n\ndplyr::ungroup()\n\n\n06: Mutate and Summarise10: Qualtrics and Labelled Data\n\n\n\n\nunique()\n\n\nunique()\n\n\n10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nunlist()\n\n\nunlist()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nuser_na_to_na()\n\n\nlabelled::user_na_to_na()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nval_label()\n\n\nlabelled::val_label()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nval_labels()\n\n\nlabelled::val_labels()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nvar_label()\n\n\nlabelled::var_label()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nview_df()\n\n\nsjPlot::view_df()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nwhere()\n\n\ndplyr::where()\n\n\n05: Filter and Select08: Analysis\n\n\n\n\nwhich()\n\n\nwhich()\n\n\n06: Mutate and Summarise"
  },
  {
    "objectID": "quick_ref.html#index-of-functions",
    "href": "quick_ref.html#index-of-functions",
    "title": "Quick Reference",
    "section": "",
    "text": "Looking for a function you can’t quite remember how to use? You’re in the right place! The table below is arranged alphabetically by function name, and the linked full name (including relevant package calls) will take you to the help documentation.\n\n\n\n\n\nFunction Name\n\n\nLink to Help Documentation\n\n\nUsed In…\n\n\n\n\n\n\nacross()\n\n\ndplyr::across()\n\n\n06: Mutate and Summarise08: Analysis\n\n\n\n\nall()\n\n\nall()\n\n\n05: Filter and Select07: Visualisations\n\n\n\n\nanova()\n\n\nanova()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nany()\n\n\nany()\n\n\n05: Filter and Select\n\n\n\n\napa_print()\n\n\npapaja::apa_print()\n\n\n04: Reporting Linear Models with Quarto05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis\n\n\n\n\napa_table()\n\n\npapaja::apa_table()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\narrange()\n\n\ndplyr::arrange()\n\n\n05: Filter and Select\n\n\n\n\nas.numeric()\n\n\nas.numeric()\n\n\n01/02: IntRoduction\n\n\n\n\nas_tibble()\n\n\ntibble::as_tibble()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nautoplot()\n\n\nggplot2::autoplot()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nbetween()\n\n\ndplyr::between()\n\n\n05: Filter and Select\n\n\n\n\nboxplot()\n\n\nboxplot()\n\n\n03: Datasets\n\n\n\n\nc()\n\n\nc()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nc_across()\n\n\ndplyr::c_across()\n\n\n06: Mutate and Summarise\n\n\n\n\nchisq.test()\n\n\nchisq.test()\n\n\n07: Visualisations\n\n\n\n\nclass()\n\n\nclass()\n\n\n01/02: IntRoduction\n\n\n\n\ncontains()\n\n\ndplyr::contains()\n\n\n05: Filter and Select06: Mutate and Summarise\n\n\n\n\ncontr.sum()\n\n\ncontr.sum()\n\n\n08: Analysis\n\n\n\n\ncontrasts()\n\n\ncontrasts()\n\n\n08: Analysis\n\n\n\n\ncor.test()\n\n\ncor.test()\n\n\n06: Mutate and Summarise\n\n\n\n\ncorrelation()\n\n\ncorrelation::correlation()\n\n\n06: Mutate and Summarise\n\n\n\n\ncount()\n\n\ndplyr::count()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise\n\n\n\n\ndata()\n\n\ndata()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\ndescribe_distribution()\n\n\ndatawizard::describe_distribution()\n\n\n03: Datasets\n\n\n\n\ndrop_na()\n\n\ntidyr::drop_na()\n\n\n05: Filter and Select\n\n\n\n\nemmeans()\n\n\nemmeans::emmeans()\n\n\n08: Analysis\n\n\n\n\neverything()\n\n\ndplyr::everything()\n\n\n05: Filter and Select\n\n\n\n\nfilter()\n\n\ndplyr::filter()\n\n\n05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis\n\n\n\n\ngeom_histogram()\n\n\nggplot2::geom_histogram()\n\n\n07: Visualisations\n\n\n\n\nggpairs()\n\n\nGGally::ggpairs()\n\n\n06: Mutate and Summarise\n\n\n\n\nggscatmat()\n\n\nGGally::ggscatmat()\n\n\n06: Mutate and Summarise\n\n\n\n\nglance()\n\n\nbroom::glance()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\nglimpse()\n\n\ndplyr::glimpse()\n\n\n03: Datasets\n\n\n\n\ngsub()\n\n\ngsub()\n\n\n06: Mutate and Summarise10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nhere()\n\n\nhere::here()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nhist()\n\n\nhist()\n\n\n03: Datasets\n\n\n\n\nifelse()\n\n\nifelse()\n\n\n07: Visualisations\n\n\n\n\nis.character()\n\n\nis.character()\n\n\n05: Filter and Select\n\n\n\n\nis.na()\n\n\nis.na()\n\n\n05: Filter and Select10: Qualtrics and Labelled Data\n\n\n\n\nis.null()\n\n\nis.null()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\njoint_tests()\n\n\nemmeans::joint_tests()\n\n\n08: Analysis\n\n\n\n\nkable_classic()\n\n\nkableExtra::kable_classic()\n\n\n06: Mutate and Summarise\n\n\n\n\nkable_styling()\n\n\nkableExtra::kable_styling()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nlibrary()\n\n\nlibrary()\n\n\n03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nlm()\n\n\nlm()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\nlmRob()\n\n\nrobust::lmRob()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nmax()\n\n\nmax()\n\n\n06: Mutate and Summarise\n\n\n\n\nmean()\n\n\nmean()\n\n\n01/02: IntRoduction03: Datasets\n\n\n\n\nmedian()\n\n\nmedian()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nmodel_parameters()\n\n\nparameters::model_parameters()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\nn()\n\n\ndplyr::n()\n\n\n06: Mutate and Summarise\n\n\n\n\nna_values()\n\n\nlabelled::na_values()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nnames()\n\n\nnames()\n\n\n03: Datasets10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nncol()\n\n\nncol()\n\n\n03: Datasets\n\n\n\n\nnice_table()\n\n\nrempsyc::nice_table()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nnrow()\n\n\nnrow()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations\n\n\n\n\noptions()\n\n\noptions()\n\n\n07: Visualisations\n\n\n\n\npaste0()\n\n\npaste0()\n\n\n08: Analysis10: Qualtrics and Labelled Data\n\n\n\n\npick()\n\n\ndplyr::pick()\n\n\n05: Filter and Select\n\n\n\n\nplot()\n\n\nplot()\n\n\n03: Datasets\n\n\n\n\nposition_jitterdodge()\n\n\nggplot2::position_jitterdodge()\n\n\n08: Analysis\n\n\n\n\nprint()\n\n\nprint()\n\n\n06: Mutate and Summarise\n\n\n\n\npull()\n\n\ndplyr::pull()\n\n\n03: Datasets06: Mutate and Summarise10: Qualtrics and Labelled Data\n\n\n\n\nrange()\n\n\nrange()\n\n\n03: Datasets\n\n\n\n\nread_csv()\n\n\nreadr::read_csv()\n\n\n03: Datasets05: Filter and Select06: Mutate and Summarise07: Visualisations08: Analysis09: Test Flight\n\n\n\n\nread_sav()\n\n\nhaven::read_sav()\n\n\n10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nrename()\n\n\ndplyr::rename()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nrep()\n\n\nrep()\n\n\n10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nreturn()\n\n\nreturn()\n\n\n06: Mutate and Summarise\n\n\n\n\nrglwidget()\n\n\nrgl::rglwidget()\n\n\n07: Visualisations\n\n\n\n\nround()\n\n\nround()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto\n\n\n\n\nrowMeans()\n\n\nrowMeans()\n\n\n06: Mutate and Summarise\n\n\n\n\nrowSums()\n\n\nrowSums()\n\n\n05: Filter and Select\n\n\n\n\nrowid_to_column()\n\n\ntibble::rowid_to_column()\n\n\n08: Analysis\n\n\n\n\nscale()\n\n\nscale()\n\n\n08: Analysis\n\n\n\n\nsd()\n\n\nsd()\n\n\n03: Datasets\n\n\n\n\nse()\n\n\npapaja::se()\n\n\n06: Mutate and Summarise06: Mutate and Summarise\n\n\n\n\nsqrt()\n\n\nsqrt()\n\n\n06: Mutate and Summarise\n\n\n\n\nstarts_with()\n\n\ndplyr::starts_with()\n\n\n05: Filter and Select06: Mutate and Summarise\n\n\n\n\nstr_to_title()\n\n\nstringr::str_to_title()\n\n\n06: Mutate and Summarise07: Visualisations08: Analysis10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nsub()\n\n\nsub()\n\n\n08: Analysis\n\n\n\n\nsum()\n\n\nsum()\n\n\n05: Filter and Select\n\n\n\n\nsummary()\n\n\nsummary()\n\n\n03: Datasets04: Reporting Linear Models with Quarto\n\n\n\n\nt.test()\n\n\nt.test()\n\n\n01/02: IntRoduction05: Filter and Select\n\n\n\n\ntheme_apa()\n\n\npapaja::theme_apa()\n\n\n07: Visualisations08: Analysis\n\n\n\n\ntheme_minimal()\n\n\nggplot2::theme_minimal()\n\n\n08: Analysis\n\n\n\n\ntidy()\n\n\nbroom::tidy()\n\n\n04: Reporting Linear Models with Quarto08: Analysis\n\n\n\n\ntolower()\n\n\ntolower()\n\n\n06: Mutate and Summarise\n\n\n\n\ntoupper()\n\n\ntoupper()\n\n\n06: Mutate and Summarise\n\n\n\n\nungroup()\n\n\ndplyr::ungroup()\n\n\n06: Mutate and Summarise10: Qualtrics and Labelled Data\n\n\n\n\nunique()\n\n\nunique()\n\n\n10: Qualtrics and Labelled Data11: Qualtrics II\n\n\n\n\nunlist()\n\n\nunlist()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nuser_na_to_na()\n\n\nlabelled::user_na_to_na()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nval_label()\n\n\nlabelled::val_label()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nval_labels()\n\n\nlabelled::val_labels()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nvar_label()\n\n\nlabelled::var_label()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nview_df()\n\n\nsjPlot::view_df()\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nwhere()\n\n\ndplyr::where()\n\n\n05: Filter and Select08: Analysis\n\n\n\n\nwhich()\n\n\nwhich()\n\n\n06: Mutate and Summarise"
  },
  {
    "objectID": "quick_ref.html#index-of-topics",
    "href": "quick_ref.html#index-of-topics",
    "title": "Quick Reference",
    "section": "Index of Topics",
    "text": "Index of Topics\nIf you’re looking for a particular section of a tutorial, use this handy summary to jump straight to the section you want.\n\n\n\n\n\nTopic\n\n\nSub-Topic\n\n\n\n\n\n\n01/02: IntRoduction\n\n\n\n\nOrientation\n\n\nThe RStudio Interface\n\n\n\n\nErrors\n\n\nGlossoRlia: the Language of Errors\n\n\n\n\nTypes of Data\n\n\nNumeric Data, Character Data, Logical Data\n\n\n\n\nClass and Coercion\n\n\n\n\n\n\nObjects\n\n\nCreating an Object, Calling an Object, Using Objects, Overwriting Objects\n\n\n\n\nFunctions\n\n\nBasics and Help, Arguments, Using Functions\n\n\n\n\nQuick Test: t-test\n\n\n\n\n\n\n03: Datasets\n\n\n\n\nSetup\n\n\nProjects, Documents, Installing and Loading Packages\n\n\n\n\nReading In\n\n\nReading from File\n\n\n\n\nCodebook\n\n\n\n\n\n\nViewing\n\n\nCall the Object, A Glimpse of the Data, View Mode\n\n\n\n\nOverall Summaries\n\n\nBasic Summary, Other Summaries\n\n\n\n\nThe Pipe\n\n\n\n\n\n\nDescribing Datasets\n\n\n\n\n\n\nDescribing Variables\n\n\nCounting, Subsetting, Descriptives, Visualisations\n\n\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nThe Linear Model\n\n\nData and Codebook, One Predictor, Hierarchial Models, Assumptions Checks\n\n\n\n\nQuarto\n\n\nGetting Started, Creating a Code Chunk, Body Text, Dynamic Reporting, Rendering\n\n\n\n\n05: Filter and Select\n\n\n\n\nSetup\n\n\nPackages, Data\n\n\n\n\nFilter\n\n\nGeneral Format, Filtering with Assertions, Multiple Assertions, Data Cleaning\n\n\n\n\nSelect\n\n\nGeneral Format, Selecting Directly, Using {tidyselect}\n\n\n\n\nQuick Test: t-test redux\n\n\n\n\n\n\n06: Mutate and Summarise\n\n\n\n\nSetup\n\n\nPackages, Data\n\n\n\n\nMutate\n\n\nGeneral Format, Adding and Changing Variables, Composite Scores, Conditionals, Iteration\n\n\n\n\nSummarise\n\n\nGeneral Format, By Group, Iteration\n\n\n\n\nFormatting with {kableExtra}\n\n\nEssential Formatting, Dynamic Formatting\n\n\n\n\nQuick Test: Correlation\n\n\nVisualisation, Testing Correlation\n\n\n\n\n07: Visualisations\n\n\n\n\nNA\n\n\nWhat do UGs know?\n\n\n\n\nSetup\n\n\nPackages, Data\n\n\n\n\nGrammar of Graphics\n\n\nLayers, Mapping, Geoms, etc.\n\n\n\n\nHistograms and Density Plots\n\n\nAdjusting Binwidth, Colour and Fill Manual, Adjusting Axes with scale_*(), Adding a Theme, Facet Wrap\n\n\n\n\nBarplots\n\n\nPosition Dodge, Reordering Categories\n\n\n\n\nQuick Test: \\(\\chi^2\\)\n\n\n\n\n\n\nRaincloud and Violin Plots\n\n\nCalculating Stats\n\n\n\n\nScatterplots\n\n\nLine of Best Fit, Colour and Fill Palette, 3D Plots\n\n\n\n\nReporting with Quarto\n\n\nCaptions and Alt Text, Cross-Referencing\n\n\n\n\n08: Analysis\n\n\n\n\nSetup\n\n\nPackages, Data\n\n\n\n\nCategorical Predictors\n\n\nComparing Several Means, What about ANCOVA?, Factorial Designs, Mixed Designs\n\n\n\n\nContinuous Predictors\n\n\nMediation, Moderation\n\n\n\n\n09: Test Flight\n\n\n\n\nSetup\n\n\nData\n\n\n\n\nData Analysis Tips\n\n\nPlan Your Process, Keep a Record, Be Consistent, Reduce Redundancies\n\n\n\n\nThe Whole Shebang\n\n\nPlanning, Inspecting, Cleaning, Wrangling, Summarising, Analysing, Visualising, Reporting\n\n\n\n\n10: Qualtrics and Labelled Data\n\n\n\n\nNA\n\n\nAcknowledgements\n\n\n\n\nSetup\n\n\nPackages, Data\n\n\n\n\nQualtrics Data\n\n\nSAV Data, Exporting from Qualtrics\n\n\n\n\nThe Plan\n\n\n\n\n\n\nCleanup and Data Dictionary\n\n\nRenaming Variables, Separating Columns, Viewer Data Dictionary\n\n\n\n\nLabelled Data\n\n\nVariable Labels, Value Labels, Missing Values\n\n\n\n\nConverting Variables\n\n\nFactor, Numeric, Conditional Conversion\n\n\n\n\n11: Qualtrics II\n\n\n\n\nNA\n\n\nAcknowledgements\n\n\n\n\nSetup\n\n\nPackages, Data\n\n\n\n\nBuilding a Qualtrics Project\n\n\nChoose Your Path, Accessing Qualtrics, Creating a Project\n\n\n\n\nMinimal\n\n\nSet Up Qualtrics, Testing the Study\n\n\n\n\nModerate\n\n\nUsing Templates, Developing a Script\n\n\n\n\nMajority\n\n\nSurvey Creation System, Survey Access System"
  },
  {
    "objectID": "data_workbooks.html",
    "href": "data_workbooks.html",
    "title": "Data and Workbooks",
    "section": "",
    "text": "Download and save datasets to use for tutorial tasks here. Either copy the link to use in a reading-in function (e.g. readr::read_csv(), haven::read_sav()), or save the data at the link to a file to read in. Make sure that if you save the file, you keep the correct file suffix!\n\n\n\n\n\nFilename\n\n\nCitation/Source\n\n\nComments\n\n\nDownload\n\n\n\n\n\n\nanx_data.csv\n\n\nTerry, Lea, & Field (in prep)\n\n\nDataset shared for teaching purposes. Note that demographics in this dataset are SIMULATED FOR TEACHING PURPOSES and are NOT real data.\n\n\nDownload anx_data.csv\n\n\n\n\nanx_scores_data.csv\n\n\nTerry, Lea, & Field (in prep)\n\n\nDataset shared for teaching purposes, with mean subscale scores instead of individual items. Note that demographics in this dataset are SIMULATED FOR TEACHING PURPOSES and are NOT real data.\n\n\nDownload anx_scores_data.csv\n\n\n\n\nbp_data.csv\n\n\nSimon & Hurst (2021)\n\n\nDataset publicly available. Note that this subset of the public data has had ID variables and missing values randomly introduced for teaching purposes.\n\n\nDownload bp_data.csv\n\n\n\n\nmil_data.sav\n\n\nEldarwish et al., (in prep)\n\n\nDataset expected to be publicly available in the future. Note that this dataset is entirely simulated, based on existing real data.\n\n\nDownload mil_data.sav\n\n\n\n\nmil_data_wkshp.sav\n\n\nEldarwish et al., (in prep)\n\n\nVersion of mil_data.sav with additional simulated variables for workshop use, contributed by Dr Dan Evans\n\n\nDownload mil_data_wkshp.sav\n\n\n\n\nsyn_data.csv\n\n\nMealor et al., 2016\n\n\nDataset publicly available\n\n\nDownload syn_data.csv"
  },
  {
    "objectID": "data_workbooks.html#datasets",
    "href": "data_workbooks.html#datasets",
    "title": "Data and Workbooks",
    "section": "",
    "text": "Download and save datasets to use for tutorial tasks here. Either copy the link to use in a reading-in function (e.g. readr::read_csv(), haven::read_sav()), or save the data at the link to a file to read in. Make sure that if you save the file, you keep the correct file suffix!\n\n\n\n\n\nFilename\n\n\nCitation/Source\n\n\nComments\n\n\nDownload\n\n\n\n\n\n\nanx_data.csv\n\n\nTerry, Lea, & Field (in prep)\n\n\nDataset shared for teaching purposes. Note that demographics in this dataset are SIMULATED FOR TEACHING PURPOSES and are NOT real data.\n\n\nDownload anx_data.csv\n\n\n\n\nanx_scores_data.csv\n\n\nTerry, Lea, & Field (in prep)\n\n\nDataset shared for teaching purposes, with mean subscale scores instead of individual items. Note that demographics in this dataset are SIMULATED FOR TEACHING PURPOSES and are NOT real data.\n\n\nDownload anx_scores_data.csv\n\n\n\n\nbp_data.csv\n\n\nSimon & Hurst (2021)\n\n\nDataset publicly available. Note that this subset of the public data has had ID variables and missing values randomly introduced for teaching purposes.\n\n\nDownload bp_data.csv\n\n\n\n\nmil_data.sav\n\n\nEldarwish et al., (in prep)\n\n\nDataset expected to be publicly available in the future. Note that this dataset is entirely simulated, based on existing real data.\n\n\nDownload mil_data.sav\n\n\n\n\nmil_data_wkshp.sav\n\n\nEldarwish et al., (in prep)\n\n\nVersion of mil_data.sav with additional simulated variables for workshop use, contributed by Dr Dan Evans\n\n\nDownload mil_data_wkshp.sav\n\n\n\n\nsyn_data.csv\n\n\nMealor et al., 2016\n\n\nDataset publicly available\n\n\nDownload syn_data.csv"
  },
  {
    "objectID": "data_workbooks.html#workbooks",
    "href": "data_workbooks.html#workbooks",
    "title": "Data and Workbooks",
    "section": "Workbooks",
    "text": "Workbooks\nRight-click the link below and choose Save link as… to save as a .Qmd, or click through to view the text and copy/paste into an empty .Qmd document.\n\n\n\n\n\nTutorial/Workshop Name\n\n\nDownload\n\n\n\n\n\n\n01/02: IntRoduction\n\n\nDownload 01_02_intro_workbook.qmd\n\n\n\n\n03: Datasets\n\n\nDownload 03_datasets_workbook.qmd\n\n\n\n\n04: Reporting Linear Models with Quarto\n\n\nDownload 04_lm_workbook.qmd\n\n\n\n\n05: Filter and Select\n\n\nDownload 05_filter_workbook.qmd\n\n\n\n\n06: Mutate and Summarise\n\n\nDownload 06_changes_workbook.qmd\n\n\n\n\n07: Visualisations\n\n\nDownload 07_dataviz_workbook.qmd\n\n\n\n\n08: Analysis\n\n\nDownload 08_analysis_workbook.qmd\n\n\n\n\n09: Test Flight\n\n\nDownload 09_testflight_workbook.qmd\n\n\n\n\n10: Qualtrics and Labelled Data\n\n\nDownload 10_qtrics_workbook.qmd\n\n\n\n\nWorking with Qualtrics Data\n\n\nDownload qualtrics_workshop_workbook.qmd"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jennifer Mankin",
    "section": "",
    "text": "Jennifer is a senior teaching-focused lecturer in Psychology at the University of Sussex. She is committed to making statistics and coding accessible, enjoyable, and engaging, particularly for learners who aren’t that enthusiastic about the whole endeavour.\nJennifer began teaching at the University of Sussex in 2016, originally teaching cognitive psychology and research methods and statistics with SPSS. In the 2019/2020 academic year, Psychology at Sussex switched to R for undergraduate methods and statistics teaching1. Since then, Jennifer has been just on the bright side of being hell-bent on helping anyone who will sit still long enough experience the joy of R.\nThe materials on this website are an attempt to do that: to share some of the hard-won skills and knowledge to support others to get excited about the potential of coding in R.\nIn her non-coding time, Jennifer does cross-stitch, plays D&D, potters about the garden, and collects oversized earrings."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "Jennifer Mankin",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYep, the timing was just about as bad as it could have been!↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Training at Sussex",
    "section": "",
    "text": "This is the website for training courses in the programming language R, run by the Methods Teaching Team in the School of Psychology, University of Sussex.\n\n\nAt the moment, our training sessions are only open to members of staff in the School of Psychology at Sussex, or by invitation.\nIf you are staff in the School of Psychology, see the School Bulletin for how to join the Canvas site and live sessions. Otherwise, you can email the training lead, but be aware that places are only available a case-by-case, exceptional basis for University of Sussex staff (academic or Professional Services) and only if there is space in the planned sessions."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "R Training at Sussex",
    "section": "",
    "text": "This is the website for training courses in the programming language R, run by the Methods Teaching Team in the School of Psychology, University of Sussex.\n\n\nAt the moment, our training sessions are only open to members of staff in the School of Psychology at Sussex, or by invitation.\nIf you are staff in the School of Psychology, see the School Bulletin for how to join the Canvas site and live sessions. Otherwise, you can email the training lead, but be aware that places are only available a case-by-case, exceptional basis for University of Sussex staff (academic or Professional Services) and only if there is space in the planned sessions."
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "R Training at Sussex",
    "section": "Materials",
    "text": "Materials\n\nTutorials\nThe Tutorials section of the site contains tutorial documents designed to accompany live training sessions. They provide explanations, examples, and exercises designed for complete beginners through improvers.\n\n\nWorksheets\nWorksheets are hosted on the Posit Cloud workspace for the training course. You can join the workspace via Canvas.\n\n\nRecordings\nLive sessions are recorded and made available, with automatically-generated captions, as soon as possible after the sessions are complete. View session recordings on Canvas."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Under Construction\n\n\n\nThis section is still under construction. Check back for more in the future!\n\n\nThis section will be added to as necessary to support staff in their progress with R, for dissertation supervision and for the own work. See the sections in the sidebar for the documents currently available.\nIf you have a suggestion for a FAQ, guide, or other resource that would be helpful, you can drop it in the Suggestion Box on Canvas or get in touch."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Under Construction\n\n\n\nMany of the tutorials are still incomplete or under construction. If you encounter a banner like this, just check back another time!"
  },
  {
    "objectID": "tutorials.html#using-the-tutorials",
    "href": "tutorials.html#using-the-tutorials",
    "title": "Tutorials",
    "section": "Using the Tutorials",
    "text": "Using the Tutorials\nThese tutorials are designed to accompany live training sessions, but they also serve as quick-reference guides for all the material covered in those sessions.\n\nOpening the Tutorials\nIn live sessions, it is recommended to open the corresponding tutorial in the Viewer pane in Posit Cloud so that solutions and explanations are easily available. The workbook documents provided for each week will already contain the code to do this.\nHowever, the tutorials can also be easily accessed at any time through this website, so it isn’t necessary to open Posit Cloud to view them - simply use the sidebar to jump to the tutorial you want!\n\n\nExercises\nThe exercises are designed to build your skills in R. Some will ask you to try something you might not know how to do. Just give it your best shot, and if you get stuck, solutions and explanations are always provided!\n\n\n\n\n\n\nDon’t Skip the Exercises!\n\n\n\nIt is strongly recommended that you don’t skip the exercises. Try each one, even if it seems completely trivial, or too hard. You will learn R much faster and more thoroughly by getting your hands dirty.\n\n\nAll data and workbooks will be provided on Posit Cloud for completing the exercises.\n\nChallenges\nSome exercises will be clearly labeled as “Challenges”. These exercises are optional and are meant to go beyond the core tutorial material. However, if you skip them, you will still be able to understand everything that follows; you won’t need to complete them in order to proceed."
  },
  {
    "objectID": "tutorials.html#content",
    "href": "tutorials.html#content",
    "title": "Tutorials",
    "section": "Content",
    "text": "Content\n\n\n\n\n\n\nTip\n\n\n\nLooking for a particular topic or function? Use the Quick Reference to find what you need!\n\n\nTutorials are divided into three sections.\n\nFundRmentals\nThe three-part FundRmentals series covers essential basic skills in R, and is designed for absolute beginners who have never seen R before and who have little to no coding experience of any kind.\nBy the end of this series, you will be able to:\n\nNavigate the RStudio IDE\nCreate and work with different types of data\nWork with objects and functions\nPerform calculations and logical tests on single values and vectors\nRead in data from a .csv file into a tibble\nView, summarise, and arrange the order of a tibble\nCreate and render Quarto documents\nPerform and report t-test and linear model analyses\n\n\n\nEssentials\nThe four-part Essentials series is designed for novices with some basic skills in R, and follows on from the FundRmentals series. It covers the core data wrangling and analysis skills that we teach throughout the first year of the undergraduate Psychology course at Sussex, along with extra tips and techniques for efficient and transparent analysis to support dissertation supervisors to help their students.\nBy the end of this series, you will be able to:\n\nFilter cases and select variables, including efficient &lt;tidyselect&gt; semantics\nCreate new variables in a dataset, or change/recode existing ones\nCreate a variety of customised data visualisations\nPerform and efficiently report the results of t-tests, chi-squared tests, correlations, and simple and hierarchical linear models\n\n\n\nImprovRs\nThe three-part ImprovRs series is designed for those with a strong foundation in R who want to move to using R in part or entirely for their data management and analysis process, and follows on from the Essentials series. It covers specific skills in creating Qualtrics questionnaires, working with questionnaire data, and advanced data wrangling, with the aim of building a diverse toolbox of R skills.\nBy the end of this series, you will be able to:\n\nWork with a new dataset from beginning (reading in, inspecting, cleaning) to end (wrangling, visualising, analysing, reporting)\nDesign a Qualtrics questionnaire with efficient analysis in mind\nWork with labelled Qualtrics data and factors\nCreate standardised subscale scores\nGenerate automatic codebooks for Qualtrics datasets"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html",
    "title": "03: Datasets",
    "section": "",
    "text": "This tutorial is focused on working with datasets. It covers key functions and tips for reading in, viewing, and summarising datasets. It also introduces the pipe operator and a variety of common descriptive functions for investigating both whole datasets and individual variables, and concludes with a brief look at data visualisations with base R."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overview",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overview",
    "title": "03: Datasets",
    "section": "",
    "text": "This tutorial is focused on working with datasets. It covers key functions and tips for reading in, viewing, and summarising datasets. It also introduces the pipe operator and a variety of common descriptive functions for investigating both whole datasets and individual variables, and concludes with a brief look at data visualisations with base R."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#setup",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#setup",
    "title": "03: Datasets",
    "section": "Setup",
    "text": "Setup\nIn each session, we will always follow the same steps to set up. We’ll walk through the key elements here in detail and then provide a brief summary in future tutorials.\n\n\n\n\n\n\nSetup Steps\n\n\n\n\nCreate or open a project in RStudio\nCreate or open a document to work in\nLoad the necessary packages\n\n\n\n\nProjects\nProjects are the main way that RStudio organises related files and information. Projects are associated with a working directory; everything in that directory, including all the sub-folders, are part of the same project.\nIt is highly recommended that you create a new project for each separate thing you want to work on in R. Among other advantages, it makes navigating folders much easier (see Reading In), allows you to easily pick up work from where you left off, and retain all the settings and options you have set each time you work on the same project.\n\n\n\n\n\n\nCreating a Project\n\n\n\n\n\nOn Posit Cloud, you don’t really have a choice in the matter - you must create or open a project in the Cloud workspace in order to do anything.\nOn a desktop installation, you can create a new directory as a project or associate a project file with an existing directory.\nSee Posit’s Using RStudio Projects guide or Andy Field’s video guide to RStudio Desktop for more information.\n\n\n\n\n\nDocuments\nAs we discussed in the previous tutorial, one of the key strengths of doing your work using R (or any other programming language) is reproducibility - in other words, every step of the process from raw file to output is documented and replicable. However, this is only the case if you do in fact write your code down somewhere! To do that, you’ll need to create some kind of document to record your code. There are two main types of documents you might consider using: a script or a Quarto document.\n\nQuarto documents\nQuarto documents contain a combination of both non-code text and code. The main strength of these documents is that they can be rendered to some other output format, such as HTML, PDF, or Word, by executing all of the code and combining it with the text to create a nicely formatted document.\nWe will investigate the options for Quarto documents in depth in the next tutorial. For now, use the Quarto document in your project on Posit Cloud for your work in this tutorial.\n\n\nScripts\nScripts are text files that RStudio knows to read as R code. They have a .R file extension and can ONLY contain code. They are very useful for running code behind the scenes, so to speak, but not great for reviewing or presenting results.\n\n\n\n\n\n\nQuarto or Script?\n\n\n\nWhen deciding what kind of document to create, think about what you want to do with the output of your work.\n\nUse Quarto if the document needs to contain any amount of text, or will be used to share the output of your code in a presentable way, such as notes for yourself, reports, articles, websites, etc.\nThe page you’re reading now is (or was!) a Quarto document.\nUse a script if the document only needs to contain code and has a solely functional purpose, such as cleaning a dataset, manipulating files, defining new functions, etc.\nI use a script to process all of the tutorial documents and generate the Quick Reference page.\n\nIn this series, we will almost always use Quarto documents, but scripts are an essential part of the development side of R.\n\n\n\n\n\nInstalling and Loading Packages\nIn the previous tutorial, we saw how the main way that R does anything is via functions. All functions belong to a package, which are extensions to the R language. Packages can contain functions, documentation for those functions, datasets, sample code, and more. Some packages, like the {base} and {stats} packages that contain the mean() and t.test() functions that we saw previously, are included by default with R. However, you will often want to use functions from packages that aren’t included by default, so you must do this explicitly.\nIn order to utilise the functions in a package, you must do two things:\n\nInstall the package (only once per device, or to update the package) using install.packages(\"package_name\") in the Console\nLoad the package (every time you want to use it) using library(package_name) at the beginning of each new document\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you are working on these tutorials on the Posit Cloud workspace, all of the packages you need have been installed already. Please do not try to install any packages, as this could cause unexpected conflicts or errors.\n\n\n\n\n\n\n\n\nMore About Installing vs Loading Packages\n\n\n\n\n\nWhen you install R and RStudio for the first time on a device, this is like buying a new mobile phone. When you get a new phone, it comes with some apps pre-installed, like a messaging app, a camera, a calculator, etc. If you only ever wanted to take pictures and do basic maths with your phone, you could probably leave it at that. Most likely, though, you want to use other apps that don’t come with the phone - like WhatsApp, or Facebook. Let’s say you’ve just got a new phone and you want to use WhatsApp. To do this, you’ll need to:\n\nGo to your phone’s app store and download WhatsApp (only once per device, or to update the app)\nOpen the app (every time you want to use it)\n\nAs you can see, these steps correspond almost exactly to the installing vs loading steps described above. In order to use a package that doesn’t come pre-installed with R, you have to do both of these things.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLoad the {tidyverse} package in your Quarto document.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\nWhen you load {tidyverse} for the first time, quite a lot of extra stuff gets printed along with it. All this output looks alarming, but these aren’t errors or warnings - they’re just messages. Messages are like warnings, but neutral: they just contain information that you might find helpful.\nThe usual {tidyverse} message contains two parts:\n\nAttaching core tidyverse packages tells you which packages have just been loaded. Essentially, library(tidyverse) is a shortcut for loading all of these packages individually. Somewhat confusingly, installing {tidyverse} installs more packages than are in this list (for example, {magrittr} and {rlang}), many of which other {tidyverse} packages rely on to function. If you want to load them, you can use library() to do this - but you don’t need to unless you’re using those packages explicitly. For our purposes now, just the default {tidyverse} packages are fine.\nConflicts tells you about any package conflicts as a result of loading the packages. If you’re curious, conflicts are explained further in the callout box below.\n\n\n\n\n\n\n\nConflicts\n\n\n\n\n\nThere are lots and lots of packages for R. At the time of this writing, CRAN (the repository for R packages) contains just shy of 20,000 R packages, with many, many more on Github and elsewhere. Although people generally try to avoid it, it is necessarily the case that sometimes, people give the same name to two different functions from two different packages.\nSo, if you have those packages both loaded, how does R know which one to use? This situation is called a conflict, and is resolved in a few different methods.\nMethod 1: Recency\nIn the absence of any other information, R will use the function from the package that was loaded most recently. This is exactly what’s happening in the {tidyverse} message above.\nThere are two conflicts mentioned. one of which reads:\nx dplyr::filter() masks stats::filter()\n{stats}, you might remember, is a package that is always installed with R and is loaded by default. So, the {stats} package has a function called filter() that is already loaded to begin with. When we loaded {tidyverse}, one of the new packages, {dplyr}, also contains a function called filter(). Because {dplyr} has been loaded more recently, if you write a bit of code using filter(), the one you will get is dplyr::filter()1. In other words, the more recently loaded dplyr::filter() covers over, steps in front of, or (in R terminology) “masks” stats::filter().\nNow, what if you actually want to use filter() from {stats} instead? Well, in that case you might want to use…\nMethod 2: Explicit style\nAbove we saw several examples of the package::function() notation, called “explicit” or “verbose” coding style. With explicit style, there isn’t actually a conflict between stats::filter() and dplyr::filter() anymore, because their package calls are clearly stated so R doesn’t have to guess which filter() you want. So, if you had loaded {tidyverse}, you could write stats::filter() in your code, and you would still get the function from the {stats} package even with {dplyr} loaded.\nAnother, secret benefit of explicit style is that as long as you have a package installed, you can use a function from that package without having to load it. Imagine I start a new project (so I have stats::filter() already loaded by default). If I only want to quickly use dplyr::filter(), I can use explicit style to use that function without having to run library(tidyverse).\n\n\n\nThe style you’ll see in these tutorials is a pretty devotedly explicit style: that is, I’ll always write dplyr::filter() instead of just filter(). I only leave out the package name in a few situations:\n\nWhen the function is from a default-loaded package, like {base} or {stats} (so I write mean() instead of base::mean()). This is mostly just convenience!\nWhen there are lots of functions from the same package in row, all of which have very distinctive names, that would make reading the code very difficult and writing the code very repetitive. This is the case, for example, with {ggplot2}, which we will encounter in the next section of the course. For cases like this, I make sure to explicitly load the relevant package and then leave off the package name.\n\nI like explicit style because I never have to deal with package conflicts; I rarely have to load packages; and it helps me understand better how my code works. You don’t have to use it, and most of the time it won’t make that much difference, so do what makes sense to you. Just to be safe, though, we’ll load {tidyverse} regularly."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#reading-in",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#reading-in",
    "title": "03: Datasets",
    "section": "Reading In",
    "text": "Reading In\nNow that we’ve completed our core setup, we’re ready to get stuck in working with datasets. For the purposes of practicing, we’re going to use some real, open-source data from a real paper by Mealor et al. from their (2016) paper developing the Sussex Cognitive Styles Questionnaire (SCSQ). The SCSQ is a six-factor cognitive styles questionnaire validated by comparing responses from people with and without synaesthesia.\nWe’re going to start by importing, or “reading in”, the data from a location outside of R. The first job is to work out how that data is stored: the file format that it’s in, and the location we need to give to R to look for it.\nFor the purposes of these tutorials, we’ll primarily make use of .csv file types when reading in. CSV (Comma-Separated Values) is a common, programme-agnostic file type without any fancy formatting, just plain text. To practice this, we’ll use the read_csv() function from the {readr} package (part of {tidyverse}).\nOne advantage of readr::read_csv() is that it will output a special kind of dataset, called a tibble. Tibbles are a fundamental component of {tidyverse}. They are a sort of embellished dataframe or table (“table” &gt; “tbl” &gt; “tibble”) with some extra bells and whistles for convenience. We’ll discover their features as we go, but you can get a quick overview of tibbles here.\n\n\n\n\n\n\nReading Other Filetypes\n\n\n\n\n\nIf you have data stored in other types of files, you may need other functions from other packages to read them in. It will depend substantially on what’s in the file and how the data are structured, so you will likely need to do some experimentation to find the best option.\nHere are some possibilities to get you started. All of them (except the last) output a tibble.\n\nExcel (xlsx): readxl::read_xlsx()\nSPSS (.sav): haven::read_sav()\nSAS (.sas): haven::read_sas()\nJSON: rjson::fromJSON()\n\n\n\n\nOur next job is to figure out where the data is stored. For the purposes of practice, we’ll look at two possibilities. First, that the data is stored in a local file on your computer; and second, that the data is hosted online somewhere, accessible via URL.\n\nReading from File\nThe scenario you are most likely to encounter is that you have some data in a folder on your computer, and you’d like to import, or “read in”, this data to R so you can work with it. To practice this, in your Posit Cloud project there is a folder named “data” that contains a file called “syn_data.csv”. (If you are not on the Cloud, skip down to the next section.)\nIn order to use the read_csv() function, we need to give it the file path as a string (i.e. in \"quotes\"). Let’s make this easier by using a helper function: here::here().\n\n\n\n\n\n\nExercise\n\n\n\nRun the here::here() function to see what it does.\n\n\nWhat you should get is a string - a file path to the project you are currently in. On Posit Cloud, this will always be “/cloud/project” (unless you change it). In any case, it will always point to the location of the .Rproj file that denotes your current project.\nWhy is this useful? Instead of having to write out long file paths (“C/Users/my_folder/What Was It called-again?/…”), or trying to figure out where your current file is relative to the data (or image, or whatever) that you are trying to find, here::here() uses the project file as a fixed point. So, all file paths can be written starting from the same point.\n\n\n\n\n\n\nExercise\n\n\n\nUse here::here() to generate a file path to the syn_data data file.\nThen, use readr::read_csv() to read in the syn_data.csv file and store the result in an object called syn_data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming you are on the Cloud, your here::here() command should look like this. The first part of the file path will be generated by here::here() up to the project file; from there, we look in the data folder, in which we can find the syn_data.csv file. (Don’t forget the .csv file extension!)\n\nhere::here(\"data/syn_data.csv\")\n\nTo read in the file, we add two things. First, we put the here::here() command - which outputs the file path - into readr::read_csv(), which actually imports the data at that file path into a tibble. Then, we save that tibble into an object called syn_data using the assignment operator, &lt;-.\n\nsyn_data &lt;- readr::read_csv(here::here(\"data/syn_data.csv\"))\n\n\n\n\n\n\n\nReading from URL\nIf the data is hosted somewhere online, you can give the hosting URL to R as a string. Assuming you have an Internet connection (!), R will go to that URL and parse the data.\n\n\n\n\n\n\nExercise\n\n\n\nRead the CSV file hosted at https://raw.githubusercontent.com/drmankin/practicum/master/data/syn_data.csv and save it to the object name syn_data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/syn_data.csv\")"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#codebook",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#codebook",
    "title": "03: Datasets",
    "section": "Codebook",
    "text": "Codebook\nAs this dataset is likely unfamiliar, the codebook below explains what the variables in this dataset represent.\n\n\n\n\n\n\nMore About Synaesthesia\n\n\n\n\n\nThis dataset focuses on cognitive styles, particularly in people with and without a neuropsychological condition called synaesthesia. Synaesthesia is colloquially referred to as a “blurring of the senses” that can manifest in many different ways. For example, some people with synaesthesia may percieve colours associated with letters or words, or see shapes when they hear music. These additional perceptions are typically automatic and consistent across time.\nThis particular study focused on two different types of synaesthesia: grapheme-colour and sequence-space. People with grapheme-colour synaesthesia experience colour associated with written language, i.e. graphemes. For instance, the letter “Q” may be purple, or the word “cactus” may be red (or a combination of colours).\nPeople with sequence-space synaesthesia associate sequences, such as numbers, days of the week, or months of the year, with particular locations in physical space. For instance, Monday may be located up and to the right, or July near the hip. Sequence-space synaesthetes can often precisely describe and point to the specific location of each element of the sequence.\nThere are also a variety of qualities associated with having synaesthesia of any type, so this dataset also includes a variable coding for having either (or both) types.\n\n\n\n\n\n\n\n\nVariable Name\nType\nDescription\n\n\n\n\nid_code\nfactor\nParticipant ID number\n\n\ngender\nfactor\nParticipant gender, 0 = female, 1 = male\n\n\ngc_score\nnumeric\nScore on the grapheme-colour test of the Synesthesia Battery (Eagleman et al., 2007). Scores of 1.43 or lower indicate genuine synaesthesia (Rothen et al., 2013)\n\n\nsyn\nfactor\nWhether the participant is a synaesthete (Yes) or not (No), regardless of type of synaesthesia\n\n\nsyn_graph_col\nfactor\nWhether the participant has grapheme-colour synaethesia (Yes) or not (No)\n\n\nsyn_seq_space\nfactor\nWhether the participant has sequence-space synaethesia (Yes) or not (No)\n\n\nscsq_imagery\nnumeric\nMean score on the Imagery Abilitysubscale of the SCSQ\n\n\nscsq_techspace\nnumeric\nMean score on the Technical/Spatial subscale of the SCSQ\n\n\nscsq_language\nnumeric\nMean score on the Language and Word Forms subscale of the SCSQ\n\n\nscsq_organise\nnumeric\nMean score on the Organisation subscale of the SCSQ\n\n\nscsq_global\nnumeric\nMean score on the Global Bias subscale of the SCSQ\n\n\nscsq_system\nnumeric\nMean score on the Systemising Tendency subscale of the SCSQ"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#viewing",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#viewing",
    "title": "03: Datasets",
    "section": "Viewing",
    "text": "Viewing\nWe’ve now got some data to work with! Before we jump into doing anything with it, though, we should take a look at it. This is always a good idea to check that our data has read in correctly without any parsing errors. But our data is tucked away in an object! How can we take a look at it?\n\nCall the Object\nOur first option is to call the object that contains our data. This is almost always an easy and straightforward way to get an instant look at what’s in our data.\n\n\n\n\n\n\nExercise\n\n\n\nCall the syn_data object to see what it contains.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data\n\n\n\n  \n\n\n\n\n\n\n\n\nYou may notice a few of those “bells and whistles” I mentioned earlier here.\n\nBy default, tibbles like this one only print out up to the first ten rows at a time, and as many columns as conveniently fit in your current window size.\nYou can scroll through this printout by clicking the numbers at the bottom (to move through rows) or the left and right arrows at the top (to scroll through columns).\nEach column has a little tag underneath it to tell you what kind of data is currently stored in it, for example &lt;dbl&gt; for numeric/double and &lt;chr&gt; for character.\nIn the top left, the little box tells you what it is (“A tibble”) and the size of the dataset (“1211 x 12”).\n\n\n\n\n\n\n\nWarning\n\n\n\nThere’s a big caveat here: this works great with tibbles. For data stored in other formats, like matrices, there’s no preset formatting like this. If you accidentally call the name of an object that contains thousands of rows, R will try to print them all, which can lead to crashes. So, avoid calling very large objects directly like this if they aren’t tibbles.\n\n\n\n\nA Glimpse of the Data\nAs mentioned above, just calling the dataset isn’t always super helpful - it depends on the size of your screen and even the width of your current window! As the next step, let’s get an overview of this dataset using the glimpse() function from the {dplyr} package.\n\n\n\n\n\n\nExercise\n\n\n\nUse dplyr::glimpse() to get a glimpse of your dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndplyr::glimpse(syn_data)\n\nRows: 1,211\nColumns: 12\n$ id_code        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ gender         &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ gc_score       &lt;dbl&gt; NA, NA, NA, NA, 1.40, 1.34, 1.30, 1.19, 1.03, 1.02, 1.0…\n$ syn            &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\",…\n$ syn_graph_col  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n$ syn_seq_space  &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"…\n$ scsq_imagery   &lt;dbl&gt; 4.88, 4.82, 3.59, 4.71, 4.65, 4.06, 4.71, 4.47, 4.12, 3…\n$ scsq_techspace &lt;dbl&gt; 2.06, 2.18, 3.35, 2.82, 3.47, 2.88, 2.76, 2.76, 2.00, 2…\n$ scsq_language  &lt;dbl&gt; 3.33, 5.00, 2.83, 4.67, 4.83, 4.67, 3.67, 3.67, 4.00, 4…\n$ scsq_organise  &lt;dbl&gt; 2.67, 4.00, 2.33, 3.33, 2.00, 4.00, 2.83, 1.83, 3.33, 3…\n$ scsq_global    &lt;dbl&gt; 1.50, 1.13, 2.13, 2.25, 2.50, 4.13, 3.00, 2.50, 3.88, 1…\n$ scsq_system    &lt;dbl&gt; 2.00, 4.83, 3.50, 3.83, 4.50, 2.00, 2.83, 2.33, 1.50, 2…\n\n\n\n\n\n\n\nThis gives us a nice overview of all the variables we have in the data (each preceded by $, which we’ll come back to in the second half); what kind of data they are (e.g. &lt;chr&gt;, &lt;dbl&gt;, and so on); and a look at the first few values in each variable. This is a great way to check that all the variables you expect to be there are there, and that they contain (more or less) what you thought they should.\nBut what if we really want to get a look at the entire dataset? For that we need…\n\n\nView Mode\nWe can have a look at the whole dataset more easily - and interact with it to some degree - by viewing it, which opens a copy of the dataset in the Source window to look through. We can do this with the View() function (note the capital “V”!).\n\n\n\n\n\n\nExercise\n\n\n\nOpen the syn_data dataset using the View() function in the Console.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nView(syn_data)\n\n\n\n\n\n\nThis View mode has a few very handy features. Take a moment now to explore and work out how to do the following.\n\n\n\n\n\n\nExercise\n\n\n\nUsing only View mode, figure out the following:\n\nWhat is the range of the variable gc_score?\nHow can you arrange the dataset by score in scsq_imagery?\nHow many participants had “Yes” in the variable syn_graph_col?\nWhich gender category had more participants?\nOf the participants who said “Yes” to syn_seq_space, what was the highest SCSQ technical-spatial score?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHover your mouse over the variable label gc_score to see a tooltip reporting the range.\nUse the small up/down arrows next to each variable label to reorder the dataset by the values in that variable.\nClick on the “Filter” button in the top left to open filter view. Then, click on the text box under syn_graph_col and type “Yes”. You can now see a dataset with only the Yes responses.\nIn filter view, click on the text box under gender to open a histogram of the values in this variable.\nIn filter view, click on the text box under syn_seq_space and type “Yes”. Then, use the up and down buttons to arrange in descending order for the variable scsq_techspace.\n\n\n\n\n\n\nThese features are really useful to have a quick poke around the data or check that everything is in order. However, keep in mind an important point: None of the changes made in View mode affect the data. View mode is essentially read-only; there’s no way to actually change the dataset or extract the values (like the range or max value) outside of copying them down by hand2. We’ll have to use R to work with the data in order to do that.\n\n\n\n\n\n\nNo Touching\n\n\n\nIf you are used to programmes like SPSS or Excel, where you can directly edit or work with the data in the spreadsheet, switching to R can be quite a frustrating change. Even though View mode looks similar, it’s like the dataset is behind glass - you can’t affect it directly. As we start working with the data via objects and functions, it may feel a bit like you are trying to work blindfolded - you can’t actually “see” what you are doing as you do it.\nIf you feel that way, be reassured that it’s normal. Working with objects rather than with spreadsheets or data directly takes some getting used to, and it will get easier with practice. Use View() freely to check your work - I do!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overall-summaries",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overall-summaries",
    "title": "03: Datasets",
    "section": "Overall Summaries",
    "text": "Overall Summaries\nWe’ve now gained some confidence that our data looks like data should. We got a look at some summary information in View mode, but although this might have been useful for us in our initial checks, we can’t easily record or reproduce that information. Next, we’re going to look at some options for getting summary information about the whole dataset.\n\nBasic Summary\nThe quickest and easiest check for a whole dataset is the base R function summary(). This function doesn’t do anything fancy (at all) but it does give you a very quick look at how all the variables have been read in, and an early indication if there’s anything exciting wonky going on.\n\n\n\n\n\n\nExercise\n\n\n\nPrint out a summary of syn_data using the summary() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(syn_data)\n\n    id_code           gender          gc_score          syn           \n Min.   :   1.0   Min.   :0.0000   Min.   :0.3500   Length:1211       \n 1st Qu.: 303.5   1st Qu.:0.0000   1st Qu.:0.5600   Class :character  \n Median : 606.0   Median :0.0000   Median :0.7200   Mode  :character  \n Mean   : 606.0   Mean   :0.1982   Mean   :0.7558                     \n 3rd Qu.: 908.5   3rd Qu.:0.0000   3rd Qu.:0.8750                     \n Max.   :1211.0   Max.   :1.0000   Max.   :1.4000                     \n                                   NA's   :1168                       \n syn_graph_col      syn_seq_space       scsq_imagery   scsq_techspace\n Length:1211        Length:1211        Min.   :1.240   Min.   :1.24  \n Class :character   Class :character   1st Qu.:3.410   1st Qu.:2.41  \n Mode  :character   Mode  :character   Median :3.760   Median :2.82  \n                                       Mean   :3.715   Mean   :2.86  \n                                       3rd Qu.:4.060   3rd Qu.:3.29  \n                                       Max.   :5.000   Max.   :4.88  \n                                                                     \n scsq_language   scsq_organise    scsq_global     scsq_system   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:2.670   1st Qu.:2.500   1st Qu.:2.170  \n Median :3.670   Median :3.170   Median :2.880   Median :2.670  \n Mean   :3.581   Mean   :3.109   Mean   :2.939   Mean   :2.672  \n 3rd Qu.:4.170   3rd Qu.:3.670   3rd Qu.:3.380   3rd Qu.:3.170  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :4.830  \n                                                                \n\n\n\n\n\n\n\nHere, for example, notice the gender variable. This is intended to be a categorical variable, but clearly something has gone pear-shaped, because it has read in as a numeric variable. We have a related, but different issue with the syn_* variables, which also should be categorical (“Yes” and “No”), but instead have been read as numeric. Our other variables, gc_score and the scsq variables, should contain numeric information and it appears they do; for them, we get some helpful information and measures of central tendancy.\nWe will ignore the categorical issue for now until we cover how to make changes to the dataset in the next tutorial.\nsummary() is quick, and because it’s a base-R function, it doesn’t need any package installations to work. However, it’s also of limited use: its output is ugly, and it would be pretty difficult to get any of those values out of that output for reporting!\n\n\nOther Summaries\nBesides the basic summary, there are many ready-made options in various packages to quickly produce summary tables. At the UG level, students are introduced datawizard::describe_distribution(), which is one such function. To use it, simply put the name of the dataset object inside the brackets.\n\n\n\n\n\n\nTip\n\n\n\nBesides its default settings, the output can be further customised to add or remove particular statistics; see the help documentation.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nPrint out a summary of syn_data using the datawizard::describe_distribution() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndatawizard::describe_distribution(syn_data)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: There are some variables missing from this output. What are they? Why aren’t they included?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe two character variables, syn_seq_space and syn_graph_col, are missing from the output. Under Details, the help documentation says: “If x is a data frame, only numeric variables are kept and will be displayed in the summary.” Since these are not numeric variables, they’ve been dropped."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#the-pipe",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#the-pipe",
    "title": "03: Datasets",
    "section": "The Pipe",
    "text": "The Pipe\nBefore we go on, we’re going to meet a new operator that will form the core of our coding style from this point onwards: the pipe. We’ll begin working with it a bit today, so let’s first explore why it’s so useful.\nIn this and the previous tutorial, we’ve seen some examples of “nested” code - functions nested within functions, as below.\nround(mean(quiz_9am), digits = 2)\nTo read this code, you have to start at the innermost level of nesting and work outwards. So, first R gets the quiz_9am object; then calculates the mean using mean(); then the output of mean() is the input to round(). For one or two levels of nesting, this is still legible, but can quickly become very difficult to track.\nOne solution is to use the pipe operator, |&gt;. The pipe “chains” commands one after the other by taking the output of the preceding command and “piping it into” the next command, allowing a much more natural and readable sequence of steps - sequentially, rather than nested. The pipe version of the above might look like this:\nquiz_9am |&gt; \n  mean() |&gt; \n  round(digits = 2)\nThis style maps on a lot more naturally to how we would read or understand the steps in this command in natural language.\n\n\n\n\n\n\nDefinition: Pipe\n\n\n\nThe pipe operator may appear in two formats.\n\nThe native pipe, |&gt;. This is the pipe we will use throughout these tutorials. It is called the “native” pipe because it is inbuilt into R and doesn’t require any specific package to use.\nThe {magrittr} pipe, %&gt;%. This pipe comes from {tidyverse}, and in particular requires the {magrittr} package to use. You will very commonly see this pipe in scripts, Stack Overflow posts, from ChatGPT, etc. as until the native pipe was introduced to R in 2022, the {magrittr} pipe was “the pipe” for R.\n\nIn most cases, including almost all of the code we will learn in these tutorials, the two pipes are interchangeable and will result in the same output.\n\n\nConceptually, the pipe works taking the output of the code on the left and passing it, by default, into the first unnamed argument of whatever comes after it on the right3. Many functions - both from the {tidyverse} and not - are already set up so that the first argument is the data, and {tidyverse} functions are explicitly designed this way in order to work best with the pipe.\nFor functions where this is not the case, you can determine where the piped-in information should go using a “placeholder”. The most noticeable difference between the two pipes is that they have different placeholders. The native pipe (|&gt;) uses underscore _ as its placeholder, while the magrittr pipe (%&gt;%) uses the dot/full stop . There’s an example using placeholders below to help make this clearer.\nFrom this point forward, we’ll start working with the native pipe. The following sections will have specific examples on using the pipe to practice. Working with the pipe is also a good chance to practice “translating” R code into English, which we’ll do as we go. To give you an idea, here’s an example of how we can read the following code. We’ll typically read |&gt; as “and then”, taking the output of whatever the preceding line produces and passing it on to the next line.\n\n1some_numbers &lt;- c(3.52, 7.03, 9.2, 10.11, 2)\n\n2some_numbers |&gt;\n3  mean() |&gt;\n4  round(2)\n\n\n1\n\nCreate a new object, some_numbers, that contains a vector of some numbers.\n\n2\n\nTake some_numbers, and then…\n\n3\n\nCalculate the mean of those numbers, and then…\n\n4\n\nRound that mean to two decimal places, which outputs:\n\n\n\n\n[1] 6.37\n\n\nIt might seem strange that these functions appear to be “empty”. Take the third step above, which just reads mean(). There’s nothing in the brackets, so it appears that the mean() function is working on nothing! However, remember that the pipe passes whatever is on its left-hand side (abbreviated LHS) to the first unnamed argument of the function on its right-hand side (RHS). In this example, the object some_numbers is the input on the LHS, which is being “piped into” the first unnamed argument of the mean() function on the RHS, which is x, the data or values to work with. We don’t have to explicitly specify this; it’s just how the pipe works by default.\nTo illustrate this more clearly, here’s the same code explicitly including the placeholders to indicate where the piped-in information goes:\n\nsome_numbers &lt;- c(3.52, 7.03, 9.2, 10.11, 2)\n\nsome_numbers |&gt;\n  mean(x = _) |&gt;\n  round(x = _, 2)\n\nThink of the placeholder like a bucket or landing pad, where the information coming in from the pipe falls. In this example, the placeholders aren’t necessary, because at each step, we already want the piped-in information to go into the first argument, which we have left unnamed, and which is conveniently, in both cases, x, the data or values to work with; but including the placeholders (and, necessarily if we want to use placeholders, the argument names) helps to see what’s going on.\nTo make this fully explicit, this illustration shows the progress of information through the same pipe. The orange arrows show how the piped-in information is passed from one function to the next. The teal arrows show what is produced at each step of the pipe - which is what is passed on to the next function via the orange arrows/pipe.\n\n\n\n\n\n\n\nA Sweet Pipe Example\n\n\n\n\n\nImagine we wanted to bake a Victoria sponge cake using R. Translating the steps into R, we might get something like this:\ningredients |&gt; \n  mix(order = c(\"wet\", \"dry\")) |&gt; \n  pour(shape = \"round\", number = 2, lining = TRUE) |&gt; \n  bake(temp = 190, time = 20) |&gt; \n  cool() |&gt; \n  assemble(filling = c(\"buttercream\", \"jam\"), topping = \"icing_sugar\") |&gt; \n  devour()\nAt each step, |&gt; takes whatever the previous step produces and passes it on to the next step. So, we begin with ingredients - presumably an object that contains our flour, sugar, eggs, etc - which is “piped into” the mix() function. The output of that function might be all our ingredients mixed together in a bowl, which is then piped into the pour() function, and so on.\nNotice for example, the function cool(), which doesn’t appear to have anything in it. It actually does: the cool() function would work with whatever the output of the bake() function was above it: a freshly baked cake straight out of the oven.\nWithout the pipe, our command might look something like this, which must be read from the inside out rather from top to bottom:\ndevour(\n  assemble(\n      cool(\n        bake(\n          pour(\n            mix(ingredients, \n                order = c(\"wet\", \"dry\")),\n            shape = \"round\", number = 2, lining = TRUE),\n          temp = 190, time = 20)\n      ),\n    filling = c(\"buttercream\", \"jam\"), topping = \"icing_sugar\"\n  )\n)\nThis is, I am sure you will agree, as absolutely horrifying as a soggy bottom on a cake."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-datasets",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-datasets",
    "title": "03: Datasets",
    "section": "Describing Datasets",
    "text": "Describing Datasets\nTo start, we’ll work again with the whole dataset and look at some helpful functions that are often important for validating our data processing.\n\nnrow(): Returns the number of rows as a numeric value.\nncol(): Returns the number of columns as a numeric value.\nnames(): Returns a character vector of the names of the columns of a dataset (and also the names of elements for other types of input).\n\nIf your dataset is structured like this one is - with a single participant per row - then nrow() is a common stand-in for counting participants.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the native pipe, print out the number of columns and the names of those columns in the syn_data dataset.\nHint: This will be two separate commands!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data |&gt; \n  ncol()\n\n[1] 12\n\nsyn_data |&gt; \n  names()\n\n [1] \"id_code\"        \"gender\"         \"gc_score\"       \"syn\"           \n [5] \"syn_graph_col\"  \"syn_seq_space\"  \"scsq_imagery\"   \"scsq_techspace\"\n [9] \"scsq_language\"  \"scsq_organise\"  \"scsq_global\"    \"scsq_system\"   \n\n\nThe new line after the pipe isn’t essential (it will run exactly the same way) but it is highly recommended. Although it doesn’t make much of a difference here, we will shortly get to longer commands where the new line for each new function will make a big difference to legibility!\n\n\n\n\n\n\n\n\n\n\n\nWhat’s Going On With the Pipe?\n\n\n\n\n\nIf a command like syn_data |&gt; names() looks a bit strange, let’s take a closer look at it.\nThis command is equivalent to names(syn_data), which might look a bit more familiar based on what we’ve done so far. The pipe takes whatever comes before it - in this case, the dataset syn_data - and pipes it into the first argument of the function that comes after it. The names() function only accepts one object as input, so syn_data is passed to names() as that single object. It looks like the names() function is empty, because there’s nothing in the brackets, but that’s because the dataset is being “piped in” from above.\nWe can make this a bit more explicit using the placeholder:\n\nsyn_data |&gt; \n  names(x = _)\n\n [1] \"id_code\"        \"gender\"         \"gc_score\"       \"syn\"           \n [5] \"syn_graph_col\"  \"syn_seq_space\"  \"scsq_imagery\"   \"scsq_techspace\"\n [9] \"scsq_language\"  \"scsq_organise\"  \"scsq_global\"    \"scsq_system\"   \n\n\nThe underscore is the “placeholder” for the native pipe; in other words, it explicitly indicates where the object should be placed that is being piped in, like a “bucket” that catches whatever comes out of the pipe! This makes it a bit clearer to see that the object syn_data is going into the names() function, and specifically the x argument.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUsing the native pipe, save the number of participants in the syn_data dataset in a new object of your choice.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npx_initial &lt;- syn_data |&gt; \n  nrow()\n\nThis format takes a bit of getting used to. The new object, which I’ve called px_initial4, is created at the first line of the command by the &lt;-. However, this object contains whatever the final output of this pipe is at the end - in this case, the number of rows as a numeric value produced by nrow()."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-variables",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-variables",
    "title": "03: Datasets",
    "section": "Describing Variables",
    "text": "Describing Variables\nOnce we’ve had a look at the whole dataset, it’s time to drill down into individual variables. We may want to calculate quick descriptives or investigate what’s going on with particular variables that seem to have issues.\nTo do this, we’ll start working quite a bit with the {dplyr} package. {dplyr} is a core part of the {tidyverse}, and the package generally is focused on user-friendly and easily readable tools for data manipulation. The essential {dplyr} functions will form a core part of the Essentials part of the course, when we really get into to working with data.\n\nCounting\nWe’ll start by having a look at character or categorical variables. Here we’ll meet our first {dplyr} function: dplyr::count(). This function is a friendly way to obtain (as you might expect!) counts of the number of times each unique value appears in a variable. As with just about everything in {dplyr}, it takes a tibble as input and produces a new tibble as output.\nUsing the pipe structure we’ve seen previously, the general form is:\ndataset_name |&gt; \n  dplyr::count(variable_to_count, optionally_another, ...)\nMinimally you need to provide a single variable to count the values in, but you can add more, separated by commas, to further subdivide the counts.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the syn_data dataset, produce a tibble of counts of how many participants had any kind of synaesthesia. Then, produce a second tibble, adding in gender as well.\nHint: Use the codebook to find the variables to use.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data |&gt; \n  dplyr::count(syn)\n\n\n\n  \n\n\nsyn_data |&gt; \n  dplyr::count(syn, gender)\n\n\n\n  \n\n\n\nAs you can see, the output from this function is a new summary tibble containing only the unique values in each variable, and a count, in the new “n” variable, of how many times that value (or combination of values) appeared.\nNote that this does not change or add anything to your original dataset! Instead, this function creates a brand-new tibble with the requested information.\n\n\n\n\n\n\n\nSubsetting\nTo work with individual variables, we need to get them out of the dataset. Specifically, for many of the functions we’re about to use, we will need the values stored in those variables as vectors. We can do this in two ways: $ notation, or the function dplyr::pull()5.\nSubsetting with $ is the base-R method, and it takes the general form:\ndataset_name$variable_to_subset\nSubsetting with dplyr::pull() is a {tidyverse} method of accomplishing the same thing. Using the pipe structure we’ve seen previously, the general form is:\ndataset_name |&gt; \n  dplyr::pull(variable_to_subset)\n\n\n\n\n\n\nExercise\n\n\n\nSubset syn_data using $ to get out all the values stored in the scsq_organise variable.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo keep this tutorial legible, I’ve only printed out the first 10 values.\n\nsyn_data$scsq_organise\n\n [1] 2.67 4.00 2.33 3.33 2.00 4.00 2.83 1.83 3.33 3.17\n [ reached getOption(\"max.print\") -- omitted 1201 entries ]\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSubset syn_data using dplyr::pull() to get out all the values stored in the gc_score variable. How would you read this code?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAgain, I’ve only printed out the first 10 values.\n\nsyn_data |&gt; \n  dplyr::pull(gc_score)\n\n [1]   NA   NA   NA   NA 1.40 1.34 1.30 1.19 1.03 1.02\n [ reached getOption(\"max.print\") -- omitted 1201 entries ]\n\n\nYour exact translation may vary, but one option is:\n\nTake the syn_data dataset, and then pull out all the values in the gc_score variable.\n\n\n\n\n\n\nIf you’re wondering when to use $ and when to use dplyr::pull(), the answer depends on what you want to do! We’ll see some examples of both in just a moment.\n\n\nDescriptives\nNext up, we can start working with these values in the dataset. The base-R {stats} package contains a wide variety of very sensibly-named functions that calculate common descriptive statistics. These include:\n\nmean() and median() (there is a function mode(), but it doesn’t do what we’d like it to here!)\nmin() for minimum value, max() for maximum value\nrange() for both minimum and maximum value in a single vector\nsd() for standard deviation\n\nA key feature of all of these functions is that, by default, they return NA if there are any NAs (missing values) present. (This is very sensible behaviour by default, but is frequently not the information we want when we use them.) So, they all have an argument, na.rm =, which determines whether NAs should be removed. By default this argument is set to FALSE (NAs should NOT be removed), but if you want to get the calculation ignoring any NAs, you can set this argument to TRUE instead.\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the mean, standard deviation, and median of the SCSQ global subscale, and the range of the grapheme-colour synaesthesia score.\nTry using each subsetting method at least once.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe’ll start by using $ subsetting for the first bit, and pull() for the second. Not for any principled reason - feel free to try it either way.\n\nmean(syn_data$scsq_global)\n\n[1] 2.939389\n\nsd(syn_data$scsq_global)\n\n[1] 0.6011931\n\nmedian(syn_data$scsq_global)\n\n[1] 2.88\n\n\nThe gc_score variable has a large number of NAs. If we use the range() function without any other changes, we’ll just get NAs back as output. To remove NAs and work only with the non-missing values, we have to include the argument na.rm = TRUE.\n\nsyn_data |&gt; \n  dplyr::pull(gc_score) |&gt; \n  range(na.rm = TRUE)\n\n[1] 0.35 1.40\n\n\n\n\n\n\n\nAs a quick check to get an individual number, this method is quite useful. However, you may have noticed that if we wanted this information for lots of variables, this would be repetitive, laborious, and prone to error. We’ve already seen how to use existing summary functions, but we will also look at creating custom summary tables in a future tutorial.\n\n\nVisualisations\nThe final piece we will look at today will be base-R data visualisations in the {graphics} package. These built-in graphics functions are particularly helpful for quick spot checks during data cleaning and manipulation. For high-quality, fully customisable, professional data visualisations, we will use the {ggplot2} package, covered in depth in the next section of the course.\nTo get the idea, there are a few options for common plots:\n\nhist() for histograms\nboxplot() and barplot()\nplot() for scatterplots\n\nFor more help and examples with base R graphics, try this quick guide.\n\n\n\n\n\n\nExercise\n\n\n\nTry making a histogram and a boxplot, using any of the variables in the syn_data dataset. Try using $ and pull() once each.\nOptionally, if you feel so inclined, use the help documentation to spruce up your plots a bit, such as changing the title and axis labels.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHistogram:\n\nhist(syn_data$scsq_language)\nhist(syn_data$scsq_language,\n     main = \"Histogram of the Language subscale of the SCSQ\",\n     xlab = \"SCSQ Language score\")\n\n\n\n\n\n\n\n\n\n\n\nBoxplot:\n\nsyn_data |&gt; \n  dplyr::pull(gc_score) |&gt; \n  boxplot()\nsyn_data |&gt; \n  dplyr::pull(gc_score) |&gt; \n  boxplot(\n     main = \"Boxplot of grapheme-colour score\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Try making a barplot and a scatterplot.\nFor the barplot, make a visualisation of how many people are synaesthetes or not (regardless of synaesthesia type).\nFor the scatterplot, choose any two SCSQ measures.\nBoth of these require some creative problem-solving using the help documentation and the skills and functions covered in this tutorial.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe following solutions are options - if you found another way to make the same or similar plots, well done!\nBarplots require two sets of values: a categorical one on the horizontal x axis and a continuous one on the vertical y axis. For something like frequency counts, then, we have to first do the counting, then pass those counts onto barplot(). Luckily we already know how to count categorical variables.\nThe help documentation is most helpful in the Examples section, where it shows actual examples of how the function works. There we can see an example of the formula method, y ~ x, which I’ve used below. Since we’re piping in the data to an argument that is not the first, I’ve used the placeholder in the data = _ argument to finish the command.\n\nsyn_data |&gt;\n  dplyr::count(syn) |&gt;\n  barplot(\n    n ~ syn,\n    data = _)\n\n\n\n\n\n\n\n\n\nFor the scatterplot, there are a couple of options. We can either supply x and y separately using $ subsetting, or use the same y ~ x formula we saw for barplots previously.\n\n## Using subsetting\nplot(syn_data$scsq_techspace, syn_data$scsq_imagery)\n## Using a formula\nsyn_data |&gt; \n  plot(scsq_imagery ~ scsq_techspace, data = _)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nWell Done!\n\n\n\nThat’s the end of this tutorial. Very well done on all your hard work!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#footnotes",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#footnotes",
    "title": "03: Datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can read this notation as “the filter function from the dplyr package”, or just “dplyr filter”. As for how to pronounce “dplyr”, the official pronunciation is “dee-ply-er”, with “plier” like the tool for which it’s named. I have heard other people say “dipler”. Since code is always a bit tricky to read aloud, just go with whatever sounds good to you.↩︎\n[](https://media.giphy.com/media/12XMGIWtrHBl5e/giphy.gif, fig-alt=‘A gif of Michael from the Office (US) shouting “Oh God no! Please no! NOOOOO”’)↩︎\nRemember last week I mentioned that “first unnamed arguments” would be important? Here’s why! Look back on 01/02 IntRoductions if you’d like a refresher.↩︎\nYou can call this object anything you like; I use “px” as shorthand for “participant.”↩︎\nThis function always makes me think of one of those arcade claw machines reaching into the dataset to grab the values you want!↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html",
    "title": "05: Filter and Select",
    "section": "",
    "text": "This tutorial covers two important {dplyr} functions: filter() and select(). Easy to confuse, filter() uses logical assertions to return a subset of rows (cases) in a dataset, while select() returns a subset of the columns (variables) in the dataset.\n\n\n\n\n\n\nTip\n\n\n\nTo remember which does which:\n\nfilter() works on rows, which starts with “r”, so it contains the letter “r”.\nselect() works on columns, which starts with “c”, so it contains the letter “c”."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#overview",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#overview",
    "title": "05: Filter and Select",
    "section": "",
    "text": "This tutorial covers two important {dplyr} functions: filter() and select(). Easy to confuse, filter() uses logical assertions to return a subset of rows (cases) in a dataset, while select() returns a subset of the columns (variables) in the dataset.\n\n\n\n\n\n\nTip\n\n\n\nTo remember which does which:\n\nfilter() works on rows, which starts with “r”, so it contains the letter “r”.\nselect() works on columns, which starts with “c”, so it contains the letter “c”."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#setup",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#setup",
    "title": "05: Filter and Select",
    "section": "Setup",
    "text": "Setup\n\nPackages\nWe will be focusing on {dplyr} today, which contains both the filter() and select() functions. You can either load {dplyr} alone, or all of {tidyverse} - it won’t make a difference, but you only need one or the other.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the necessary packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(dplyr)\n## OR\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\nData\nToday we’re going to start working with a dataset that we’re going to get familiar with over the next few weeks. Courtesy of fantastic Sussex colleague Jenny Terry, this dataset contains real data about statistics and maths anxiety.\n\n\n\n\n\n\nExercise\n\n\n\nRead in the dataset and save it in a new object, anx_data.\nOn the Cloud, you can read in this dataset from the data folder using here::here().\nElsewhere, you can download the dataset, or copy the dataset URL, from the Data and Workbooks page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRead in from file:\n\nanx_data &lt;- readr::read_csv(here::here(\"data/anx_data.csv\"))\n\nRead in from URL:\n\nanx_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/anx_data.csv\")\n\n\n\n\n\n\n\nCodebook\nThere’s quite a bit in this dataset, so you will need to refer to the codebook below for a description of all the variables.\nThis study explored the difference between maths and statistics anxiety, widely assumed to be different constructs. Participants completed the Statistics Anxiety Rating Scale (STARS) and Maths Anxiety Rating Scale - Revised (R-MARS), as well as modified versions, the STARS-M and R-MARS-S. In the modified versions of the scales, references to statistics and maths were swapped; for example, the STARS item “Studying for an examination in a statistics course” became the STARS-M item “Studying for an examination in a maths course”; and the R-MARS item “Walking into a maths class” because the R-MARS-S item “Walking into a statistics class”.\nParticipants also completed the State-Trait Inventory for Cognitive and Somatic Anxiety (STICSA). They completed the state anxiety items twice: once before, and once after, answering a set of five MCQ questions. These MCQ questions were either about maths, or about statistics; each participant only saw one of the two MCQ conditions.\n\n\n\n\n\n\nImportant\n\n\n\nFor learning purposes, I’ve randomly generated some additional variables to add to the dataset containing info on distribution channel, consent, gender, and age. Especially for the consent variable, don’t worry: all the participants in this dataset did consent to the original study. I’ve simulated and added this variable in later to practice removing participants.\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nid\nCategorical\nUnique ID code\n\n\ndistribution\nCategorical\nChannel through which the study was completed, either \"preview\" or \"anonymous\" (the latter representing \"real\" data). Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nconsent\nCategorical\nWhether the participant read and consented to participate (\"Yes\") or not (\"No\"). Note that this variable has been randomly generated and does NOT reflect genuine responses; all participants in this dataset did originally consent to participate.\n\n\ngender\nCategorical\nGender identity, one of \"female\", \"male\", \"non-binary\", or \"other/pnts\". \"pnts\" is an abbreviation for \"Prefer not to say\". Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nage\nNumeric\nAge in years. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nmcq\nCategorical\nIndependent variable for MCQ question condition, whether the participant saw MCQ questions about mathematics (\"maths\") or statistics (\"stats\").\n\n\nstars_[sub][number]\nNumeric\nItem on the Statistics Anxiety Rating Scale. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [help]: Asking for Help\n- [int]: Interpretation Anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (no anxiety) to 5 (a great deal of anxiety), so higher scores reflect higher levels of anxiety.\n\n\nstars_m_[sub][number]\nNumeric\nItem on the Statistics Anxiety Rating Scale - Maths, a modified version of the STARS with all references to statistics replaced with maths. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [help]: Asking for Help\n- [int]: Interpretation Anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (no anxiety) to 5 (a great deal of anxiety), so higher scores reflect higher levels of anxiety.\n\n\nrmars_[sub][number]\nNumeric\nItem on the Revised Maths Anxiety Rating Scale. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [num]: Numerical Task Anxiety\n- [course]: Course anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 5 (very much), so higher scores reflect higher levels of anxiety.\n\n\nrmars_s_[sub][number]\nNumeric\nItem on the Revised Maths Anxiety Rating Scale - Statistics, a modified version of the MARS with all references to maths replaced with statistics. There are three subscales, denoted with [sub] in the name:\n- [test]: Test anxiety\n- [num]: Numerical Task Anxiety\n- [course]: Course anxiety.\n[num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 5 (very much), so higher scores reflect higher levels of anxiety.\n\n\nsticsa_trait_[number]\nNumeric\nItem on the State-Trait Inventory for Cognitive and Somatic Anxiety, Trait subscale. [num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 4 (very much so), so higher scores reflect higher levels of anxiety.\n\n\nsticsa_[time]_state_[number]\nNumeric\nItem on the State-Trait Inventory for Cognitive and Somatic Anxiety, State subscale. [time] denotes one of two times of administration: before completing the MCQ task (\"pre\"), or after (\"post\"). [num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 4 (very much so), so higher scores reflect higher levels of anxiety.\n\n\nmcq_stats_[num]\nCategorical\nCorrect (1) or incorrect (0) response to MCQ questions about statistics, covering mean ([number] = 1), standard deviation (2), confidence intervals (3), beta coefficient (4), and standard error (5).\n\n\nmcq_maths_[num]\nCategorical\nCorrect (1) or incorrect (0) response to MCQ questions about maths, covering mean ([number] = 1), standard deviation (2), confidence intervals (3), beta coefficient (4), and standard error (5)."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#filter",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#filter",
    "title": "05: Filter and Select",
    "section": "Filter",
    "text": "Filter\nThe filter() function’s primary job is to easily and transparently subset the rows within a dataset - in particular, a tibble. filter() takes one or more logical assertions and returns only the rows for which the assertion is TRUE. Columns are not affected by filter(), only rows.\n\nGeneral Format\n\n1dataset_name |&gt;\n2  dplyr::filter\n3    logical_assertion\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nA logical assertion about the variable(s) in dataset_name that returns logical (TRUE or FALSE) values.\n\n\n\n\n\n\nFiltering with Assertions\nThe logical_assertion in the general format above is just like the assertions we saw in the first tutorial. The rows where the assertion returns TRUE will be included in the output; those that return FALSE will not. Inside the filter() command, use the names of the variable in the piped-in dataset to create the logical assertions.\nAs a first example, let’s use some of our familiar operators from the first tutorial. To retain only people who completed the maths MCQs, we can run:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    mcq == \"maths\"\n    )\n\n\n1\n\nTake the dataset anx_data, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the mcq variable is exactly and only equal to \"maths\".\n\n\n\n\nSo, the tibble we get as output contains cases that have the value \"maths\", and NOT \"stats\", nor any NAs (because NA does not equal \"maths\"!).\n\n\n\n\n\n\nError Watch: Detected a named input\n\n\n\n\n\nRemember that for exact matches like this, we must use double-equals == and not single-equals =. If you use single equals, you’re not alone - this is such a common thing that the (incredibly friendly and helpful) error message tells you what to do to fix it!\n\nanx_data |&gt; \n  dplyr::filter(mcq = \"maths\")\n\nError in `dplyr::filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `mcq == \"maths\"`?\n\n\n\n\n\nNaturally, we can also filter on numeric values. If we wanted to keep only participants younger than 40 years old, we can filter as follows:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    age &lt; 40\n    )\n\n\n1\n\nTake the dataset anx_data, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the age variable is less than 40.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProduce a subset of anx_data that doesn’t contain any male participants.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::filter(\n    gender != \"male\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProduce a subset of anx_data that contains only participants as old as the median age, or younger.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere we can take advantage of the fact that we can use variable names as objects inside {dplyr} functions like filter()1. Then we write a logical assertion just like we have done in previous tutorials.\n\nanx_data |&gt; \n  dplyr::filter(\n    age &lt;= median(age, na.rm = TRUE)\n  )\n\nIf you mysteriously got an empty tibble, you may have missed out the na.rm = TRUE argument to median().\n\n\n\n\n\nAs a final example, let’s consider a situation where we want to retain only participants that gave a gender identity of either “male” or “female”.2\nTo do this, we need a new operator: %in%, which God knows I just pronounce as “in” (try saying “percent-in-percent” three times fast!). This looks for any matches with any of the elements that come after it:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    gender %in% c(\"female\", \"male\")\n    )\n\n\n1\n\nTake the dataset anx_dat, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the gender variable matches any of the values “female” or “male”.\n\n\n\n\n\n\n\n\n\n\nWhy not ==?\n\n\n\nWhat follows here is a rabbit hole that gets into some gritty detail. If you’re happy to take my word for it that you absolutely, definitely needed %in% and not == in the previous exercise, you can skip the explanation below. If you’re keen to understand all the nuance, click to expand and read on!\n\n\n\n\n\n\nThe Danger of == vs %in%\n\n\n\n\n\nFor this matching task, you might have thought we’d use gender == c(\"female\", \"male\"), which runs successfully and sure looks okay. So why isn’t this right?\n\n## DO NOT DO THIS\nanx_data |&gt; \n  ## THIS DOES NOT DO WHAT WE WANT!!\n  dplyr::filter(gender == c(\"female\", \"male\"))\n\n\n\n  \n\n\n## DANGER WILL ROBINSON\n\nAt a glance it looks like this produces the same output as the solution above - gender now contains only male or female participants. As you might have gathered from the all-caps comments above - intended to prevent you from accidentally using this code in the future for tasks like this - this is NOT what this code does.\nTo demonstrate what it does do, I need the dplyr::mutate() function from the next tutorial to create some new variables. The first new variable, double_equals, contains TRUEs and FALSEs for each case using the assertion with ==. The second is exactly the same, but reverses the order of the genders - something that should NOT make a difference to the matching! (We want either female OR male participants, regardless of which we happen to write first.) The third, in_op, contains the same again but this time with %in%. The final arrange() line sorts the dataset by gender to make the output easier to read.\n\nanx_data |&gt; \n  dplyr::mutate(\n    double_equals = (gender == c(\"female\", \"male\")),\n    double_equals_rev = (gender == c(\"male\", \"female\")),\n    in_op = (gender %in% c(\"female\", \"male\")),\n    .keep = \"used\"\n  ) |&gt; \n  dplyr::arrange(gender)\n\n\n\n  \n\n\n\nNotice anything wild?\nFor participants with the same value in gender, the assertions with == both flip between TRUE and FALSE, but in the reverse pattern to each other. The assertion with %in% correctly labels them all as TRUE. WTF?\nWhat’s happening is that because the vector c(\"female\", \"male\") contains two elements, the assertion with == matches the first case to the first element - female - and returns TRUE. Then it matches the second case to the second element - male - and this time returns FALSE. Then because there are more cases, it repeats: the next (third) case matches female and returns TRUE, the next male and FALSE, and so forth. The == assertion with the gender categories reversed does the same, but starts with male first and female second. Only %in% actually does what we wanted, which was to return TRUE for any case that matches female OR male.\nThis is a good example of what I think of as “dangerous” code. I don’t mean “reckless” or “irresponsible” - R is just doing exactly what I asked it to do, and it’s not the job of the language or package creators to make sure my code is right. I mean dangerous because it runs as expected, produces (what looks like) the right output, and even with some brief checking, would appear to contain the right cases - but would quietly result in a large chunk of the data being wrongly discarded. If you didn’t know about %in%, or how to carefully double-check your work, you could easily carry on from here and think no more about it.\nSo, how can we avoid a problem like this? Think of any coding task - especially new ones, where you’re not completely familiar with the code or functions you’re working with - as a three-step process3.\n\nAnticipate. Form a clear picture of the task you are trying to achieve with your code. What do you expect the output of the code to look like when it runs successfully?\nExecute. Develop and run the code to perform the task.\nConfirm. Compare the output to your expectations, and perform tests to confirm that what you think the code has done, is in fact what it has done.\n\nSo, what might the Confirm step look like for a situation like this?\nOne option is the code I created above, with new columns for the different assertion options - but this might be something you’d only think to do if you already knew about %in% or suspected there was a problem. A more routine check might look like:\n\nI expect that when my filtering is accomplished, my dataset will contain all and only the participants who reported a gender identy of female or male, and no others. I will also have the same number of cases as the original dataset, less the number of other gender categories.\n\nFirst, I’ll create a new dataset using the filtered data.\n\n## SERIOUSLY THIS IS BAD\nanx_data_bd &lt;- anx_data |&gt; \n## DON'T USE THIS CODE FOR MATCHING\n  dplyr::filter(gender == c(\"female\", \"male\"))\n## STOP OH GOD PLEASE JUST DON'T\n\nCheck 1: Filtered data contains only male and female participants.\n\nanx_data_bd |&gt; \n  dplyr::count(gender)\n\n\n\n  \n\n\n\nOnly female and male participants! Tick ✅\nAt this point, though, I might become suspicious. The original dataset contained 465 cases - we’ve lost more than half! Can that be right?? Better check the numbers.\n\n## Get the numbers from the original dataset\nanx_data |&gt; \n  dplyr::count(gender)\n\n\n\n  \n\n\n\nUh oh. Already we can see that something’s wrong with the numbers. But instead of relying on visual checks, let’s let R tell us.\n\n## Calculate how many cases we expect if the filtering had gone right\nexpected_n &lt;- anx_data |&gt; \n  dplyr::count(gender) |&gt; \n  ## This isn't the best way to filter\n  dplyr::filter(gender != \"non-binary\") |&gt;\n  ## The next section on multiple assertions has a much better method!\n  dplyr::filter(gender != \"other/pnts\") |&gt; \n  dplyr::pull(n) |&gt; \n  sum()\n\n## Ask R whether the expected number of rows is equal to the actual number of rows in the filtered data\nexpected_n == nrow(anx_data_bd)\n\n[1] FALSE\n\n\nNow we know for sure there’s a problem and can investigate what happened more thoroughly.\nAs a final stop on this incredibly lengthy detour (are you still here? 👋), you might wonder whether the check above would give me the wrong answer, because I used two filter()s in a row, and the whole point of this goose chase is how to accomplish that exact filtering task. First, this is NOT the way I would do this (as the comments suggest), but I’m really trying to stick to ONLY what we’ve already covered wherever possible. But let’s say I’d tried to do this with the bad == filtering that caused all this faff in the first place.\nFor this particular case there are four values in gender. If I try gender == c(\"female\", \"male\") here, this DOES actually work fine - because the categories are in the right order and are a multiple of the length of the dataset 🤦 But at least the numbers still wouldn’t match, which would tell me that something went wrong with filtering the whole dataset.\n\nanx_data |&gt; \n  dplyr::count(gender) |&gt; \n  dplyr::filter(gender == c(\"female\", \"male\"))\n\n\n\n  \n\n\n\nIf I happened to have had the genders the other way round, I would have got an empty tibble, and hopefully that also would have clued me in that there was a problem with the original filtering.\n\nanx_data |&gt; \n  dplyr::count(gender) |&gt; \n  dplyr::filter(gender == c(\"male\", \"female\"))\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMultiple Assertions\nLogical assertions can also be combined to specify exactly the cases you want to retain. The two most important operators are:\n\n& (AND): Only cases that return TRUE for all assertions will be retained.\n| (OR): Any cases that return TRUE for at least one assertion will be retained.\n\n\n\n\n\n\n\nMore on AND and OR\n\n\n\n\n\nLet’s look at a couple minimal examples to get the hang of these two symbols. For each of these, you can think of the single response R gives as the answer to the questions, “Are ALL of these assertions true?” for AND, and “Is AT LEAST ONE of these assertions true?” for OR.\nFirst, let’s start with a few straightforward logical assertions:\n\n\"apple\" == \"apple\"\n\n[1] TRUE\n\n23 &gt; 12\n\n[1] TRUE\n\n42 == \"the answer\"\n\n[1] FALSE\n\n10 &gt; 50\n\n[1] FALSE\n\n\nNext, let’s look at how they combine.\nTwo true statements, combined with &, return TRUE, because it is true that all of these assertions are true.\n\n\"apple\" == \"apple\" & 23 &gt; 12\n\n[1] TRUE\n\n\nTwo true statements, combined with |, also return TRUE, because it true that at least one of these assertions is true.\n\n\"apple\" == \"apple\" | 23 &gt; 12\n\n[1] TRUE\n\n\nTwo false statements, combined with &, return FALSE, because it is NOT true that all of them are true.\n\n42 == \"the answer\" & 10 &gt; 50\n\n[1] FALSE\n\n\nTwo false statements, combined with |, return FALSE, because it is NOT true that at least one of them is true.\n\n42 == \"the answer\" | 10 &gt; 50\n\n[1] FALSE\n\n\nOne true and one false statement, combined with &, return FALSE, because it is NOT true that all of them are true.\n\n23 &gt; 12 & 42 == \"the answer\"\n\n[1] FALSE\n\n\nOne true and one false statement, combined with |, return TRUE, because it is true that at least one of them is true.\n\n23 &gt; 12 | 42 == \"the answer\"\n\n[1] TRUE\n\n\n\n\n\nTo see how this works, let’s filter anx_data to keep only cases that saw the stats MCQs, OR that scored 3 or higher on the first STARS test subscale item.\nThis requires two separate statements, combined with | “OR”:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    mcq == \"stats\" |\n4     stars_test1 &gt;= 3\n    ) \n\n\n1\n\nTake the dataset anx_data, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the mcq variable is only and exactly equal to \"stats\", OR\n\n4\n\nThe value in stars_test1 is greater than or equal to 3.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFilter anx_data to keep only cases where the value of rmars_s_test2 is between 2 and 4.\nHint: You can use two separate assertions to do this, or check out dplyr::between().\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the first solution, we must use & “AND” to ensure that both these conditions are met simultaneously.\nFor the second solution, the dplyr::between() function does the same operation, without having to worry about getting AND vs OR right.\n\nanx_data |&gt; \n  dplyr::filter(\n    rmars_s_test2 &gt;= 2 & rmars_s_test2 &lt;= 4\n  )\n\nanx_data |&gt; \n  dplyr::filter(\n    dplyr::between(rmars_s_test2, 2, 4)\n  )\n\n\n\n\n\n\n\n\nData Cleaning\nFiltering is absolutely invaluable in the process of data cleaning. In order to practice this process, I’ve introduced some messy values into the data, so let’s have a look at a method of cleaning up the dataset and documenting our changes as we go.\n\nPre-Exclusions\nFor data collected on platforms like Qualtrics, you can frequently test out your study via a preview mode. Responses completed via preview are still recorded in Qualtrics, but labeled as such in a variable typically called “DistributionChannel” or similar. In this dataset, we have a similar variable, distribution, that labels whether the data was recorded in a preview (\"preview\") or from real participants (\"anonymous\").\nYour method may vary, but I wouldn’t bother to document these cases as “exclusions” because they aren’t real data. I would just drop them from the dataset - but of course make sure to record the code that does so!\n\n\n\n\n\n\nExercise\n\n\n\nRemove any preview runs from the dataset, keeping only real data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::filter(distribution == \"anonymous\")\n\n\n\n\n\n\n\n\nRecording Exclusions\nAs a part of complete and transparent reporting, we will want to report all of the reasons we excluded cases from our dataset, along with the number excluded. We can build this counting process into our workflow so that at the end, we have a record of each exclusion along with initial and final numbers.\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the following data cleaning steps, trying them out in a code chunk for yourself as you go. You’ll need them at the end!\n\n\nFor each check below, our recording process will have two steps:\n\nProduce a dataset of the cases you will exclude, and count the number of rows (cases).\nRemove the cases and overwrite the old dataset with the new one.\n\nIn my process, I’m going to keep anx_data as the original, “raw” version of the dataset. So, I’ll create a copy in a new dataset object to use while “processing” that I will update as I go.\n\nanx_data_proc &lt;- anx_data\n\nTo begin, we will count the initial number of cases before any exclusions.\n\nn_initial &lt;- nrow(anx_data_proc)\nn_initial\n\n[1] 453\n\n\n(Remember that we can use nrow() because there is only one participant per row. If we had long-form data with observations from the same participant across multiple rows, we would have to do something a bit different!)\n\n\nConsent\nFor many datasets, you would likely have a variable with responses from your participants about informed consent. How you filter this depends on what that variable contains, of course. However, we’ve already seen examples of this kind of operation earlier in this tutorial.\nFor the first assertion, we capture any responses that don’t match “Yes”, but for the second, we need to use a function from a family we met all the way back in Tutorial 01/02, namely is.na().\nYou can think of is.na() as a question about whatever is in its brackets: “Is (this) NA?” If the value IS an NA, R will return TRUE; if it’s anything else at all, R will return FALSE. So, to get an accurate count, we need to capture people who either answered something other than “Yes”, or didn’t answer at all.\n\nn_no_consent &lt;- anx_data_proc |&gt; \n  dplyr::filter(consent != \"Yes\" | is.na(consent)) |&gt; \n  nrow()\n\nn_no_consent\n\n[1] 33\n\n\nThen, we remove all participants who did not actively consent and assign the resulting dataset to the same name, overwriting the previous version. As we saw before, the below would discard cases that answered “No” (along with any other value not exactly matching “Yes”) and cases with NAs from people who didn’t answer.\n\nanx_data_proc &lt;- anx_data_proc |&gt; \n  dplyr::filter(consent == \"Yes\")\n\n\n\nAge\nFor low-risk ethics applications, you may want to exclude people who reported an age below the age of informed consent (typically 18). This may look like age &gt;= 18 or similar in your dataset. However, it’s also important to check for errors or improbable ages, or to remove any participants that are too old if your study has an upper age limit. In this case, my hypothetical study didn’t have an upper age limit, but I’ll designate any ages as 100 or above as unlikely to be genuine responses.\nSince these are removed for two different reasons, I’ll save them as two separate objects.\n\n## Store the number to be removed\nn_too_young &lt;- anx_data_proc |&gt; \n  dplyr::filter(age &lt; 18) |&gt; \n  nrow()\nn_too_young\n\n[1] 22\n\nn_too_old &lt;- anx_data_proc |&gt; \n  dplyr::filter(age &gt;= 100) |&gt; \n  nrow()\nn_too_old\n\n[1] 5\n\n## Remove them\nanx_data_proc &lt;- anx_data_proc |&gt; \n  dplyr::filter(\n    dplyr::between(age, 18, 99)\n  )\n\n\n\nMissing Values\nFinally (for now), just about any study will have to decide how to deal with missing values. The possibilities for your own work are too complex for me to have a guess at here, so for now we’ll only look at how to identify and remove missing values.\n\nSingle Variable\nLet’s look at a single variable to begin with - for example, sticsa_trait_3. We can confirm that this variable has a/some NAs to consider by counting the unique values:\n\nanx_data |&gt; \n  dplyr::count(sticsa_trait_3)\n\n\n\n  \n\n\n\nThe first thing you might think to try is to filter on sticsa_trait_3 == NA, but weirdly enough this doesn’t work. Instead, we again need the increasingly versatile is.na(), which again, we can think of as a question about whatever is in its brackets: “Is (this) NA?” Let’s see this in action:\n\n1anx_data_proc |&gt;\n2  dplyr::filter(\n3    is.na(sticsa_trait_3)\n  )\n\n\n1\n\nTake the dataset anx_data_proc, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the sticsa_trait_3 variable IS missing (is NA).\n\n\n\n\nThese are the cases we want to remove, so we count how many there are and assign that number to a useful object name, as we did before.\n\nn_sticsa_t3_missing &lt;- anx_data_proc |&gt;\n  dplyr::filter(\n    is.na(sticsa_trait_3)\n  ) |&gt; \n  nrow()\n\nn_sticsa_t3_missing\n\n[1] 3\n\n\nNext, we need to actually exclude these cases. This time, we want to retain the inverse of the previous filtering requirement: that is, we only want to keep the cases that are NOT missing a value, the opposite of what we got from is.na(sticsa_trait_3). You may recognise “the inverse” or “not-x” as something we’ve seen before with !=, “not-equals”. For anything that returns TRUE and FALSE, you can get the inverse by putting an ! before it. (Try running !TRUE, for example!)\nSo, to create my clean anx_data_final dataset, I can use the assertion !is.na(sticsa_trait_3) to keep only the participants who answered this question - who do NOT have a missing value.\nFinally, I can store the actual number of usable cases, according to my cleaning requirements, in a final object to use when reporting.\n\nanx_data_final &lt;- anx_data_proc |&gt;\n  dplyr::filter(\n    !is.na(sticsa_trait_3)\n  )\n\nn_final &lt;- nrow(anx_data_final)\nn_final\n\n[1] 390\n\n\n\n\nAll Variables\nRemoving NAs is a tricky process, but if you’re sure that you want to drop all cases with missing values in your dataset, there are few helper functions to make this easy.\nFor this, we’re going to leave filter() for a moment at look at a different function, tidyr::drop_na(). This function takes a tibble as input, and returns the same tibble as output, but with any rows that had missing values removed.\n\n\n\n\n\n\nWarning\n\n\n\nThis is a pretty major step and should be used with caution! If we didn’t check our data carefully, we could easily end up dropping a bunch of cases we didn’t want to get rid of.\n\n\nFor example, if we apply it uncautiously here:\n\nanx_data_proc |&gt; \n  tidyr::drop_na()\n\n\n\n  \n\n\n\nWell, there goes all our data!\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Why has every single row in the dataset been dropped? Using any method you like, investigate what’s happened.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is something we could work out without any R whatsoever, just using the codebook and a bit of View mode to confirm. The Codebook tells us that participants were in one of two independent conditions: \"maths\" or \"stats\". Because of the wide format of the data, there are mcq_maths questions that are always NA for people in the statistics-MCQ condition, and vice versa for the mcq_stats questions and people in the maths-MCQ condition. So, every single participant - even those who answered every question - has at least some missing values, and dropping NAs without checking just bins the whole dataset.\nIf I wanted to check this with R, I’d be hard pressed to do it with only what we’ve covered so far. Using the some extra challenge functions from the next tutorial, though, I’d do this:\n\nanx_data |&gt; \n  dplyr::mutate(\n    ## Create a new variable containing the number of missing values in each row\n    number_nas = rowSums(is.na(pick(everything())))\n  ) |&gt; \n  ## Count how many missing values there are\n  dplyr::count(number_nas)\n\n\n\n  \n\n\n\nSo, there’s at least 5 NAs in every single row, and when we call tidyr::drop_na(), every single row is dropped.\n\n\n\n\n\n\n\n\nReporting\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Using the objects counting intial, final, and excluded cases and what we covered last time about inline code, write a brief journal-style description of your exclusion process.\nWhat is the benefit of taking the extra effort to store these counts in objects? Under what circumstances might this be (particularly) useful?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can write whatever you like, but here’s an example using inline code.\n\nThe initial sample consisted of `r n_initial` cases. We removed `r n_no_consent` cases that did not consent, `r n_too_young` cases that reported an age below the ethical age of consent, and `r n_too_old` cases that reported improbable ages (100 years old or older). Finally, we removed `r n_sticsa_t3_missing` cases with who had not responded to the third trait item on the STICSA.This left us with a final sample of `r n_final` cases.\n\nWhen you render your document, this should come out as:\n\nThe initial sample consisted of 453 cases. We removed 33 cases that did not consent, 22 cases that reported an age below the ethical age of consent, and 5 cases that reported improbable ages (100 years old or older). Finally, we removed 3 cases with who had not responded to the third trait item on the STICSA. This left us with a final sample of 390 cases.\n\nThere’s a huge advantage of this, namely ease of change. Imagine you had researchers from labs all over the world join the study and add a huge amount of new data to a massive collaborative dataset. In order to update all your numbers, all you have to do is update your initial anx_data dataset with the new cases, and then re-run all your code as is. Because these objects count whatever is in the data, they will automatically contain and record the correct numbers for the data you put into them4.\nThere are other advantages too - like confidence that you, a human person who may occasionally make errors (sorry, no offence meant!), won’t misread, mistype, or otherwise mistake the numbers, because at no point do you actually type a particular number yourself.\nNifty, eh?"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#select",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#select",
    "title": "05: Filter and Select",
    "section": "Select",
    "text": "Select\nThe select() function is probably the most straightforward of the core {dplyr} functions. Its primary job is to easily and transparently subset the columns within a dataset - in particular, a tibble. Rows are not affected by select(), only columns.\n\nGeneral Format\nTo subset a tibble, use the general format:\n\n1dataset_name |&gt;\n2  dplyr::select(\n3    variable_to_include,\n4    -variable_to_exclude,\n5    keep_this_one:through_this_one,\n6    new_name = variable_to_rename,\n7    variable_number\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n2\n\nSelect the following variables:\n\n3\n\nThe name of a variable to be included in the output. Multiple variables can be selected separated by commas.\n\n4\n\nThe name of a variable to be excluded from the output. Use either an exclamation mark (!) or a minus sign (-) in front of each variable to exclude. Multiple variables can be dropped, separated by commas with a ! (or -) before each.\n\n5\n\nA range of variables to include in the output. All the variables between and including the two named will be selected (or dropped, with !(drop_this_one:through_this_one)).\n\n6\n\nInclude variable_to_rename in the output, but call it new_name.\n\n7\n\nInclude a variable in the output by where it appears in the dataset, numbered left to right. For example, “2” will select the second column in the original dataset.\n\n\n\n\nColumns will appear in the output in the order they are selected in select(), so this function can also be used to reorder columns.\n\n\nSelecting Directly\nThe best way to get the hang of this will be to give it a go, so let’s dive on in!\n\n\n\n\n\n\nExercise\n\n\n\nCreate a subset of anx_data that contains the following variables:\n\nThe participant’s age\nThe first variable in the original dataset\nAll of the STARS variables\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    age, 1,\n    stars_test1:stars_help4\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a subset of anx_data that contains the following variables:\n\nAll of the original variables but NOT distribution\nmcq renamed condition\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    -distribution,\n    condition = mcq\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\nThat’s really all there is to it!\n…Or is it?5\n\n\nUsing {tidyselect}\nThe real power in select(), and in many other {tidyverse} functions, is in a system of helper functions and notations collectively called &lt;tidyselect&gt;. The overall goal of “&lt;tidyselect&gt; semantics” (as you will see it referred to in help documentation) is to make selecting variables easy, efficient, and clear.\n\n\n\n\n\n\nNew to UGs\n\n\n\nAt UG level at Sussex, students are not taught about &lt;tidyselect&gt; in core modules. However, &lt;tidyselect&gt; is desperately useful and makes complex data wrangling/cleaning a lot faster and more efficient, especially (for instance) for questionnaires with similarly-named subscales, so would make for a great collaborative activity with supervisors!\n\n\nThese helper functions can be combined with the selection methods above in any combination. Some very convenient options include:\n\neverything() for all columns\nstarts_with(), ends_with(), and contains() for selecting columns by shared name elements\nwhere() for selecting with a function, described in the next section\n\n\n\n\n\n\n\nExercise\n\n\n\nOpen the help documentation by running ?dplyr::select in the Console to see examples of how to use all of the &lt;tidyselect&gt; helper functions.\n\n\nRather than list examples of all the helper functions here, it’s best to just try them out for yourself!\n\n\n\n\n\n\nExercise\n\n\n\nSelect the variables in anx_data that have to do with state anxiety.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    contains(\"state\")\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\nSelect all the variables in anx_data that are NOT the R-MARS, R-MARS-S, or STICSA.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    ## contains() also fine (in this case)\n    !starts_with(c(\"rmars\", \"sticsa\"))\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\nCHALLENGE: Select all the stars variables but NOT the stars_m variables.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis one’s a bit tricky because both starts_with() and contains() will return both types of STARS variables, because of the way the variables are named. We’ll have to provide multiple assertions, like we did earlier with filter().\n\nanx_data |&gt; \n  dplyr::select(starts_with(\"stars\") & !contains(\"_m_\"))\n\n\n\n\n\n\n \n\nUsing Functions\nLet’s say we want to generate a summary table of the variables in our dataset. Before we can create our summary in the next tutorial, we may first want to produce a subset of our dataset that only contains numeric variables.\nTo do this, we can use the &lt;tidyselect&gt; helper function where(). This helper function lets us use any function that returns TRUE and FALSE to select columns. Essentially, we don’t have to select columns using name or position - we can use any criteria we want, as long as we have (or can create…!) a function that expresses that criteria.\nEspecially helpful here is the is.*() family of functions in base R. This group of functions all have the same format, where the * is a stand-in for any type of data or object, e.g. is.logical(), is.numeric(), is.factor() etc. (The very useful is.na() that we’ve seen with filter() above is also a member of this family.) These functions work like a question about whatever you put into them - for example, is.numeric() can be read as, “Is (whatever’s in the brackets) numeric data?”\n\n\n\n\n\n\nTip\n\n\n\nYou can quickly find all of the functions in this family by typing is. in a code chunk and pressing Tab.\n\n\nPutting these two together, we could accomplish the task of selecting only numeric variables as follows:\n\nanx_data |&gt; \n  dplyr::select(\n    where(is.numeric)\n  )\n\n\n\n  \n\n\n\nThis command evaluates each column and determines whether they contain numeric data (TRUE) or not (FALSE), and only returns the columns that return TRUE.\n\n\nUsing Custom Functions\n\n\n\n\n\n\nHere There Be Lambdas\n\n\n\nThe material in this section isn’t covered in the live workshops. It’s included here for reference because it’s extremely useful in real R analysis workflows, but it won’t be essential for any of the workshop tasks.\n\n\nThe function in where() that determines which columns to keep doesn’t have to be an existing named function. Another option is to use a “purrr-style lambda” or formula (a phrase you may see in help documentation) to write our own criteria on the spot.\nFor example, let’s select all of the numeric variables that had a mean of 3 or higher:\n\nanx_data |&gt;\n  dplyr::select(\n    where(~is.numeric(.x) & mean(.x, na.rm = TRUE) &gt;= 3)\n  )\n\nInstead of just the name of a function, as we had before, we now have a formula. This formula has a few key characteristics:\n\nThe ~ (apparently pronounced “twiddle”!) at the beginning, which is a shortcut for the longer function(x) ... notation for creating functions.\nThe .x, which is a placeholder for each of the variables that the function will be applied to.\n\nSo, this command can be read: “Take the anx_data dataset and select all the columns where the following is true: the data type is numeric AND the mean value in that column is greater than or equal to 3 (ignoring missing values).”\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Select the variables in anx_data that are character type, or that do NOT contain any missing values.\nHint: You may need to use function(s) that we haven’t covered in the tutorials so far to solve this.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis one is a doozy! Very well done if you worked it out, either using your own solution or one like this, or if you got partway there.\n\nanx_data |&gt; \n  dplyr::select(\n    where(~ is.character(.x) | all(!is.na(.x)))\n  )\n\n\n\n  \n\n\n\nHere’s the process to understand/solve this using this particular solution.\nThe first half of the formula in where() should be okay - you may have noticed the &lt;chr&gt; label in the tibble output and/or guessed that there might be an is.*() function for this purpose.\nThe second half is a bit rough. You may have tried !is.na(.x) and got an error, namely: Predicate must return TRUE or FALSE, not a logical vector. In other words, this has to return a SINGLE logical value, and is.na() will return a vector containing logical values for each individual value in the variable.\nTo solve this - at least the way I’ve done - you need the {base} function all(), which answers the question, “Are all of these values TRUE?” It also has a (non-identical) twin any(), which (as you might guess) answers the question, “Are any of these values TRUE?” So, all() does a similar job as AND, and any() a similar job as OR.\nTo see what I mean, let’s just try it out:\n\nall(TRUE, TRUE)\n\n[1] TRUE\n\nall(TRUE, FALSE)\n\n[1] FALSE\n\nall(FALSE, FALSE)\n\n[1] FALSE\n\nany(TRUE, TRUE)\n\n[1] TRUE\n\nany(TRUE, FALSE)\n\n[1] TRUE\n\nany(FALSE, FALSE)\n\n[1] FALSE\n\n\nLike AND and OR, all() and any() only give different responses when there are a mix of TRUEs and FALSEs. For this task, we only wanted to retain variables where ALL of the values produced by !is.na(x) were TRUE - that is, it was true that ALL of the values in that variable do NOT contain NAs. So, we wanted all(). This returns a single TRUE or FALSE value for each variable that dplyr::select() can use."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#quick-test-t-test-redux",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#quick-test-t-test-redux",
    "title": "05: Filter and Select",
    "section": "Quick Test: t-test redux",
    "text": "Quick Test: t-test redux\nIn the very first of these tutorials, we ran a t-test using some vectors of made-up data. We’re going to do the same thing again, but look at how we can use real data in a dataset instead.\n\n\n\n\n\n\nExercise\n\n\n\nBring up the help documentation for t.test() and use it to run a t-test comparing responses for the first item of the STICSA post-test state anxiety scale between people who saw the maths MCQs vs the stats MCQs.\nHint: Try using the formula notation, just like we did in the previous tutorial for lm().\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCall up the help documentation in the Console:\n\n?t.test\nhelp(t.test)\n\nRun the test:\n\n## Pipe method\nanx_data |&gt; \n  t.test(sticsa_post_state_1 ~ mcq, data = _)\n\n\n    Welch Two Sample t-test\n\ndata:  sticsa_post_state_1 by mcq\nt = 0.88618, df = 448.97, p-value = 0.376\nalternative hypothesis: true difference in means between group maths and group stats is not equal to 0\n95 percent confidence interval:\n -0.09741366  0.25741366\nsample estimates:\nmean in group maths mean in group stats \n               2.00                1.92 \n\n## Single command method\nt.test(sticsa_post_state_1 ~ mcq, data = anx_data)\n\n\n    Welch Two Sample t-test\n\ndata:  sticsa_post_state_1 by mcq\nt = 0.88618, df = 448.97, p-value = 0.376\nalternative hypothesis: true difference in means between group maths and group stats is not equal to 0\n95 percent confidence interval:\n -0.09741366  0.25741366\nsample estimates:\nmean in group maths mean in group stats \n               2.00                1.92 \n\n\n\n\n\n\n\nThere are a lot of options in the t.test() function, which can be used, through different arguments, to run almost any variety of t-test you can think of. Here, the “stats” and “maths” groups are independent groups, so we can mostly go with the defaults. Just as we saw in Tutorial 04 with the lm() function, the formula takes the form outcome ~ predictor, where the predictor is the grouping variable.\nNote that the output mentions “Welch Two Sample t-test”, which is a version of the test that does not assume equal variances. This is the version that is taught to undergraduates, because we have not at this point introduced the process of assumption testing. If you definitely know that the variances are equal and you definitely want Student’s t-test, you can instead change the default setting.\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Using the help documentation, re-run the t-test with equal variances assumed.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nt.test(sticsa_post_state_1 ~ mcq, \n       data = anx_data, \n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  sticsa_post_state_1 by mcq\nt = 0.88619, df = 449, p-value = 0.376\nalternative hypothesis: true difference in means between group maths and group stats is not equal to 0\n95 percent confidence interval:\n -0.09741222  0.25741222\nsample estimates:\nmean in group maths mean in group stats \n               2.00                1.92 \n\n\n\n\n\n\n\nFinally, we would like to turn this ugly analysis output into a nicely formatted report of the results. You now have all the skills to do this programmatically - without directly typing any of the numbers yourself.\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Using what you known about papaja, inline code, and Quarto, report the results of this t-test in full without typing any of the numerical results into the text directly.\nThen, render your workbook to see the results!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo begin, save the t-test output in a new object to work with. Then, generate the report output using papaja and store that in another new object.\n\nsticsa_t &lt;- t.test(sticsa_post_state_1 ~ mcq, data = anx_data)\nsticsa_t_out &lt;- papaja::apa_print(sticsa_t)\n\nNext, write the text, and fill in the statistical reporting with inline code where necessary. Here’s an example:\n\nWe compared mean scores on the STICSA state anxiety item 1 between two groups, who either saw the mathematics MCQ questions (M = `r round(sticsa_t$estimate[1], 2)`) or the statistics MCQ questions (M =`r  round(sticsa_t$estimate[2], 2)`). A Welsh’s robust t-test revealed no statistically significant difference in scores between groups (`r  sticsa_t_out$full_result`).\n\nWhich should render as:\n\nWe compared mean scores on the STICSA state anxiety item 1 between two groups, who either saw the mathematics MCQ questions (M = 2) or the statistics MCQ questions (M =1.92). A Welsh’s robust t-test revealed no statistically significant difference in scores between groups (\\(\\Delta M = 0.08\\), 95% CI \\([-0.10, 0.26]\\), \\(t(448.97) = 0.89\\), \\(p = .376\\)).\n\n\n\n\n\n\n \nWell done getting through all that! In the next tutorial, we will look at more {dplyr} powerhouses to round out your data wrangling toolkit."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#footnotes",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#footnotes",
    "title": "05: Filter and Select",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis incredibly useful property is called “data masking”. If you want to know more, run vignette(\"programming\") in the Console.↩︎\nI’m not wild about this example - the experiences of non-binary and other genders are just as important! Unfortunately it’s the only variable in the dataset with the right number of categories.↩︎\nI did try to think of a snazzy acronym here, but all I came up with is AEC (yikes). I’ll keep thinking and try to update this with something better, and I welcome suggestions if you’ve made it this far!↩︎\nI’m sure Jenny would tell you there’s a little more to it than that, especially with 12,570 students from 100 universities in 35 countries, collected in 21 languages! But that’s both the dream and the general idea.↩︎\nHave you seen the size of this tutorial?? Of course it isn’t!↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html",
    "title": "07: Visualisations",
    "section": "",
    "text": "This tutorial focuses on creating data visualisations using the {ggplot2} package. It covers the conceptual grammar of {ggplot} and a variety of examples of common plots. It also covers how to customise key elements of the plots, such as colour, axis labeling, and adding statistical summaries, and finishes with some tips on reporting figures in Quarto.\n\n\n\n\n\n\n{ggplot2} and Data Visualisation Resources\n\n\n\nCheck out the following indispensible resources for building beautiful visualisations:\n\nFrom Data to Viz, a beautifully designed helper site to help you explore the options for visualising your data, with thorough links to…\nR Graph Gallery, an archive of plots in R all with reproducible code and tutorials to build them yourself.\nTidy Tuesdays, which releases a new dataset each week, and people build and share beautiful visualisations, along with the code and frequently their process.\nData visualisation using R, for researchers who don’t use R, a fantastic how-to from the brilliant team at PsychTeachR at Glasgow, that provides a very friendly, start-to-finish walkthrough of the whole process of data reading, manipulation, and plot-building.\n\n\n\n\n\nData visualisation with {ggplot2} is explicitly introduced near the end of the first term of Year 1 at Sussex, and practiced throughout the second term. In Year 2, there is less focus on constructing plots from scratch, and instead students are taught how to get a basic plot pre-made and then modify it using their {ggplot2} skills.\nThis tutorial includes essentially everything students are taught in Year 1, with a variety of extRas sprinkled in. In the next tutorial, we will look at the plots they are taught in Year 2, which accompany the relevant analysis (e.g. ANOVA, mediation, and moderation)."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#overview",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#overview",
    "title": "07: Visualisations",
    "section": "",
    "text": "This tutorial focuses on creating data visualisations using the {ggplot2} package. It covers the conceptual grammar of {ggplot} and a variety of examples of common plots. It also covers how to customise key elements of the plots, such as colour, axis labeling, and adding statistical summaries, and finishes with some tips on reporting figures in Quarto.\n\n\n\n\n\n\n{ggplot2} and Data Visualisation Resources\n\n\n\nCheck out the following indispensible resources for building beautiful visualisations:\n\nFrom Data to Viz, a beautifully designed helper site to help you explore the options for visualising your data, with thorough links to…\nR Graph Gallery, an archive of plots in R all with reproducible code and tutorials to build them yourself.\nTidy Tuesdays, which releases a new dataset each week, and people build and share beautiful visualisations, along with the code and frequently their process.\nData visualisation using R, for researchers who don’t use R, a fantastic how-to from the brilliant team at PsychTeachR at Glasgow, that provides a very friendly, start-to-finish walkthrough of the whole process of data reading, manipulation, and plot-building.\n\n\n\n\n\nData visualisation with {ggplot2} is explicitly introduced near the end of the first term of Year 1 at Sussex, and practiced throughout the second term. In Year 2, there is less focus on constructing plots from scratch, and instead students are taught how to get a basic plot pre-made and then modify it using their {ggplot2} skills.\nThis tutorial includes essentially everything students are taught in Year 1, with a variety of extRas sprinkled in. In the next tutorial, we will look at the plots they are taught in Year 2, which accompany the relevant analysis (e.g. ANOVA, mediation, and moderation)."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#setup",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#setup",
    "title": "07: Visualisations",
    "section": "Setup",
    "text": "Setup\n\nPackages\nWe will be relying heavily on the {ggplot2} package, naturally, which is part of {tidyverse}. You can load {ggplot2} by itself, but since we will also make use of some other {tidyverse} functions, it’s probably most efficient to simply load {tidyverse}.\nWe will also be using {ggrain} for raincloud plots and {viridis} for colour palettes.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(ggrain)\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Explicit Function Calls\n\n\n\n\n\nYou may notice that I will abandon my usual explicit function style for {ggplot2} functions - so, for example, instead of ggplot2::ggplot(ggplot2::aes(...)) I’ll just write ggplot(aes()). In this case, it is much easier to simply load the {ggplot2} package and drop the package call, than to type the same package name over and over and over. It also makes the chonky {ggplot2} code a lot easier to read!\n\n\n\n\n\nData\nToday we’re continuing to work with the dataset courtesy of fantastic Sussex colleague Jenny Terry. This dataset contains real data about statistics and maths anxiety. For these latter two tutorials, I’ve created averaged scores for each subscale, and dropped the individual items.\n\n\n\n\n\n\nExercise\n\n\n\nRead in the dataset and save it in a new object, anx_score_data.\nOn the Cloud, you can read in this dataset from the data folder using here::here().\nElsewhere, you can download the dataset, or copy the dataset URL, from the Data and Workbooks page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRead in from file:\n\nanx_scores_data &lt;- readr::read_csv(here::here(\"data/anx_scores_data.csv\"))\n\nRead in from URL:\n\nanx_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/anx_scores_data.csv\")\n\n\n\n\n\n\n\nCodebook\nThere’s quite a bit in this dataset, so you will need to refer to the codebook below for a description of all the variables.\n\n\n\n\n\n\nDataset Info Recap\n\n\n\n\n\nThis study explored the difference between maths and statistics anxiety, widely assumed to be different constructs. Participants completed the Statistics Anxiety Rating Scale (STARS) and Maths Anxiety Rating Scale - Revised (R-MARS), as well as modified versions, the STARS-M and R-MARS-S. In the modified versions of the scales, references to statistics and maths were swapped; for example, the STARS item “Studying for an examination in a statistics course” became the STARS-M item “Studying for an examination in a maths course”; and the R-MARS item “Walking into a maths class” because the R-MARS-S item “Walking into a statistics class”.\nParticipants also completed the State-Trait Inventory for Cognitive and Somatic Anxiety (STICSA). They completed the state anxiety items twice: once before, and once after, answering a set of five MCQ questions. These MCQ questions were either about maths, or about statistics; each participant only saw one of the two MCQ conditions.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor learning purposes, I’ve randomly generated some additional variables to add to the dataset containing info on distribution channel, consent, gender, and age. Especially for the consent variable, don’t worry: all the participants in this dataset did consent to the original study. I’ve simulated and added this variable in later to practice removing participants.\n\n\n\n\n\n\n\nVariable\nType\nDescription\nValues\n\n\n\n\nid\nCategorical\nUnique ID code\nNA\n\n\ndistribution\nCategorical\nChannel through which the study was completed, either as a preview (before real data collection) or anonymous genuine responses. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\"preview\" or \"anonymous\"\n\n\nconsent\nCategorical\nWhether the participant read and consented to participate. Note that this variable has been randomly generated and does NOT reflect genuine responses; all participants in this dataset did originally consent to participate.\n\"Yes\" or \"No\"\n\n\ngender\nCategorical\nGender identity. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\"female\", \"male\", \"non-binary\", or \"other/pnts\". \"pnts\" is an abbreviation for \"Prefer not to say\".\n\n\nage\nNumeric\nAge in years. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n18 - 99\n\n\nmcq\nCategorical\nIndependent variable for MCQ question condition, whether the participant saw MCQ questions about mathematics or statistics.\n\"maths\" or \"stats\"\n\n\nstars_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Statistics Anxiety Rating Scale (STARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_int_score\nNumeric\nAveraged score on the Interpretation Anxiety subscale of the Statistics Anxiety Rating Scale (STARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_help_score\nNumeric\nAveraged score on the Asking for Help subscale of the Statistics Anxiety Rating Scale (STARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_m_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Statistics Anxiety Rating Scale - Maths (STARS-M), a modified version of the STARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_m_int_score\nNumeric\nAveraged score on the Interpretation Anxiety subscale of the Statistics Anxiety Rating Scale - Maths (STARS-M), a modified version of the STARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nstars_m_help_score\nNumeric\nAveraged score on the Asking for Help subscale of the Statistics Anxiety Rating Scale - Maths (STARS-M), a modified version of the STARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Revised Maths Anxiety Rating Scale (R-MARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_num_score\nNumeric\nAveraged score on the Numerical Task Anxiety subscale of the Revised Maths Anxiety Rating Scale (R-MARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_course_score\nNumeric\nAveraged score on the Course Anxiety subscale of the Revised Maths Anxiety Rating Scale (R-MARS)\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_s_test_score\nNumeric\nAveraged score on the Test Anxiety subscale of the Revised Maths Anxiety Rating Scale - Statistics (R-MARS-S), a modified version of the MARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_s_num_score\nNumeric\nAveraged score on the Numerical Anxiety subscale of the Revised Maths Anxiety Rating Scale - Statistics (R-MARS-S), a modified version of the MARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nrmars_s_course_score\nNumeric\nAveraged score on the Course Anxiety subscale of the Revised Maths Anxiety Rating Scale - Statistics (R-MARS-S), a modified version of the MARS with all references to maths replaced with statistics.\n1 (low anxiety) to 5 (high anxiety)\n\n\nsticsa_trait_score\nNumeric\nAveraged score on the Trait Anxiety subscale of the State-Trait Inventory for Cognitive and Somatic Anxiety.\n1 (not at all) to 4 (very much so)\n\n\nsticsa_pre_state_score\nNumeric\nAveraged score on the State Anxiety subscale of the State-Trait Inventory for Cognitive and Somatic Anxiety, pre-MCQ.\n1 (not at all) to 4 (very much so)\n\n\nsticsa_post_state_score\nNumeric\nAveraged score on the State Anxiety subscale of the State-Trait Inventory for Cognitive and Somatic Anxiety, post-MCQ.\n1 (not at all) to 4 (very much so)\n\n\nmcq_score\nNumeric\nTotal (summed) score on the MCQ questions.\n0 (all incorrect) to 5 (all correct)\n\n\n\n\n\n\n\nIf you have some experience with R, you are welcome to instead use another dataset that you are familiar with or are keen to explore. However, remember that anything you upload to the training Posit Cloud workspace is visible to all workspace admins, so keep GDPR in mind."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#grammar-of-graphics",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#grammar-of-graphics",
    "title": "07: Visualisations",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nThe {ggplot} package is a bit of a universe in its own right. Like R itself, it’s extremely powerful and versatile, and that also means there are a near-endless variety of things you could do and different ways to do them.\nWhat we’ll cover in this tutorial is the core structure of building data visualisations in {ggplot2}, so that you have a solid base to build your own designs on. No matter what you’re trying to do, there’s nearly always a blog post, help document, or Stack Overflow post titled “How to do [whatever] in ggplot” to point you in the right direction!\n\nLayers\nPlots in {ggplot2} are built in layers. Each layer adds to or changes something about the plot; these can be big elements, like determining the type of plot to create, to small details like editing axis labels or changing colours. In {ggplot2}, each layer is a function.\nIf it helps, you can think of layers like different colours in a linocut print. Each additional layer of colour adds a bit more to the overall picture, building up from big blocks of colour to small details.\nTo build a visualisation in {ggplot2}, it’s a good idea to build your plot in the same way, from big picture to small detail. As we’ll see, I usually build plot layers like this:\ndataset |&gt; \n  aesthetics_mapping +\n  choose_type_of_plot +\n  add_more_elements +\n  edit_labels_or_colours +\n  apply_a_theme\nThese are guidelines, but the general =&gt; specific flow is for a good reason: layers are evaluated from top to bottom. So, it’s best to get the big pieces in place first, then fine-tune, than to have those fiddly bits overwritten by a major change at the end of the code.\nNotice as well that layers are added to a plot object with + and NOT with |&gt;. This is specific to {ggplot2} (AFAIK!) and is easy to forget, but don’t worry - it’s such a common thing that {ggplot2} has a very friendly error message for fixing it.\n\n\n\n\n\n\nError Watch: mapping must be created by aes()\n\n\n\n\n\nThe actual error that pops up when you use a pipe instead of + isn’t super transparent. However, there is a very friendly reminder directly underneath to nudge you in the right direction. It’s a good reason to always read the error message in full!\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_sum)) |&gt; \n  geom_histogram()\n\nError in `geom_histogram()`:\n! `mapping` must be created by `aes()`\nℹ Did you use `%&gt;%` or `|&gt;` instead of `+`?\n\n\n\n\n\n\n\nMapping\nAs we saw in the error just above, mapping is created with aes(). This little function defines the aesthetics of the plot - in other words, this is how you tell R what data you want it to plot. We’ll use the following general format to set up a plot:\ndataset_name |&gt; \n  ggplot(aes(x = variable_on_x_axis, y = variable_on_y_axis, ...))\nThe ... takes additional arguments to add things like colour and fill.\n\n\nGeoms, etc.\nSo, how do we actually add layers? There are several common types of functions with shared prefixes that do particular things. We’ll meet lots of examples of them just below, but as a quick reference for some of the more common function types:\n\ngeom_*(): Draw geometric objects to represent the data.\nstat_*(): Add elements to the plot calculated with statistical functions.\nscale_*(), labs(), and lims(): Adjust the appearance of the axes (labels, title, limits, etc.) or quickly adjust the labels or limits only\nguide_*(): Make adjustments to the scales or to other interpretational elements of the plot (such as legends for categories)\ntheme_*(): Apply a pre-made theme to the entire plot\n\n\n\n\n\n\n\nTip\n\n\n\nSee the {ggplot} reference documentation for a comprehensive list and detailed guide to these functions and more.\n\n\nRight, the best way to get a handle on these functions is to start building plots! So let’s jump in."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#histograms-and-density-plots",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#histograms-and-density-plots",
    "title": "07: Visualisations",
    "section": "Histograms and Density Plots",
    "text": "Histograms and Density Plots\nLet’s start with a histogram, a very common type of visualisation that represents the frequency of each value in a variable. We’re going to first create a histogram of the mcq_score variable in the anx_scores_data dataset.\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the instructions below to build your own plot as we go. Feel free to tweak details like colour and labels as you like.\n\n\nFor a basic histogram, we need three elements. First, pipe in the data; then, set the aesthetics; then, use geom_histogram() to draw the plot.\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHooray! The downside is… that’s pretty ugly. For a quick glance this isn’t really any better than hist(). So - let’s make it better by doing the following:\n\nAdjusting the binwidth to present the values more sensibly\nAdding colour and fill\nAdjusting the axis labels and tick marks\nApplying a theme.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you haven’t yet, you might want to pull up the help documentation for geom_histogram() to get a look at the options available.\n\n\n\nAdjusting Binwidth\nThe message from the previous output gave us a helpful tip to deal with those weird bins:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nSo, let’s use binwidth = to choose a more appropriate value for this data. Because these are the summed MCQ scores, the values can only be whole numbers between 0 and 5. So, let’s try setting the binwidth to 1.\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\nThis is better, but now we’ve got some new problems: one, the automatically generated axis now only has 0, 2, and 4, when it would be better to have 0 - 5; and the histogram now looks like quite a shapeless lump. Let’s work on the colour first.\n\n\nColour and Fill Manual\nIt’s not obvious from the help documentation unfortunately (just mentioned in passing under the ... argument) but this function, and most geom_*() functions!, will take the arguments colour and fill. For this, we give each argument a string with a colour name in it. You can just guess at colour names until you find one you like, or you can refer to this massive list of R colour names to pick one.\nHere’s an example of how that looks with some very boring colours, that’s a bit easier on the eyes than the dark grey lump above.\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score)) +\n  geom_histogram(binwidth = 1, \n                 colour = \"black\", \n                 fill = \"lightgray\")\n\n\n\n\nThis would be fine for formal reporting, but we’re going to be all about the aesthetics today and those colours are downright dreary. Let’s instead borrow from the Sussex official colour palette to set custom colours. I’m choosing “Sussex Flint”1 for the outline colour and “Deep Aquamarine” for the fill, but feel free to choose whatever you like. You could also use an online colour picker like this one to choose any hex code for any colour you can dream of to use!\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score)) +\n  geom_histogram(binwidth = 1, \n                 colour = \"#003b49\", \n                 fill = \"#007a78\")\n\n\n\n\nWhat we can see here is that the “colour” in this case is the outline of the bars; and the “fill” is the colour inside the bars. This system is fairly consistent across geoms, but you may need to mix up the “colour” vs “fill” arguments depending on the exact effect you want.\nWith that sorted, let’s move onto the axis and ticks.\n\n\nAdjusting Axes with scale_*()\nYou can use the labs() shortcut to adjust labels easily, but personally I prefer to go the long way round (surprise lol) and use the scale_*() family instead. These functions have specific names depending on which axis scale you want to adjust, and the way that data is measured. In this case I want to adjust the x-axis and the data is continuous2, so the function I need is scale_x_continuous().\nThe advantage of going through the extra trouble to use the scale_*() functions is that they allow you to adjust everything about that scale at once. So, I can use the arguments for scale_x_continuous() to change the axis label with name =, the breaks with breaks =, the limits with limits = …well, I’m sure you get the idea!\nI can also do the same with the y axis, using (as you might have guessed) scale_y_continuous().\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score)) +\n  geom_histogram(binwidth = 1, \n                 colour = \"#003b49\", \n                 fill = \"#007a78\") +\n  scale_x_continuous(name = \"Total MCQ Score (out of 5)\",\n                     breaks = c(0:5)) +\n  scale_y_continuous(name = \"Frequency\",\n                     breaks = seq(from = 0, \n                                  to = 120, \n                                  by = 20),\n                     limits = c(0, 120))\n\n\n\n\nIf you have a look at the argument to breaks and limits, you can see why we took some time early on learning about numeric vectors. breaks requires a vector of values indicating where the breaks should be, and limits requires a vector of two values, one for the minimum limit and the other for the maximum limit.\nI’ve also used a new function, seq(), to create the y-axis breaks. This handy little base-R function generates a sequence of values, given the first and last values in the sequence and how much you want to increment by. I’ve asked it to start at 0, end at 120, and increment by 20 each time, instead of typing c(0, 20, 40, ...) myself.\n\n\nAdding a Theme\nThat’s actually looking pretty snazzy at this point. The last thing we might want to do for now is to apply a theme. The default is theme_grey(), which I’m not a fan of. If you type theme_ into a code chunk, you’ll get a list of options to try out and find one you like.\nHowever, I’m going to go straight the most extRa option, namely: an inbuilt APA-style theme from the {papaja} package, papaja::theme_apa(). Because I like how it looks, I’ll use the box = TRUE argument to draw a box around the whole plot. Teal (excuse me, Deep Aquamarine) isn’t a particularly APA-standard colour, but I like it, so there.\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score)) +\n  geom_histogram(binwidth = 1, \n                 colour = \"#003b49\", \n                 fill = \"#007a78\") +\n  scale_x_continuous(name = \"Total MCQ Score (out of 5)\",\n                     breaks = c(0:5)) +\n  scale_y_continuous(name = \"Frequency\",\n                     breaks = seq(from = 0, \n                                  to = 120, \n                                  by = 20),\n                     limits = c(0, 120)) +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\nThere you have it! That’s looking pretty nice, I think.\nTo lock it in, let’s get some practice using these functions with a minimally different example.\n\n\n\n\n\n\nExercise\n\n\n\nCreate a density plot of the rmars_s_test_score variable (or another continuous variable in the dataset of your choosing). For a finished plot, make sure you:\n\nChoose colours for your plot.\nAdjust the labels, breaks, and limits as necessary.\nAdd a theme.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can start the same way we did above, just switching out our previous variable in aes() for the new one and changing geom_histogram() to geom_density(). We can drop the binwidth argument since it isn’t relevant anymore. I’ve left the colours in because they work the same way (and look nice!).\n\nanx_scores_data |&gt; \n  ggplot(aes(x = rmars_s_test_score)) +\n  geom_density(colour = \"#003b49\", fill = \"#007a78\")  +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\nFrom here we might want to do a bit more tweaking to get the axes right, using the same functions as we did for the histogram.\n\nanx_scores_data |&gt; \n  ggplot(aes(x = rmars_s_test_score)) +\n  geom_density(colour = \"#003b49\", fill = \"#007a78\") +\n  scale_x_continuous(name = \"Mean Score on the R-MARS-S Test Subscale\",\n                     breaks = c(0:5)) +\n  scale_y_continuous(name = \"Probability Density\",\n                     limits = c(0, .5)) +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nFacet Wrap\n\n\n\n\n\n\nWarning\n\n\n\nNote: This gets a bit into the weeds, so if you’d like to skip this section, feel free to skip down to the next type of plot.\n\n\nBefore we move on, a useful option here would be to visualise the MCQ scores for the two MCQ groups side by side, since this was our independent manipulation. To do this, we can use the facet_wrap() function to split our single plot into two based on a grouping variable - here, mcq. I’ve also changed the settings on the y-axis to adjust to the new scale.\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score)) +\n  geom_histogram(binwidth = 1, \n                 colour = \"#003b49\", \n                 fill = \"#007a78\") +\n  scale_x_continuous(name = \"Total MCQ Score (out of 5)\",\n                     breaks = c(0:5)) +\n  scale_y_continuous(name = \"Frequency\",\n                     breaks = seq(from = 0, \n                                  to = 70, \n                                  by = 10),\n                     limits = c(0, 70)) +\n  facet_wrap(~mcq) +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\nThere are a few more things I’d like to tweak about this plot. First, I’d love to have the maths and stats groups in different colours to differentiate them visually. Second, I want to format the “maths” and “stats” labels at the top of the plot.\nTo do the first, I’m going to actually go back up to the aes() mapping function and add mcq as the variable that determines both colour and fill, and drop the color and fill arguments from geom_histogram().\nNext, I’m going to add a vector of names and values to the labeller = as_labeller(...) argument of facet_wrap(), which adds nice labels above each mini-plot.\nFinally, I use two more scale_*() functions to set the colour and fill, with the order of the colours in the same order as the values of the mcq variable. The guide = \"none\" argument removes the automatically generated legend, which I didn’t need because I added the labels.\nWhew! That’s it - here’s the final product.\n\nanx_scores_data |&gt; \n  ggplot(aes(x = mcq_score, colour = mcq, fill = mcq)) +\n  geom_histogram(binwidth = 1) +\n  scale_x_continuous(name = \"Total MCQ Score (out of 5)\",\n                     breaks = c(0:5)) +\n  scale_y_continuous(name = \"Frequency\",\n                     breaks = seq(from = 0, \n                                  to = 70, \n                                  by = 10),\n                     limits = c(0, 70)) +\n  facet_wrap(~mcq, \n             labeller = as_labeller(\n               c(`maths` = \"Maths MCQs\",\n                 `stats` = \"Stats MCQs\"))) +\n  scale_color_manual(values = c(\"#003b49\", \"#1b365d\"), \n                     guide = \"none\") +\n  scale_fill_manual(values = c(\"#007a78\", \"#7da1c4\"), \n                    guide = \"none\") +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\n\n\n\n\n\n\nChecking In\n\n\n\nHow are you doing? This is a bunch of new information, so don’t worry if you feel a bit overwhelmed. Honestly when I am constructing plots, a good portion of the time I spend to make them is either 1) digging back through my own previous documents to lift code I’ve already written or 2) searching online for help posts to figure out how to do things! I didn’t know how to do basically any of the formatting for the facet-wrapped plot above, and it took me about an hour of testing and searching to figure it out. I hope having these templates saves you a bit of time in the future."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#barplots",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#barplots",
    "title": "07: Visualisations",
    "section": "Barplots",
    "text": "Barplots\nWith some key ideas for building plots under our belts, let’s have a look at creating visualisations of categorical data.\nTo start with, we’ll need some (preferably interesting) categorical data to work with. Let’s begin by creating a categorical variable out of one of our continuous variables to practice what we covered in the previous tutorial. (This kind of operation is often not recommended for real analysis, but we’re learning so, y’know, it’s fine.)\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new variable in the dataset, stars_help_cat, that contains either “low” for people who scored below 3 on the STARS Asking for Help subscale, or “high” for people who scored 3 or above. Make sure to save your changes back to the dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, let’s use case_when() to create our variable, and check whether everything looks good using the .keep = \"used\" argument in mutate().\n\nanx_scores_data |&gt; \n  dplyr::mutate(\n    stars_help_cat = dplyr::case_when(\n      stars_help_score &lt; 3 ~ \"low\",\n      stars_help_score &gt;= 3 ~ \"high\",\n      .default = NA\n    ),\n    .keep = \"used\"\n  )\n\n\n\n  \n\n\n\nThat looks good to me, so let’s assign that output back to the dataset, removing the .keep argument before we do.\n\nanx_scores_data &lt;- anx_scores_data |&gt; \n  dplyr::mutate(\n    stars_help_cat = dplyr::case_when(\n      stars_help_score &lt; 3 ~ \"low\",\n      stars_help_score &gt;= 3 ~ \"high\",\n      .default = NA\n    )\n  )\n\n\n\n\n\n\nNow, let’s get started on our barplot.\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the instructions to build your own plot as we go. Feel free to tweak details like colour and labels as you like.\n\n\nTo keep this to a 2x2 comparison, let’s compare our new STARS help category in male vs female participants. So, we’ll need to filter before beginning the plot.\nThen, we’ll put our variable of interest - here, stars_help_cat - on the x-axis. Remember that in plots like this, the y-axis is the number of responses, and as we saw with histograms just above, R can do that for us automatically. So, we’ll skip the y-axis and instead add gender as a fill variable. Then, we’ll ask R to draw the right kind of geom for us - the transparently named geom_bar() - and throw on a nice theme on the end for good measure.\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"male\",\"female\")) |&gt; \n  ggplot(aes(x = stars_help_cat, fill = gender)) +\n  geom_bar() +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\nHmm. This barplot is okay, but it’s not easy to compare the gender categories directly. The bars are filled with different colours based on the gender variable, which we wanted, but they’re stacked on top of each other!\n\nPosition Dodge\nTo “fix” this (assuming it isn’t what you want - it might be!), we can use the position argument in geom_bar() to move the bars side by side, instead of stacked on top of each other. This option is called “dodge”, so let’s see how it looks now:\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"male\",\"female\")) |&gt; \n  ggplot(aes(x = stars_help_cat, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\nNice! Already that’s much better, and looking more like we might expect a grouped barplot to look. We do need to do something about those awful colours, but first let’s fix something that’s bothering me: the order of the x-axis categories.\n\n\nReordering Categories\nThe stars_help_cat variable on the x-axis has automatically ordered the categories in alphabetical order, “high” and then “low”. As I am a native speaker of a language that reads left to right, I’d prefer to have “low” on the left and “high” on the right3. So, how can I change the order of these categories?\n\nOption 1: Scale Labels\nWe’ve seen the scale_*() functions already - this time, we have a discrete x-axis variable, so we need scale_x_discrete(). We’ve even already seen the limits = argument used to set the upper and lower bound of the plot. Here, we can use this argument to specify the order we want the categories to be displayed in as well.\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"male\",\"female\")) |&gt; \n  ggplot(aes(x = stars_help_cat, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  scale_x_discrete(limits = c(\"low\", \"high\")) +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\n\n\nOption 2: Factor Levels\nWe haven’t covered factors much in this course so far; for our purposes, most of the time, R treats character data more or less the same way it would treat factors so it hasn’t been a problem (yet…). However, converting our stars_help_cat variable to a factor would let us specify an order of factor levels.\nLet’s see how that would work. Here I’m using mutate() to do the factor conversion before I pipe the data on into ggplot(). We’ve seen the factor() function before, just adding the levels = argument to reorder the levels in the order we wanted. The resulting plot then has the levels in the right order.\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"male\",\"female\")) |&gt; \n  dplyr::mutate(stars_help_cat = factor(stars_help_cat, \n                                        levels = c(\"low\", \"high\"))\n                ) |&gt; \n  ggplot(aes(x = stars_help_cat, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\nWhich to prefer? If you are going to change your categorical variables into factors anyway, it might be a good idea to set any factor ordering at that point, so you don’t need to bother with this later. Otherwise, it doesn’t matter as long as your plot looks the way you want it at the end!\nRight, now that all the structural stuff is out of the way, let’s finish up.\n\n\n\n\n\n\nExercise\n\n\n\nFinish up your plot by adjusting the following elements:\n\nGive a name to each axis\nRelabel discrete categories where appropriate\nAdjust continuous limits and breaks where appropriate\nChoose new colours!\n\nHint: You’ll have three axes to adjust. Use the help documentation for each if you get stuck.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n1anx_scores_data |&gt;\n2  dplyr::filter(gender %in% c(\"male\",\"female\")) |&gt;\n3  ggplot(aes(x = stars_help_cat, fill = gender)) +\n4  geom_bar(position = \"dodge\") +\n5  scale_x_discrete(name = \"STARS Asking for Help Anxiety\",\n                   limits = c(\"low\", \"high\"),\n                   labels = c(\"Low\", \"High\")) +\n6  scale_y_continuous(name = \"Count\",\n                     limits = c(0, 175),\n                     breaks = seq(0, 175, by = 25))+\n7  scale_fill_discrete(name = \"Gender\",\n                      labels = c(\"Female\", \"Male\"),\n                      type = c(\"#1b365d\", \"#f2c75c\")) +\n8  papaja::theme_apa(box = TRUE)\n\n\n1\n\nTake the dataset, and then\n\n2\n\nFilter the dataset to only retain cases where the value in the gender variable is any of “male” or “female”, and then\n\n3\n\nSet up a plot, mapping stars_help_score to the x-axis and gender as the fill, plus\n\n4\n\nDraw a barplot, with the bars side by side, plus\n\n5\n\nAdjust the discrete x axis by giving it a name, setting the limits (i.e. order of categories), and relabeling the categories, plus\n\n6\n\nAdjust the continuous y axis by giving it a name, setting the limits, and setting the breakpoints to appear as ticks on the axis, plus\n\n7\n\nAdjust the discrete fill legend by giving it a name, relabeling the categories, and changing the type of colours (using a colourblind-friendly and greyscale-print-resilient colour scheme!), plus\n\n8\n\nAdd an APA-style theme.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Order of Labels!\n\n\n\n\n\nKeep in mind that ggplot() is just applying the labels as strings on top of your plot. This means that if you, for instance, reorder your categories on the x-axis but then forget to update the order of axis labels in your code, your plot will be wrong. It’s always important to double-check that the labels you add do in fact correspond correctly to the order of the categories in the plot to avoid mislabeling.\nAs an example, the code below is identical to the solution above, except I’ve commented out the limits argument in scale_x_discrete() that reorders the categories on the x-axis. However, I’ve left in the labels argument that whacks on the axis labels in the reordered order. Comparing to the plot above, we can see by the sizes of the bars that the actual categories are in high-low order, despite what the nicely formatted labels say.\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"male\",\"female\")) |&gt; \n  ggplot(aes(x = stars_help_cat, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  scale_x_discrete(name = \"STARS Asking for Help Anxiety\",\n                   ## limits = c(\"low\", \"high\"),\n                   labels = c(\"Low\", \"High\")) +\n  scale_y_continuous(name = \"Count\",\n                     limits = c(0, 175),\n                     breaks = seq(0, 175, by = 25))+\n  scale_fill_discrete(name = \"Gender\",\n                      labels = c(\"Female\", \"Male\"),\n                      type = c(\"#1b365d\", \"#f2c75c\")) +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\nOne method to avoid this is to add formatting outside of ggplot(). In essence, instead of pasting on the labels afterwards, you edit the data just before piping into ggplot() so that the labels are already correctly formatted and in the right order. For this current situation with one plot, it’s much of a muchness whether the relabeling is in the data (via mutate) or in the axis labels. However, if you wanted to create multiple plots with these variables, relabeling the data just before plot creation would likely be more efficient than pasting in/checking the labels for each plot.\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"male\",\"female\")) |&gt; \n  ## This backticked name is kind of an extreme step! But it's possible\n  dplyr::mutate(`STARS Asking for Help Anxiety` = factor(stars_help_cat, \n                                        levels = c(\"low\", \"high\"),\n                                        labels = c(\"Low\", \"High\")),\n                Gender = factor (gender,\n                                 levels = c(\"female\", \"male\"),\n                                 labels = c(\"Female\", \"Male\"))) |&gt; \n  ## Note the formatted names here, and the backticks for the illegal variable name\n  ggplot(aes(x = `STARS Asking for Help Anxiety`, fill = Gender)) +\n  geom_bar(position = \"dodge\") +\n  scale_y_continuous(name = \"Count\",\n                     limits = c(0, 175),\n                     breaks = seq(0, 175, by = 25))+\n  scale_fill_discrete(type = c(\"#1b365d\", \"#f2c75c\")) +\n  papaja::theme_apa(box = TRUE)"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#quick-test-chi2",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#quick-test-chi2",
    "title": "07: Visualisations",
    "section": "Quick Test: \\(\\chi^2\\)",
    "text": "Quick Test: \\(\\chi^2\\)\nWhile we’re here looking at grouped barplots for categorical data, we can also have a quick \\(\\chi^2\\) test of association as a treat. Just like we did t-tests with t.test() and correlation tests with cor.test(), for \\(\\chi^2\\) we have chisq.test().\n\n\n\n\n\n\nExercise\n\n\n\nUsing the chisq.test() help documentation, perform a \\(\\chi^2\\) test of association for the stars_help_cat and gender variables.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, you can bring up the help documentation by running ?chisq.test in the Console.\nYou might notice right away that this function has no data = argument, and neither does it have an option to specify a formula like we’ve used previously. Instead, we just need to provide two vectors (or a matrix; see the Challenge task).\nRecall that when we made our plots above, we filtered the gender variable to contain only male and female participants. Here we need to do the same thing before we can run our test. To make sure I don’t drop these cases from my actual dataset, I’m going to create a new, filtered version of the dataset only for running this test.\n\nanx_scores_data_chisq &lt;- anx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"male\",\"female\"))\n\nNext, we need to get each of the variables out of the dataset using $ subsetting. This is exactly the same method we used in the very first tutorial to run a t-test! Remember to use the new, filtered dataset rather than the original.\n\nchisq.test(anx_scores_data_chisq$gender, anx_scores_data_chisq$stars_help_cat)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  anx_scores_data_chisq$gender and anx_scores_data_chisq$stars_help_cat\nX-squared = 3.1266, df = 1, p-value = 0.07702\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Use a matrix to run this test, instead of two individual vectors.\nHint: The way I’ve solved it, this requires reading the help documentation carefully and using a new function to create a table().\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe vector method we just saw took just the data, so we might first try doing that with a smaller version of the dataset (doing the filtering on gender, then selecting only the two variables we want):\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"female\", \"male\")) |&gt; \n  dplyr::select(gender, stars_help_cat) |&gt; \n  chisq.test()\n\nError in sum(x): invalid 'type' (character) of argument\n\n\nNo dice, and a weird error. Hmm. What does the help documentation have to say?\nUnder Details, we read:\n\nIf x is a matrix with at least two rows and columns, it is taken as a two-dimensional contingency table: the entries of x must be non-negative integers.\n\nAha. Okay. So, we can’t just use the actual variables; we have to give chisq.test() a contingency table of the counts within each combination of categories. The hint points us to the table() function, which we haven’t covered before because it doesn’t produce nicely formattable output. What it DOES do, however, is produce exactly the contingency table we want, which we can then pipe on to chisq.test().\n\nanx_scores_data |&gt; \n  dplyr::filter(gender %in% c(\"female\", \"male\")) |&gt; \n  dplyr::select(gender, stars_help_cat) |&gt; \n  table() |&gt; \n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(dplyr::select(dplyr::filter(anx_scores_data, gender %in%     c(\"female\", \"male\")), gender, stars_help_cat))\nX-squared = 3.1266, df = 1, p-value = 0.07702\n\n\nThis code has a few extra steps, but allows us to avoid creating a specialised sub-copy of our dataset! Personally I’d prefer this version, but we teach UG students the vector version as it’s slightly more straightforward.\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Report the results of the \\(\\chi^2\\) test in full, including a check of whether the expected frequencies are all greater than 5.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s get the latter out of the way first. Under Examples, the help documentation shows us right at the top how to get expected frequencies: if we save our test output in an object, we can subset it. Let’s have a look:\n\nanx_gender_chisq &lt;- chisq.test(anx_scores_data_chisq$gender, anx_scores_data_chisq$stars_help_cat)\n\nanx_gender_chisq$expected\n\n                            \nanx_scores_data_chisq$gender      high       low\n                      female 156.78534 165.21466\n                      male    29.21466  30.78534\n\n\nIf we don’t want to rely on visual checks (usually wise), we can ask R to check if there’s an issue with expected frequencies:\n\nall(anx_gender_chisq$expected &gt; 5)\n\n[1] TRUE\n\n\nGreat, we can then go on to report the results! Let’s save the papaja::apa_print() output in an object, which we can then use in our inline code.\n\nchisq_out &lt;- papaja::apa_print(anx_gender_chisq, n = nrow(anx_scores_data_chisq))\n\n\nA \\(\\chi^2\\) test of association revealed no significant association relationship between gender and STARS Asking for Help anxiety (`r chisq_out$full_result[[1]]`).\n\nWhich will appear when rendered as:\n\nA \\(\\chi^2\\) test of association revealed no significant association relationship between gender and STARS Asking for Help anxiety (\\(\\chi^2(1, n = 382) = 3.13\\), \\(p = .077\\))."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#raincloud-and-violin-plots",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#raincloud-and-violin-plots",
    "title": "07: Visualisations",
    "section": "Raincloud and Violin Plots",
    "text": "Raincloud and Violin Plots\nFor this section, we’ll focus on making nice raincloud plots, which we teach repeatedly in UG Year 1. For both of these plots, we’re going to expand our {ggplot} vocab by explicitly including a y-axis variable.\nRaincloud plots are introduced in the second half of first year for Sussex UGs. I love them especially because they pack in so much useful information in a reasonably easy-to-read plot. We teach UGs to read them and practice them several times, including in an assessment. Despite their complexity, they are extremely easy to create thanks to the recently-released {ggrain} package that does most of the heavy lifting for you.\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the instructions below to build your own plot as we go. Feel free to tweak details like colour and labels as you like.\n\n\nLet’s start by creating a basic raincloud plot of STARS Test subscale score split up by MCQ group. As we already covered above, I’m going to clean up the plot a bit with some scale formatting and a theme.\n\n1anx_scores_data |&gt;\n2  ggplot(aes(x = mcq, y = stars_test_score)) +\n3  geom_rain() +\n4  scale_x_discrete(name = \"MCQ Condition\", labels = c(\"Maths\", \"Stats\")) +\n5  scale_y_continuous(name = \"Mean STARS Test Score\") +\n6  papaja::theme_apa(box = TRUE)\n\n\n1\n\nTake the dataset anx_scores_data, and then\n\n2\n\nSet up a plot, mapping mcq to the x-axis and stars_test_score to the y-axis, plus\n\n3\n\nDraw a raincloud plot, plus\n\n4\n\nAdjust the discrete x axis by giving it a name, and relabelling the group labels, plus\n\n5\n\nAdjust the continuous y axis by giving it a name, plus\n\n6\n\nAdd an APA-style theme.\n\n\n\n\n\n\n\nThe default plot has three parts. From left to right, they are: a scatter of the data points; a boxplot; and a density plot of the data, turned on its side. If you tilt your head to the right, you can see that this looks a bit like a (very lumpy) cloud with the data falling like rain underneath.\nWe already have a good amount of information here, but one thing we are missing is the means in each group. We introduce raincloud plots in the context of t-tests, where the difference in group means is the key element of interest, so let’s add them to our plot, along with some colour adjustments.\n\nCalculating Stats\nTo add means and CIs to our plot, we’ll need to calculate them. However, there’s no need to create a summary first - we can use a stat_*() function to calculate the necessary statistics inside our plot.\nIn this case, we can use stat_summary() to produce some summary statistics. To really make it quick, we’re going to quote the function mean_cl_boot, which calculates means and bootstrapped CIs. That’s it - just that one line does all the maths and adds the result to the plot. However, the new element will show up as black by default, so let’s also add in a colour to make sure we can actually see it. (I’ve chosen something obnoxious to make it easy to find!)\n\n1anx_scores_data |&gt;\n2  ggplot(aes(x = mcq, y = stars_test_score)) +\n3  geom_rain() +\n4  scale_x_discrete(name = \"MCQ Condition\", labels = c(\"Maths\", \"Stats\")) +\n5  scale_y_continuous(name = \"Mean STARS Test Score\") +\n6  stat_summary(fun.data = mean_cl_normal, colour = \"red\") +\n7  papaja::theme_apa(box = TRUE)\n\n\n1\n\nTake the dataset anx_scores_data, and then\n\n2\n\nSet up a plot, mapping mcq to the x-axis and stars_test_score to the y-axis, plus\n\n3\n\nDraw a raincloud plot, plus\n\n4\n\nAdjust the discrete x axis by giving it a name, and relabelling the group labels, plus\n\n5\n\nAdjust the continuous y axis by giving it a name, plus\n\n6\n\nCalculate a summary of the data using the function mean_cl_normal for means and CIs, and add to the plot in red, plus\n\n7\n\nAdd an APA-style theme.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIn second year, students are also introduced to violin plots. Adapt the code we’ve already written for raincloud plots to create a violin plot with means and CIs instead.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is literally only a case of changing geom_rain() to geom_violin() - everything else works as is. This helps illustrate the use of the geom_*() functions for determining the kind of plot you have.\n\nanx_scores_data |&gt;\n  ggplot(aes(x = mcq, y = stars_test_score)) +\n  geom_violin() + # Only this line is different!\n  scale_x_discrete(name = \"MCQ Condition\", labels = c(\"Maths\", \"Stats\")) +\n  scale_y_continuous(name = \"Mean STARS Test Score\") +\n  stat_summary(fun.data = mean_cl_normal, colour = \"red\") +\n  papaja::theme_apa(box = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: The raincloud plot we produced above was serviceable but not very aesthetic. See if you can adapt that code to reproduce the plot below.\nHints: Run vignette(\"ggrain\") in the Console for a friendly tour of the options for raincloud plots. The colours are all from the Sussex colour palette.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis would have involved quite a bit of experimentation. If you made something like this, well done.\n\nanx_scores_data |&gt;\n  ggplot(aes(x = mcq, y = stars_test_score, fill = mcq)) +\n  geom_rain(point.args = list(alpha = .4, aes(colour = mcq))) +\n  stat_summary(fun.data = mean_cl_normal,\n               shape = 23) +\n  scale_x_discrete(name = \"MCQ Condition\", labels = c(\"Maths\", \"Stats\")) +\n  scale_y_continuous(name = \"Mean STARS Test Score\") +\n  scale_fill_manual(values = c(\"#f2c75c\", \"#007a78\")) +\n  scale_colour_manual(values = c(\"#dc582a\", \"#003b49\")) +\n  guides(fill = 'none', color = 'none') +\n  coord_flip() +\n  papaja::theme_apa(box = TRUE)"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#scatterplots",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#scatterplots",
    "title": "07: Visualisations",
    "section": "Scatterplots",
    "text": "Scatterplots\nLet’s now turn to continuous variables only and produce a scatterplot to visualise a linear relationship.\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the instructions below to build your own plot as we go. Feel free to tweak details like colour and labels as you like.\n\n\nTo start, let’s set up a basic plot using the STICSA pre- and post-MCQ state anxiety scores. The new element is geom_point(), which draws points 4. In that layer, we also have two new arguments. The first is alpha, which sets the transparency of the points between 1 (solid) and 0 (invisible). The second is position = \"jitter\", which introduces some random noise into the placement of the points so make it easier to see overlapping values.\n\n1anx_scores_data |&gt;\n2  ggplot(aes(x = sticsa_pre_state_score, y = sticsa_post_state_score)) +\n3  geom_point(alpha = 0.4, position = \"jitter\") +\n4  scale_x_continuous(name = \"STICSA Pre-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n5  scale_y_continuous(name = \"STICSA Post-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n6  papaja::theme_apa(box = TRUE)\n\n\n1\n\nTake the dataset anx_scores_data, and then\n\n2\n\nSet up a plot, mapping sticsa_pre_state_score to the x-axis and sticsa_post_state_score to the y-axis, plus\n\n3\n\nDraw points, with 40% transparency and some random noise to separate them, plus\n\n4\n\nAdjust the continuous x axis by giving it a name, and setting the limits to 1 and 4, plus\n\n5\n\nAdjust the continuous x axis by giving it a name, and setting the limits to 1 and 4, plus\n\n6\n\nAdd an APA-style theme.\n\n\n\n\n\n\n\nNot bad, eh? Right off the bat we get a very passable scatterplot. Now, assuming that this scatterplot might accompany a linear model, we might also like to draw that model on our plot.\n\nLine of Best Fit\nUnfortunately the function to add a line of best fit isn’t as intuitively named as you might expect. Both geom_smooth() and stat_smooth() will do what we want here; in first year, we teach UGs geom_smooth() for this, so we’ll use that here too. (There is a geom_line() but isn’t quite the same!)\nAs we did with the histogram previously, we can add colour and fill arguments to adjust the colour. Here, colour determines the colour of the line, and fill determines the colour of the shaded confidence intervals.\n\nanx_scores_data |&gt;\n  ggplot(aes(x = sticsa_pre_state_score, y = sticsa_post_state_score)) +\n  geom_point(alpha = 0.4, position = \"jitter\") +\n1  geom_smooth(method = \"lm\",\n              colour = \"darkblue\",\n              fill = \"darkcyan\") +\n  scale_x_continuous(name = \"STICSA Pre-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  scale_y_continuous(name = \"STICSA Post-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  papaja::theme_apa(box = TRUE)\n\n\n1\n\nAdd a line of best fit using the “lm” function, with a dark blue line and dark cyan CIs.\n\n\n\n\n\n\n\n\n\nColour and Fill Palette\nIn the previous Colour and Fill section we chose colours by name or hex code to add to a plot. However, there are a wide variety of pre-made colour palettes to make use of. So, instead of choosing individual colours, you can quickly apply existing colour palettes.\n\n\n\n\n\n\nColour Palette Options\n\n\n\nLooking for the right colour scheme? Try these resources:\n\nThis blog post on R colour palettes is a great overview of some of the most popular options available, with lots of examples.\nThe {paletteer} package for quick access to any R colour palette and to an overview of all the palettes available\n\n\n\nA popular palette and the one we’ll use for this task is from the lovely {viridis} colour palette package, designed to be pretty, print-friendly, and accessible (robust to colourblindness).\nTo use the palettes, we start out by dropping the colour and fill arguments from within geom_smooth(), and instead setting colour and fill arguments at the beginning of the plot, in the aes() function along with x and y. This will apply the same colour palette across all elements in the plot that can have colour - namely, the points from geom_point(), and the line of best fit from geom_smooth(). By setting them both to mcq, the colours will be consistent across MCQ types between all of these elements. Incidentally, doing this will also split our single LM line into two lines: one for each level of the mcq variable.\n\nanx_scores_data |&gt;\n1  ggplot(aes(x = sticsa_trait_score, y = sticsa_post_state_score,\n             colour = mcq, fill = mcq)) +\n  geom_point(alpha = 0.4, position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(name = \"STICSA Pre-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  scale_y_continuous(name = \"STICSA Post-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  papaja::theme_apa(box = TRUE)\n\n\n1\n\nSet up the plot, mapping sticsa_pre_state_score to the x-axis and sticsa_post_state_score to the y-axis, and both colour and fill to mcq.\n\n\n\n\n\n\n\nA start - but let’s do something about that horrible default colour palette!\nJust as we had scale_x... and scale_y... functions for specifying the x and y scales, we have scale_colour... and scale_fill... for specifying the details about the way the colour and fill appears. Here, I’m using viridis, which has its own dedicated functions, scale_colour_viridis() and scale_fill_viridis()5. Within those two new elements, we have the same two elements:\n\nname: gives a title for the colour/fill legend\ndiscrete: tells {viridis} whether the variable to be coloured/filled is discrete or not. Here it is (it’s mcq, a categorical variable), so we set this to TRUE.\nlabels: applies labels to the levels of the variables.\n\n\nanx_scores_data |&gt; \n  ggplot(aes(x = sticsa_trait_score, y = sticsa_post_state_score,\n             colour = mcq, fill = mcq)) +\n  geom_point(alpha = 0.4, position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(name = \"STICSA Pre-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  scale_y_continuous(name = \"STICSA Post-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n1  scale_colour_viridis(name = \"MCQ Type\",\n                       discrete = TRUE,\n                       labels = c(\"Maths\", \"Stats\")) +\n  scale_fill_viridis(name = \"MCQ Type\",\n                     discrete = TRUE,\n                     labels = c(\"Maths\", \"Stats\")) +\n  papaja::theme_apa(box = TRUE)\n\n\n1\n\nAdjust the fill and colour by applying the viridis colour palette, giving each a name, using discrete values, and relabelling the categories.\n\n\n\n\n\n\n\nThat’s looking fairly nice now!\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: In the code above, I had to repeat a lot of elements for both fill and colour. See if you can figure out how to remove the need to change the name and labels within the plotting code by making a change to the dataset beforehand.\nHint: You might try a non-standard variable name with backticks!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn order to remove some of the near-identical code above, I can first make a change to my dataset before I start the plot. Here, I’m using mutate() to create a new variable, MCQ Type. This variable contains exactly the same information as mcq, except that the values have a capitalised first letter. Essentially, I’m removing the need to reformat later by formatting the data before it’s plotted.\nThe only thing I then have to do is swap out mcq for MCQ Type as my colour and fill variables. Notice that the backticks are essential here, in both mutate() and aes(), otherwise the space in the variable name will throw an error.\n\nanx_scores_data |&gt;\n  dplyr::mutate(\n    `MCQ Type` = stringr::str_to_title(mcq)\n  ) |&gt; \n  ggplot(aes(x = sticsa_trait_score, y = sticsa_post_state_score,\n             colour = `MCQ Type`, fill = `MCQ Type`)) +\n  geom_point(alpha = 0.4, position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(name = \"STICSA Pre-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  scale_y_continuous(name = \"STICSA Post-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  scale_colour_viridis(discrete = TRUE) +\n  scale_fill_viridis(discrete = TRUE) + \n  papaja::theme_apa(box = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n3D Plots\n\n\n\n\n\n\nWarning\n\n\n\nNote: 3D plots really should be used with caution, and this bit is just for fun - feel free to skip if you’re not inclined. If you’re keen to have a go, the code below gets you an interactive, useable 3D scatterplot in no time at all, but think carefully about whether this is really the best way to represent your data.\n\n\nIn something of a departure for us, we’re going to use a package that isn’t {ggplot2} for this next bit. (Shock! Horror!) Instead, we’ll use the plot3d() function from the {rgl} package to get a snazzy interactive 3D-plot that does rotation and zoom right out of the box. The syntax might look slightly unfamiliar, but it’s got the same underlying idea as what we’ve seen so far.\nIn order to have points coloured by MCQ type, I’ve created a new variable in the dataset assigning colours based on the values of the mcq variable. This and the rest of the code is borrowed straight from R Graph Gallery’s very helpful mini-tutorial on {rgl}.\n\n# Add a new column with color\nanx_scores_data &lt;- anx_scores_data |&gt; \n  dplyr::mutate(\n    plot_colours = ifelse(mcq == \"maths\", \"orange\", \"darkcyan\")\n  )\n\nNext, we’ll use the rgl::plot3d() function to generate the plot. The x, y, and z arguments each are specified using $ subsetting, and the colours in col come from the new variable I created just above. The rest of the arguments change the appearance of the points (type and size) and set the axis labels.\n\nrgl::plot3d(\n  x = anx_scores_data$sticsa_trait_score, \n  y = anx_scores_data$sticsa_post_state_score,\n  z = anx_scores_data$sticsa_pre_state_score,\n  col = anx_scores_data$plot_colours,\n  type = \"s\",\n  radius = .05,\n  xlab = \"STICSA Trait\", ylab = \"STICSA Post State\", zlab = \"STICSA Pre State\"\n)"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#reporting-with-quarto",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#reporting-with-quarto",
    "title": "07: Visualisations",
    "section": "Reporting with Quarto",
    "text": "Reporting with Quarto\n\n\n\n\n\n\nTip\n\n\n\nAs ever, I strongly recommend referring to the Quarto Guide as the first stop for possibilities and/or problem-solving with Quarto.\n\n\nIn a previous section, we created a lovely scatterplot for a (hypothetical) linear model. Let’s see now how we could adjust the way that figure would appear in a Quarto document.\nTo begin, I’ve stored the final output for the LM plot we created above in a new object, mcq_lm_plot, to make the subsequent sections easier to read.\n\nmcq_lm_plot &lt;- anx_scores_data |&gt;\n  ggplot(aes(x = sticsa_trait_score, y = sticsa_post_state_score,\n             colour = mcq, fill = mcq)) +\n  geom_point(alpha = 0.4, position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(name = \"STICSA Pre-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  scale_y_continuous(name = \"STICSA Post-MCQ State Anxiety Score\",\n                    limits = c(1, 4)) +\n  scale_colour_viridis(name = \"MCQ Type\", discrete = TRUE, labels = c(\"Maths\", \"Stats\")) +\n  scale_fill_viridis(name = \"MCQ Type\", discrete = TRUE, labels = c(\"Maths\", \"Stats\")) +\n  papaja::theme_apa(box = TRUE)\n\nThen, we’re just going to put our plot object in a new code chunk, in the document where we want it to appear. Equally, this could be the code that produces the plot; it doesn’t matter either way.\n```{r}\nmcq_lm_plot\n```\nTo change or add settings for a specific plot, we will use code chunk options, which we previously encountered in Tutorial 04. These options let us determine the way the code chunk behaves. Here, we’ll also see how we can use those code chunk options to style and format our plot.\n\nCaptions and Alt Text\nFirst, we might add a caption to the plot, using the fig-cap option. You may have noticed that we didn’t give our plot a caption/title previously, and this is why. You can of course write whatever you like, but here’s an example.\n```{r}\n#| fig-cap: \"Scatterplot of mean pre- and post-MCQ state anxiety scores\"\n\nmcq_lm_plot\n```\n\n\n\n\n\nScatterplot of mean pre- and post-MCQ state anxiety scores\n\n\n\n\nYou should also add alt text to your figures to make them readable by screenreaders and other assistive technology. It is important to describe the insights from the visualisations clearly, such as the key patterns, connections, or findings, to ensure they are accessible. This Medium article on scientific alt text gives some clear guidelines for keeping alt text informative, brief, and useful.\nWe can add alt text to our figure using the fig-alt option:\n```{r}\n#| fig-cap: \"Scatterplot of mean pre- and post-MCQ state anxiety scores\"\n#| fig-alt: \"Scatterplot of mean pre- and post-MCQ state anxiety scores showing a positive relationship, with separate linear regression lines for the two MCQ types both also showing a very similar positive relationship in both conditions\"\n\nmcq_lm_plot\n```\n\n\nCross-Referencing\nAs with tables, we can also include automatic cross-referencing for figures in our document. This means that Quarto will automatically number our figures and include links to those figures in the text of the document.\nTo do this, let’s first give our figure a label. Labels are essentially names for this particular figure that will allow us to refer to it in the text. In order to be recognised for cross-referencing, the label must start with the fig- prefix.\n```{r}\n#| label: fig-mcq-lm\n#| fig-cap: \"Scatterplot of mean pre- and post-MCQ state anxiety scores\"\n\nmcq_lm_plot\n```\nThen, use this label with an @ symbol in the text of the document to generate the cross-reference. For instance, I could write:\n\nThe result of this analysis is visualised in @fig-mcq-lm.\n\nWhich would appear when rendered as below - notice the caption now has “Figure 1” added to it to correspond to the in-text numbering.\n\nThe result of this analysis is visualised in Figure 1.\n\n\n\n\n\n\nFigure 1: Scatterplot of mean pre- and post-MCQ state anxiety scores\n\n\n\n\n \nVery well done on your hard work so far. We’ll keep using our {ggplot2} chops when we dig into analysis next week, and see how we can easily create plots to accompany those analyses using built-in functions."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html#footnotes",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html#footnotes",
    "title": "07: Visualisations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou may notice a certain commonality to the colour theme of this website!↩︎\nYes, this is not technically true because the values can only be whole numbers between 0 and 5, but what matters here is that R thinks this is continuous data.↩︎\nSee e.g. Shaki and Fischer, 2008↩︎\nTragically, geom_scatter() isn’t a thing, not matter how many times I try!↩︎\nNaturally, if you want to use palettes from a different package, you’ll need to use the functions from that package instead! Those functions may also work a bit differently, so make sure you read the help documentation if you get stuck.↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/09_testflight.html",
    "href": "tutorials/psychrlogy/03_improvRs/09_testflight.html",
    "title": "09: Test Flight",
    "section": "",
    "text": "Today’s tutorial is a bit different - less tutorial and more supported practice. Instead of more new information, today’s session is an opportunity to try out the skills we’ve been working on.\nWe’ll look at a new dataset and work through the process of inspecting, cleaning, summarising, visualising, analysing, and reporting. Rather than solutions, this tutorial will support you to do these tasks yourself - but writing the code is up to you!"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/09_testflight.html#overview",
    "href": "tutorials/psychrlogy/03_improvRs/09_testflight.html#overview",
    "title": "09: Test Flight",
    "section": "",
    "text": "Today’s tutorial is a bit different - less tutorial and more supported practice. Instead of more new information, today’s session is an opportunity to try out the skills we’ve been working on.\nWe’ll look at a new dataset and work through the process of inspecting, cleaning, summarising, visualising, analysing, and reporting. Rather than solutions, this tutorial will support you to do these tasks yourself - but writing the code is up to you!"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/09_testflight.html#setup",
    "href": "tutorials/psychrlogy/03_improvRs/09_testflight.html#setup",
    "title": "09: Test Flight",
    "section": "Setup",
    "text": "Setup\nThere’s no workbook this week, because we’re emulating the process of doing a data analysis from start to finish in R. Instead, create a new Quarto document to work in.\nMake sure you start out by loading any necessary packages, minimally {tidyverse}.\n\nData\nToday’s data comes from Body Positivity, but not for everyone (Simon & Hurst, 2021), which has publicly available data hosted on Figshare (h/t Dr Hurst for this!). You can certainly use the data directly from Figshare, but for the purposes of this tutorial, I’ve prepared a subset of the data that you can load in using the code below.\nRead in from file:\n\nbp_data &lt;- here::here(\"data/bp_data.csv\") |&gt; readr::read_csv()\n\nRead in from URL:\n\nbp_data &lt;- readRDS(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/bp_data.rds\")\n\n\nCodebooks\nThere are two separate codebooks for this dataset. The first describes the demographic and other single-item variables in the dataset. The second describes the variables that are items belonging to measures, and which items belong to what measures.\n\n\n\n\n\n\nImportant\n\n\n\nAs in previous datasets we’ve used, I’ve introduced some changes to the data to mess it up a bit and give us opportunities to practice. If you want the real dataset, make sure you use the links above!\n\n\n\n\n\nDemographics and Single-Item Variables\n\n\nVariable Name\nType\nItem\nResponse Options\n\n\n\n\nrecordeddate\ndate\nRecorded date\nNA\n\n\nid\ncharacter\nUnique participant id number\nAlphanumeric identifier (three letters, four numbers)\n\n\ngender\nfactor\nWhat gender do you identify with?\nFemale, Male\n\n\nage\nfactor\nWhich age bracket do you fall under?\n18-29, 30-39, 40-49, 50+\n\n\nheight\nnumeric\nHeight (in m)\nRange: 1.49 - 1.9\n\n\nweight\nnumeric\nWeight (in kg)\nRange: 44 - 158.75\n\n\ninstagram\nfactor\nDo you have an instagram account?\nYes, No\n\n\ncondition\nfactor\nWhether participants viewed a body-positivity post featuring an average-sized model, larger model, or a control image about travel\nControl, BPaverage, BPlarger\n\n\nchoice_perc\nnumeric\nPercentage of healthy picks\nRange: 0 - 100\n\n\n\n\n\n\n\n\n\n\nScale Items\n\n\nScale\nFull Name\nSubscale\nVariable Prefix\nItem Numbers\nItems\nResponse Scale\n\n\n\n\nPANAS\nPositive and Negative Affect Scale\nPositive\npanas\n1, 3, 5, 9, 10, 12, 14, 16, 17, 19\nInterested, Excited, Strong, Enthusiastic, Proud, Alert, Inspired, Determined, Attentive, Active\nVery Slightly/Not at all, A little, Moderately, Quite a bit, Extremely\n\n\nPANAS\nPositive and Negative Affect Scale\nNegative\npanas\n2, 4, 6, 7, 8, 11, 13, 15, 18, 20\nDistressed, Upset, Guilty, Scared, Hostile, Irritable, Ashamed, Nervous, Jittery, Afraid\nVery Slightly/Not at all, A little, Moderately, Quite a bit, Extremely\n\n\nBSS\nBody Satisfaction Scale (State)\nSingle scale\nbss\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17\nWhole Body, Head, Face, Jaw, Teeth, Nose, Mouth, Ears, Eyes, Shoulders, Neck, Chest, Tummy, Arms, Hands, Legs, Feet\nVery dissatisfied, Moderately dissatisfied, Slightly dissatisfied, Undecided, Slightly satisfied, Moderately satisfied, Very satisfied\n\n\nSAC\nSocial Comparison\nSingle scale\nsac\n1, 2, 3, 4\nTo what extent did you think overall about your appearance when viewing these images?To what extent did you compare your overall appearance to the individuals in the Instagram images?To what extent did you compare your stomach to the individuals in the Instagram images?To what extent did you compare your thighs to the individuals in the Instagram images?\nNot at all, 2, 3, 4, A lot\n\n\nBAS\nBody Appreciation Scale (Trait)\nSingle scale\nbas\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10\nI respect my bodyI feel good about my bodyI feel that my body has at least some good qualitiesI take a positive attitude toward my bodyI am tentative to my body's needsI feel love for my bodyI appreciate the different and unique characteristics of my bodyMy behaviour reveals my positive attitude toward my body: eg. I walk holding my head high and smilingI am comfortable in my bodyI feel like I am beautiful even if I am different from media images of attractive people eg. models, actresses\nNever, Seldom, Sometimes, Often, Always\n\n\nPACSR\nPhysical Appearance Comparison Scale, Revised\nSingle scale\npacsr\n1, 2, 3, 4\nWhen I meet a new person (same sex), I compare my body size to their body size.When I am out in public, I compare my body fat to the body fat of others.When I am at a party, I compare my body shape to the body shape of others.When I am out in public, I compare my body size to the body size of others\nNever, Sometimes, About half the time, Most of the time, Always"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/09_testflight.html#data-analysis-tips",
    "href": "tutorials/psychrlogy/03_improvRs/09_testflight.html#data-analysis-tips",
    "title": "09: Test Flight",
    "section": "Data Analysis Tips",
    "text": "Data Analysis Tips\n\nPlan Your Process\nThe headings in the next section provide a basic outline of the steps to help you progress through the steps of the analysis, along with some suggestions about things to check or do at each step. However, you may want to add steps, or find that others aren’t necessary. For your own data analysis, it’s a good idea to have a clear idea of what you are try to achieve overall and at each step, so you know when you’ve done it successfully!\n\n\nKeep a Record\nToday it’s up to you to create your own code chunks, make your own notes, and write your own report. As you go, you’ll be writing a continuous script across multiple code chunks. For this code to work as expected, it’s essential that you keep a record of each bit of code that contributes to the final output.\nHowever, for future use, it’s almost as important to clearly note the code that didn’t work, or that was only for the purpose of testing or understanding your code rather than that actually contributes to the analysis process. For this, make liberal use of # comments. You can add comments to your code, and also “comment out” code that was useful at the time but was only for you at the time.\nIn the example below, I’ve made a comment, preceded by ##. This is my personal convention - just one # will do, but I use two (or more) to visually distinguish from commented-out code. That’s the next two lines: a bit of code that I might have used to check whether my exclusions or mutate commands worked as expected. This code isn’t a part of the main tasks I want to do with my code, so I don’t need this to print out when I run the code in the future; but it’s handy to have it, so I know I checked and how, and so if I need to check again, I have the code easy to hand.\n\n## Checking whether the counts came out\n\n# data |&gt; \n#   dplyr::count(condition)\n\n\n\n\n\n\n\nTip\n\n\n\nTo easily comment out any bit of code, highlight (select) it with your cursor and press Ctrl/Cmd + Shift + C on your keyboard.\n\n\n\n\nBe Consistent\nThis tutorial is chance to work on some skills that you might not have had a lot of practice with yet, like creating and naming objects, managing your environment, and keeping track of what you have done. You may find this confusing or frustrating at times - don’t worry, that’s normal, and will get much easier with practice.\nHowever, you can make things easier from the start by thinking strategically about how to manage your code and objects. The key is to be consistent, in a variety of respects.\n\nVariable and object names\nDecide on a consistent convention for naming objects, and variables/elements within objects. Thus far we’ve been using snake_case, but you can of course use another convention of your choice; I’d recommend avoiding a_mixOf.cases\nConsider using suffixes, such as _data for datasets, _tab for tables, _lm, _afx for models, _n for objects containing counts, etc.\n\n\nPackages and functions\nDecide whether you use explicit style (i.e. package::function()) or not. Do you need to load more packages to make sure all your functions work, if you are not using explicit style?\n\n\nOverwriting vs. creating an object\nThis is one of the more nuanced elements of managing your environment. When you make many changes to a dataset, consider how many you may need to make before you’ve made something “different”. It may help to think of creating a new object as a “checkpoint” that you can return to easily.\n\n\n\nReduce Redundancies\nOnce you have some hands-on practice, you can start to look for opportunities to make your code more resilient to errors, more efficient, and more versatile. Keep an eye out for places where your code repeats or is identical, and/or you had to copy-paste multiple times; you might want to use a more efficient iteration function, or write your own function.\nSimilarly, watch out for places where you, the individual, have to remember to do something - like add or delete elements, do a calculation, or update the code - in order for it to work. These are places where you might consider instead using an object instead of hard-coded value, so you can easily update your code later."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/09_testflight.html#the-whole-shebang",
    "href": "tutorials/psychrlogy/03_improvRs/09_testflight.html#the-whole-shebang",
    "title": "09: Test Flight",
    "section": "The Whole Shebang",
    "text": "The Whole Shebang\nThe following sections are intended to prompt you through the whole process of inspecting, cleaning, wrangling, summarising, visualising, analysing, and reporting results. You will notice there are no solutions! All of the exercises (and hints) are suggestions, but you’re welcome to make use of this dataset and workbook however is helpful for you.\nRemember, rather than working in a workbook, you should create your own Quarto document and work in that file, creating code chunks as necessary.\n\nPlanning\nThe independent manipulation in this experiment was the image condition: whether participants saw an average-sized or larger-sized model in a body positivity post, or a control travel image. A key outcome was the selections participants subsequently made from an example menu, with percentage of healthy picks provided in the dataset.\nPercentage of healthy picks would be a sensible outcome variable to investigate, but you are welcome to choose something else. You may want to choose a couple categorical variables - for example, condition, age, and gender - along with a couple measures of your choice.\n\n\nInspecting\nStart by getting familiar with the overall dataset. Have a look at it, and look through the codebook. Is everything in order? Are there any issues?\nCreate some summaries of the variables in the dataset. Are there any potential problems that need addressing?\n\n\nCleaning\nMake any changes necessary to clean the data. Remember to keep track of any exclusions that you need to report.\n\n\nWrangling\nChange or create any new variables. Minimally, you will need to create mean scores for each of the multi-item measures, making use of the Codebook to do so.\n\n\nSummarising\nCreate at least one nicely formatted summary table describing the variables of interest, split up by (at least) experimental condition. Feel free to add more if you like.\n\n\nAnalysing\nChoose an analysis to perform and do it.\n\n\n\n\n\n\nTip\n\n\n\nRemember you can always refer to the discovr tutorials for more help!\n\n\n\n\nVisualising\nChoose a visualisation that reflects the key comparison or relationship of interest that you investigated in your analysis, and create it. Remember that you can build a visualisation from scratch with {ggplot2}, or use one of the “shortcut” plotting functions we have covered.\n\n\nReporting\nWrite up a formal report of the analysis from start to finish, including:\n\nA statement of the key comparison or relationship of interest\nA description of relevant data cleaning procedures, including any exclusions\nA nicely formatted descriptive table\nA full report and interpretation of the analysis\nAn accompanying visualisation"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html",
    "title": "11: Qualtrics II",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to reduce data management headaches down the line by setting up a well-structured and thoroughly labelled Qualtrics survey from the outset.\n\n\nThis tutorial was co-conceived and co-created with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. This included collecting commonly asked questions and issues with Qualtrics data analysis; discussing the topics to cover and how best to cover them; and testing code and solutions. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#overview",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#overview",
    "title": "11: Qualtrics II",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to reduce data management headaches down the line by setting up a well-structured and thoroughly labelled Qualtrics survey from the outset.\n\n\nThis tutorial was co-conceived and co-created with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. This included collecting commonly asked questions and issues with Qualtrics data analysis; discussing the topics to cover and how best to cover them; and testing code and solutions. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#setup",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#setup",
    "title": "11: Qualtrics II",
    "section": "Setup",
    "text": "Setup\n\nPackages\nAs usual, we will be using {tidyverse}. When {tidyverse} is installed, it also installs the {haven} package, which we will use for data importing. However, {haven} isn’t loaded as part of the core {tidyverse} group of packages, so let’s load it separately. Finally, we will also need the {labelled} package to work with labelled data.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(sjlabelled)\nlibrary(sjPlot)\n\n\n\n\n\n\n\n\nData\nToday’s dataset focuses on various aspects of meaning in life (MiL), and has been randomly generated based on a real dataset kindly contributed by Hanna Eldarwish and Vlad Costin. All variables have been randomly generated, but they are based on the patterns in the original dataset. The original, bigger dataset will be made available alongside article publication in the future, so keep an eye out for it!\n\n\n\n\n\n\nNew File Type\n\n\n\nYou might notice that instead of the familiar readr::read_csv(), today we have haven::read_sav(). That’s because the file I’ve prepared is a SAV file, associated with the SPSS statistical analysis programme. The next section explains why we are using this data type, but otherwise, there’s nothing new about these commands.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRead in the mil_data.sav object from the data folder, or alternatively from Github via URL, as you prefer.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom a folder:\n\nmil_data &lt;- here::here(\"data/mil_data.sav\") |&gt; haven::read_sav()\n\nFrom URL:\n\nmil_data &lt;- haven::read_sav(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/mil_data.sav\")\n\n\n\n\n\n\n\nCodebook\nThis codebook is intentionally sparse, because we’ll be generating our own from the dataset in just a moment. This table covers only the questionnaire measures to help you understand the variables.\n\n\n\n\n\nVariable Prefix\nScale\nSubscale\n\n\n\n\nglobal_meaning\nMeaning in Life\nGlobal Meaning\n\n\nmattering\nMeaning in Life\nMattering\n\n\ncoherence\nMeaning in Life\nCoherence\n\n\npurpose\nMeaning in Life\nPurpose\n\n\nsym_immortality\nSymbolic Immortality\nSingle scale\n\n\nbelonging\nBelonging\nSingle scale"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#building-a-qualtrics-project",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#building-a-qualtrics-project",
    "title": "11: Qualtrics II",
    "section": "Building a Qualtrics Project",
    "text": "Building a Qualtrics Project\nThe following sections contain not one, not two, but three different ways to do the same thing. Why? Well, at this point you probably have a good sense of how much you like or want to use R, and how confident you feel using tricks and shortcuts, like &lt;tidyselect&gt;, across(), etc. I’m sure you also have a sense of how I feel about this: I will pretty much always choose the method that is the most elaborate and efficient in the long run, even (especially??) if it means more work in R to get it set up. However, I fully appreciate that not everyone is like me, and would prefer the more transparent, less intensive options in order to get the job done.\nSo, I’ve described three ways to set up a Qualtrics project. First, by “set up a Qualtrics project”, I mean creating everything you would need for the data gathering, managing, and organising side of running a study using Qualtrics. That includes setting up Qualtrics itself and cleaning and reviewing the dataset ahead of analysis. This doesn’t include the data processing and analysing portion of the study - that’s been covered extensively in previous tutorials.\nI also hope that this description will give you a sense of what these different R-intensity levels look like, and how much of an investment each is. Do keep a few important things in mind as you’re exploring the options below:\n\nThese levels build on each other. Starting with “Minimal” might be where you’re at now, but it’s also a good idea, especially if these tutorials are your only experience with R so far! You can always come back and explore more complex options if you find you’ve mastered the first level and want to streamline your study process.\nI really, really don’t have a good sense of it, but I suspect that in a typical University school/department, not many people - maybe none - are doing the Majority option (described below). I looked into it for the purposes of writing this tutorial, so I don’t even use it myself (yet)! You might even find it doesn’t work as well as I’ve described, so let me know if you have any suggestions.\n\n\nChoose Your Path\nThe next sections will be organised as Minimal, Moderate, and Majority based on the approaches described below. Following the steps in each section will each walk you through the process of building and maintaining the data from a Qualtrics study, but you will come out with a different underlying infrastructure depending on which level you go for.\n\n\n\n\n\nR Level\nDuplication vs Efficiency\nAdvantages\nDisadvantages\n\n\n\n\nMinimal\nI want to get it done now and I'm not worried about having to do the same thing again in the future\nGet everything set up straightaway\nMay have to repeat fiddly steps, change small details, or redo large chunks of the same process for the next time round\n\n\nModerate\nI want to make it easier for myself in the future without getting bogged down in too much R\nLess work to reproduce down the line\nRequires more upfront time investment and some advanced R usage\n\n\nMajority\nI'm willing to spend more time setting things up now, so I can duplicate or redo as little as possible in the future\nSet up an efficient, replicable system that can be adapted for future projects\nMore time-intensive to create, requires more knowledge of R and/or willingness to experiment with new packages/structures\n\n\n\n\n\n\n\nMade your choice? Brill. The following sections of the tutorial will guide you through how to set up, download, and work with Qualtrics data for each level. Note that you can mix and match as you like, of course, but this tutorial isn’t designed that way.\n\n\nAccessing Qualtrics\nThe following directions walk you through logging into Qualtrics, assuming you have a Sussex login. If not (hi!!), you’ll need to get in touch with your own IT department if you’re not sure how to access Qualtrics.\nYou can start on the University of Sussex homepage, then use the Search function to search for “ITS Qualtrics”. Similarly, you can also start on Google1, and search for “ITS Sussex Qualtrics”. Either way, the first result should be the one you want: ITS Sussex’s Qualtrics Guide. There’s a friendly video and some info here, but the most useful stuff is at the bottom of the page, where there are instructions for logging in using your institutional ID.\nClicking on the big friendly “Log into Qualtrics” button under the video will get you to the home page of Qualtrics. Welcome!\n\n\nCreating a Project\nFrom the main landing page after login, click on the big blue button in the upper right-hand corner of the screen reading “Create project”.\nYou’re welcome to look through the templates that Qualtrics includes, but I always ignore them and click under “Survey” under the “From scratch” heading, then the blue “Get started” button on the sidebar. (There’s a link to the very helpful Survey Basics Overview page that it might be worth referring to if you’ve never worked on Qualtrics before.) Finally, you’ll see the “Create a new project” screen, asking you to name your project and put it in a folder. The name you give your survey under “Name” will be the name that your participants see as the title of your survey, so pick something that sounds professional! You can put your new survey in the default folder if you like, or in a folder if you’ve made one. Finally, click “Create project”."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#minimal",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#minimal",
    "title": "11: Qualtrics II",
    "section": "Minimal",
    "text": "Minimal\nAt the minimal-R level, we’re not going to worry about futureproofing any of these steps, essentially assuming this project is a one-off that just needs to be done ASAP. So, we’ll do everything “by hand” - that is, without any extra code or shortcuts.\n\nSet Up Qualtrics\nFollow the steps in the previous section to log in and create a survey first, then pick up from here once you have a new, blank survey to work with.\n\nUsing Blocks\nBlocks are the way that Qualtrics organises pieces of the survey. Essentially, everything in the same block becomes a unit. You can have multiple questions per block, or just one. Blocks are vital for creating a study that appears as you want, but they won’t have any substantial impact on the format of the data.\nExplaining blocks and how they can be arranged is a bit outside the scope of this tutorial, so see the Block Options page in the Qualtrics guide for more details.\n\n\nUsing Questions\nThe core of Qualtrics are questions, which you can create within blocks. By default, a new question is a multiple-choice question (MCQ), but you can customise this in depth in the “Edit question” sidebar to the left of the survey. To edit a question, you have to click on each question, which will outline the question in a blue box; you can then change the settings for that question in the sidebar.\n\n\n\n\n\n\nTip\n\n\n\nFor extensive help on creating and work with questions, see the Qualtrics Guide.\n\n\n\nQuestions in R\nLet’s have a look at the default question, which appears like this:\n\nAs you can see here, the way that you set up your questions translates directly into the way your dataset will appear.\n\nNames: All questions are automatically given a name, by default Q[number], e.g. Q1, Q2, etc. This question name will appear as the variable name in your exported dataset. These names are not visible to your participants.\nText: Question text is the actual question that your participants see. This question text will appear as the variable label in your exported dataset.\nChoices: For questions with a specific set of choices, like multiple-choice questions and rating scales, the choices you list here are the response options that your participants see. These choices will appear as the value labels in your exported dataset.\n\nYou may notice that there’s no evidence of the underlying numerical values for each choice. Although Qualtrics doesn’t make this immediately obvious, they are always worth checking, because sometimes they’re…creative. This doesn’t matter so much for questions that are going to become factors - whether the underlying number is 1 or 14 or 73 doesn’t matter because they’re just a marker for a unique category. However, we’ll see in a moment an example where it does matter, namely rating scales.\nTo check the values, click on the question, scroll down to the bottom of the Edit Question sidebar, and click on “x -&gt; Recode values”. This opens a new pop-up window where you can edit a few options:\n\nTick Recode Values to change the numeric values for each choice. These values are the underlying values that will appear as numbers in the dataset in R.\nTick Variable Naming to give different value labels to the choices than the ones the participants see. (Personally I’d be very wary of doing this, as it would be easy to lose track of what participants actually saw/responded to!)\n\n\nAs you can see from this simple “What’s your favourite pie?” question, these underlying numeric values can go wonky quickly. I have four options, “apple”, “cherry”, “pecan”, and “pumpkin”, which are numbered 1, 6, 2, and 3 respectively! What’s happened is that I created “apple”, “pecan”, and “pumpkin”, and then a couple other options; then I changed my mind, removed the other options (which would have been 4 and 5) and added “cherry” after “apple”. Values are assigned based on the order they are added, which is why the values came out weird and out of numerical order. If I wanted these to go in order (which isn’t a bad idea, since you want your data to be predictable), I can tick “Recode Values” and then manually enter the numeric values I want for each choice.\n\n\nMatrix Questions\nMatrix questions are very commonly used as an efficient way to present multiple questions or items with the same response scale - for example, items on a scale or subscale with a consistent Likert response scale.\nTo create one, create a “Matrix table” type question. The typical setup is for the items/questions to be presented down the left-hand side as “statements”, and the rating scale to be presented along the top as “scale points”.\nThe “Scale points” section of the Edit Question sidebar lets you control how these scale points appear. You can add or remove the number of points, and for many scales in Psychology, you can use suggested rating scales by switching the toggle on, which automatically insert labels for each scale point for you.\n\n\n\n\n\n\nTip\n\n\n\nRemember in the last tutorial, we could use labelled::unlabelled() to automatically convert variables to either factor or numeric. However, variables would be converted to numeric when they didn’t have labels for each value. If you use “suggested scale points” to automatically label the values, you’ll get a label for every value, so this automatic conversion won’t work unless you manually remove some of the labels.\n\n\nFor some reason, matrix tables are especially prone to issues with the underlying numeric values, especially if you use these automatic scale points. You’ll end up with really weird ranges, like 61-65, instead of 1-5, which will do a number on the interpretation of any descriptives. Even better, the numeric values change themselves every time you make changes to them! So, I’d strongly recommend you update the numeric values using “Recode values” as the last step to make sure you don’t have any surprises when you get round to looking at the data.\n\n\n\n\nTesting the Study\nBefore you publish your study and start collecting responses, I strongly recommend you check that everything has been set up correctly. To do this, use the Preview button to run through your study a few times as if you were a participant. This has two important benefits. First, you can check that you have set up your questions correctly, that they appear in the order you want or conditionally as required, and any settings, like Force Response, have been implemented correctly. Second, preview runs are still recorded as data in your dataset, under the “DistributionChannel” variable (as we’ve seen before). These preview runs essentially generate a small, toy dataset to work with that will be formatted identically to your real dataset.\n\nDo Your Worst\nWhile you’re previewing, I recommend that you try to “break” your survey. Try to type words into the space to write in age in years (did you set data validation correctly?). Try to proceed through the survey without consenting (did you force response on the consent questions?). Skip some questions, add extra text answers, and otherwise try to test out any kind of response that might be possible with your survey. This process will help you set up your survey to restrict the number of wild and exciting choices your participants can make, so your data cleaning is less complex.\n\n\nPreparing for Processing\nOnce you’ve done your preview runs, export your dataset and read it into R just as you would a real dataset. Here you can check that your variable labels and values are correct, that all response options appear correctly, and so forth. When you’re happy that all your data is recording as it should, you can publish your study and start collecting responses. (Good luck!)\nWe’re not done yet, though. As I said above, this preview dataset full of junk data is formatted exactly the same way as your real dataset will be. This means that while your study is collecting responses, you can already start writing the code to clean the dataset, generate the codebook, calculate subscales, etc. - because you already know what your variables will be named and what they will contain. Once you complete your data collection, you will already have the majority of your data cleaning work done - although make sure you carefully check your code step by step with the real data; there are always surprises!"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#moderate",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#moderate",
    "title": "11: Qualtrics II",
    "section": "Moderate",
    "text": "Moderate\nThe moderate level assumes that you’d like to futureproof your work somewhat, to reduce duplicating the same manual setup every time.\n\nUsing Templates\nTemplates allow you to easily replicate the same setup across multiple studies. You can turn your own surveys into templates to use in the future, or find existing templates to adapt for your purposes.\nCreating your surveys in Qualtrics with an eye to turning them into templates, or using surveys from template, has an enormous potential advantage: predictability. The demographic gender question, for example, might always be called demo_gender, so you don’t have to check or remember if it was gender or Gender_Demo or Q12 or whatever between studies - it’s always the same. Why is this the best thing ever? Because generalising code depends on predictability. If your data is predictable - if it always has the same names and the same format and contains the same type of information in the same variables from one study to another - then the code that works on one dataset will work on another2. So, not only will you save a lot of time setting up the same questions and writing the same consent forms over and over, but you can also make your data cleaning and wrangling process more automated.\n\nCreating Your Own Template\nThere are a few ways to use existing surveys as templates. This is largely preference, but might depend on what information you have access to.\n\nCopy an Existing Survey\nFor a survey you already have set up, you can straightforwardly duplicate it. This will copy all the settings, questions, survey flow, etc., but none of the actual responses.\n\nSee the Qualtrics Support on Creating from a Copy.\n\n\n\nPros\n\nReuse the entire survey as you originally set it up\nEasily edit it as normal for your new round of data collection\n\n\nCons\n\nMultiple similar copies of the same survey may get confused\nNot easy to remix elements from existing surveys\n\n\n\n\n\nAdd to Your Library\nYour Library in Qualtrics is an archive of surveys, or pieces of surveys (like blocks or questions) that you might want to reuse across multiple surveys. There are public Library elements, but you can also add elements you’ve made from your own surveys to your personal Library to reuse in the future.\n\nSee the Qualtrics Support on Library Surveys.\n\n\n\nPros\n\nAllows recombination of elements easily\nNo need to remember which survey(s) contain which element(s) you want to reuse\n\n\nCons\n\nSomewhat unwieldy to manage (lots of clicking and naming things, setting up folder organisation)\nDifficult to share?\nWould require extra work to export library elements\n\n\n\n\n\nImport/Export to File\nFor this option, you export a survey as a .qsf file. The file contains all the information about the survey, including the blocks/questions and all of the settings, survey flow, etc. You can then send this file to others, post it on Canvas, and otherwise share it however you like, and anyone who uploads it to Qualtrics will have the complete survey without any responses.\n\n\n\n\n\n\nTip\n\n\n\nSee the Qualtrics Support pages on Importing and Exporting Surveys.\n\n\n\n\nPros\n\nEasy to share complete surveys with other users, like dissertation students/supervisors, collaborators\nKeep an archive of your surveys separate from institution account\n\n\nCons\n\nWithin the same account, is essentially identical to copying a survey, just with more steps\nCan only export the entire survey, rather than parts\n\n\n\n\n\n\nUsing Existing Templates\nUsing others’ templates is largely a matter of finding something or someone who has the setup you want, and getting them to share it with you. Here are some resources to get you started:\n\nPsychology Qualtrics Template: A massive, open-source template containing a large number of common and widely used psychological measures.\nSussex low-risk ethics: A template created by me for ethically low-risk online survey studies at Sussex. Includes fill-in-the-blank ethics and debrief forms, some common demographics questions, and layouts for matrix table measures. It also includes a survey flow randomising the order of the two questionnaire blocks.\n\nTo use these or other QSF file templates, the Qualtrics guide to creating a project details the process step by step.\n\n\n\nDeveloping a Script\nAs described above, tne key advantage of using templates is that you can coordinate your Qualtrics setup across studies, and that means consistency in the details of the data, such as variable names and labels. You can then develop a script that you can use across different studies to do data cleaning and wrangling processes, without having to reinvent the wheel each time.\nBy a “script”, here, I don’t necessarily mean a .R script file; this could be a Quarto document as well. You could, for example, create a Quarto document that contains generalisable code for any similar surveys, beginning as follows:\n\nfile_name &lt;- \"your_file_name.sav\"\n\nlibrary(tidyverse)\n\nmy_data &lt;- here::here(\"data\", file_name) |&gt; haven::read_sav()\n\nFor future surveys, you can replace the file name stored in file_name in the first line and then work through the code from that point on. The best way to do this would be to use a study that is set up as you typically intend to run studies; write your script to do your processing using that dataset as a test case; and then apply the same code step by step to the next study that used that first study as a template.\nIt’s quite difficult to write a description of what that processing script might look like, but here are some elements you may want to include:\n\nDropping variables added by Qualtrics that you don’t want\nProducing a codebook\nCalculating subscale scores\nProducing visualisations of variables of interest\nProducing summaries of missing values and identifying cases to remove based on consistent criteria."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#majority",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#majority",
    "title": "11: Qualtrics II",
    "section": "Majority",
    "text": "Majority\n\n\n\n\n\n\nImportant\n\n\n\nAt the moment, I do not have API access to Qualtrics. So, I’m instead going to simply sketch the steps I will investigate when/if my institution enables API access on my account. If you want to do this process yourself, please be aware that the description below may not be accurate, and will likely require some (extensive) problem-solving!\n\n\nThe Majority level would be quite an undertaking, and as I’ve said, this is hypothetical - I haven’t even managed it myself yet! If you are looking to work at this level, you should be pretty comfortable and confident working in R, since the system would rely as little as possible on point-and-click interfaces and as much as possible on R.\nFor this system, there are a couple of promising packages. Both make use of the Qualtrics API, which is well-documented but an additional paid feature, so you need to check if you have access to it. If you don’t - well, unfortunately, this won’t work!\n\n\n\n\n\n\nChecking API Access\n\n\n\n\n\nTo check whether you have API access on your Qualtrics account:\n\nLog into Qualtrics\nClick on your account and then Account Settings\nClick the Qualtrics IDs tab\nCheck the box labeled API.\n\nCompare what you see there to the description under Getting Your API URL. If it matches what you see there, carry on! If it says what mine does - “The API is not enabled for your account” - then you’ll have to get in touch with your Qualtrics admin and ask for it.\n\n\n\nIf this is your first contact with an API, well, you’re in for a good time! As an extremely approximate and non-technical explanation, an API (Application Program Interface) is a structure for communicating with an application directly, without using their point-and-click user interface. R packages that utilise APIs have functions that convert your input in R into a format that the API can understand, or make requests for information from the API directly in R. Usually, this involves setting up an authentication system to allow requests from you to be identified and associated with your account.\nThere are two packages that I’d be interested in using in order to work with Qualtrics via the API, with unfortunately similar names:\n\n{qualtricsR} contains functions to create surveys and post them to Qualtrics from RStudio.\n{qualtRics} contains functions to retrieve surveys directly from Qualtrics, circumventing the export - download - save - import process.\n\n\n## Install devtools if necessary\nwhile(!require(devtools)) install.packages(\"devtools\")\n\n## Install qualtricsR from GitHub\ndevtools::install_github('saberry/qualtricsR')\n\n## Install qualtRics from CRAN\ninstall.packages(\"qualtRics\")\n\n\nSurvey Creation System\nThe {qualtricsR} package contains a function that takes a CSV or dataframe in R and turns it into a Qualtrics survey. So, here’s the structure I would likely implement:\n\nCreate a new project dedicated to creating Qualtrics surveys.\nCreate a new questionnaires folder.\nCreate CSV spreadsheets for each questionnaire separately, e.g. one for the STARS, one for the STAIT, one for the PANAS, and so forth, with obvious filenames (stars.csv, stait.csv, panas.csv). If I wanted to add or update these questionnaires, I would only ever do it in these CSVs.\nCreate a CSV logbook recording which questionnaires were included in which studies, and use the logbook to read in the questionnaires and send them to Qualtrics.\n\nIn brief, the idea is that you build an archive of questionnaires as CSV files which are accessed using the logbook. The logbook serves as both a system to create the studies, and as a record of what each study contained for future reference and to create codebooks.\n\n\n\n\n\n\nWarning\n\n\n\nFrom the help documentation, there are a few outstanding questions that I would want to investigate before fully committing to this system.\n\nHow to set the underlying numeric choices for response options\nHow to create different types of questions, e.g. matrix table questions, text response\n\n\n\n\n\nSurvey Access System\nThe {qualtRics} allows you to read in survey data from Qualtrics directly. So, no having to export and then choose a sensible filename and then save and then import - you could simply read in the most recent copy of the dataset. However, the package documentation says that the functions read in a CSV file - and not a SAV file, which would allow us to use the {labelled} package and all those nifty features of labelled data. So, I’d want to investigate whether the functions can be adapted to import a SAV file instead; if not, I’d probably stick to manual download, since I’d prefer to have the labels rather than the convenience of direct download.\nFor this download system, I’d have a script or short bit of code that would identify which survey I wanted - depending on which I wanted to analyse - and read it in. This would be part of the project specific to that particular study, rather than in the questionnaire project I described above."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#footnotes",
    "href": "tutorials/psychrlogy/03_improvRs/11_qtrics2.html#footnotes",
    "title": "11: Qualtrics II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou’re welcome of course to use another, less evil search engine, as you like - I just haven’t tested them all to make sure this page comes up at the top!↩︎\nI can’t leave this supremely confident, nigh-hubristic statement just hanging here without a huge flag: NEVER ASSUME YOUR CODE HAS WORKED AS EXPECTED WITHOUT THOROUGH CHECKING/TESTING! For relatively simple setups, and routine steps, you can achieve this, but even for the most basic operations, you should always always always double check what your code has produced.↩︎"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html",
    "href": "workshops/dissertations/qualtrics_workshop.html",
    "title": "Working with Qualtrics Data",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to easily produce a data dictionary straight from the dataset itself.\n\n\n\n\n\n\nImportant\n\n\n\nThis is the live version of the workshop tutorial, with no solutions for exercises. It’s recommended to use this one the first time through to work on the exercises yourself.\nIf you’d like the complete version, see the Solutions version of this same workshop tutorial which includes all code for exercises along with extra tips and explanations.\n\n\n\n\nThis tutorial was co-written with Dr Dan Evans, drawing on her existing resources for Qualtrics and her extensive experience supporting dissertation students.\nThe material in this tutorial was originally co-conceived with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin.\n\n\n\nThe first section of the tutorial gives advice for setting up a Qualtrics questionnaire, and will help you understand how the questionnaire you build will correspond to the dataset you get at the end.\nIf you are in a live workshop or already have data to work with, jump down to Setup to get started with the data portion."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#overview",
    "href": "workshops/dissertations/qualtrics_workshop.html#overview",
    "title": "Working with Qualtrics Data",
    "section": "",
    "text": "This tutorial will focus on efficient, transparent, and user-friendly techniques for working with data specifically gathered using the Qualtrics survey platform. We will cover how to import and work with labelled data from Qualtrics and how to easily produce a data dictionary straight from the dataset itself.\n\n\n\n\n\n\nImportant\n\n\n\nThis is the live version of the workshop tutorial, with no solutions for exercises. It’s recommended to use this one the first time through to work on the exercises yourself.\nIf you’d like the complete version, see the Solutions version of this same workshop tutorial which includes all code for exercises along with extra tips and explanations.\n\n\n\n\nThis tutorial was co-written with Dr Dan Evans, drawing on her existing resources for Qualtrics and her extensive experience supporting dissertation students.\nThe material in this tutorial was originally co-conceived with two brilliant PhD researchers, Hanna Eldarwish and Josh Francis, who contributed invaluable input throughout the process of developing the tutorial. Hanna Eldarwish also provided the basis for the dataset, collected during her undergraduate dissertation at Sussex under the supervision of Dr Vlad Costin.\n\n\n\nThe first section of the tutorial gives advice for setting up a Qualtrics questionnaire, and will help you understand how the questionnaire you build will correspond to the dataset you get at the end.\nIf you are in a live workshop or already have data to work with, jump down to Setup to get started with the data portion."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#qualtrics",
    "href": "workshops/dissertations/qualtrics_workshop.html#qualtrics",
    "title": "Working with Qualtrics Data",
    "section": "Qualtrics",
    "text": "Qualtrics\nQualtrics is a survey-building tool very commonly used for questionnaire-type studies, as well as some experimental work. The University of Sussex has an institutional licence for Qualtrics, so all staff and students can log in with their Sussex details and easily construct and collaborate on surveys.\nFor help using Qualtrics itself, the Qualtrics support pages are generally excellent. This tutorial will only briefly touch on the options within Qualtrics itself.\nOnce the study is complete and responses have been collected, you will need to export your data from Qualtrics so that you can analyse it. Qualtrics offers a variety of export data types, including our familiar CSV type. However, we’re going to instead explore a new option: SAV data.\n\nSAV Data\nThe .sav file type is associated with SPSS, a widely used statistical analysis programme. So, why are we using SPSS files when working in R?\nImporting via .sav has two key advantages. First, it results in a much cleaner import format. If you try importing the same data via .csv file, you’ll find that you need to do some very fiddly and pointless cleanup first. For instance, the .csv version of the same dataset will introduce some empty rows that have to be deleted with dplyr::slice() or similar. The .sav version of the dataset doesn’t have any comparable formatting issues.\nMost importantly, however, importing .sav file types into R with particular packages like {haven} gets us a dataset with a special type of data: namely, labelled data. The labels allow us to preserve important information about the questions asked and response options in Qualtrics, and to (mostly) painlessly create codebooks for datasets. We will explore these features in depth in this tutorial.\n\n\nSetting Up Qualtrics\n\n\n\n\n\n\nImportant\n\n\n\nThe following section is most useful when you are creating your Qualtrics questionnaire. If you are just starting your study, you’re recommended to read this section in full.\nIf you already have a Qualtrics questionnaire, be very careful about editing it after data collection has begun. Minimally, if you do decide you want to make changes, export copies of both your dataset and your questionnaire before you make any edits.\nIf you have a dataset in Qualtrics, jump down to exporting data.\nIf you already have data in .sav format to work with, jump down to the next section.\n\n\nIn this section we’ll have a quick look at how to set up Qualtrics to work as smoothly as possible with R. This has also previously been covered in a QQM Skills Lab.\n\nUsing Blocks\nBlocks are the way that Qualtrics organises pieces of the survey. Essentially, everything in the same block becomes a unit. You can have multiple questions per block, or just one. Blocks are vital for creating a study that appears as you want, but they won’t have any substantial impact on the format of the data.\nExplaining blocks and how they can be arranged is a bit outside the scope of this tutorial, so see the Block Options page in the Qualtrics guide for more details.\n\n\nUsing Questions\nThe core of Qualtrics are questions, which you can create within blocks. By default, a new question is a multiple-choice question (MCQ), but you can customise this in depth in the “Edit question” sidebar to the left of the survey. To edit a question, you have to click on each question, which will outline the question in a blue box; you can then change the settings for that question in the sidebar.\n\n\n\n\n\n\nTip\n\n\n\nFor extensive help on creating and work with questions, see the Qualtrics Guide.\n\n\n\nQuestions in R\nLet’s have a look at the default question, which appears like this:\n\nAs you can see here, the way that you set up your questions translates directly into the way your dataset will appear.\n\nNames: All questions are automatically given a name, by default Q[number], e.g. Q1, Q2, etc. This question name will appear as the variable name in your exported dataset. These names are not visible to your participants.\nText: Question text is the actual question that your participants see. This question text will appear as the variable label in your exported dataset.\nChoices: For questions with a specific set of choices, like multiple-choice questions and rating scales, the choices you list here are the response options that your participants see. These choices will appear as the value labels in your exported dataset.\n\nYou may notice that there’s no evidence of the underlying numerical values for each choice. Although Qualtrics doesn’t make this immediately obvious, they are always worth checking, because sometimes they’re…creative. This doesn’t matter so much for questions that are going to become factors - whether the underlying number is 1 or 14 or 73 doesn’t matter because they’re just a marker for a unique category. However, we’ll see in a moment an example where it does matter, namely rating scales.\nTo check the values, click on the question, scroll down to the bottom of the Edit Question sidebar, and click on “x -&gt; Recode values”. This opens a new pop-up window where you can edit a few options:\n\nTick Recode Values to change the numeric values for each choice. These values are the underlying values that will appear as numbers in the dataset in R.\nTick Variable Naming to give different value labels to the choices than the ones the participants see. (Personally I’d be very wary of doing this, as it would be easy to lose track of what participants actually saw/responded to!)\n\n\nAs you can see from this simple “What’s your favourite pie?” question, these underlying numeric values can go wonky quickly. I have four options, “apple”, “cherry”, “pecan”, and “pumpkin”, which are numbered 1, 6, 2, and 3 respectively! What’s happened is that I created “apple”, “pecan”, and “pumpkin”, and then a couple other options; then I changed my mind, removed the other options (which would have been 4 and 5) and added “cherry” after “apple”. Values are assigned based on the order they are added, which is why the values came out weird and out of numerical order. If I wanted these to go in order (which isn’t a bad idea, since you want your data to be predictable), I can tick “Recode Values” and then manually enter the numeric values I want for each choice.\n\n\nMatrix Questions\nMatrix questions are very commonly used as an efficient way to present multiple questions or items with the same response scale - for example, items on a scale or subscale with a consistent Likert response scale.\nTo create one, create a “Matrix table” type question. The typical setup is for the items/questions to be presented down the left-hand side as “statements”, and the rating scale to be presented along the top as “scale points”.\nThe “Scale points” section of the Edit Question sidebar lets you control how these scale points appear. You can add or remove the number of points, and for many scales in Psychology, you can use suggested rating scales by switching the toggle on, which automatically insert labels for each scale point for you.\nMatrix tables are especially prone to issues with the underlying numeric values, especially if you use these automatic scale points. You’ll end up with really weird ranges, like 61-65, instead of 1-5, which will do a number on the interpretation of any descriptives. Even better, the numeric values change themselves every time you make changes to them! So, I’d strongly recommend you update the numeric values using “Recode values” as the last step to make sure you don’t have any surprises when you get round to looking at the data.\n\n\n\n\nExporting Data\nIf you’d like to work with your own study data, you will need to export your data in SAV format from Qualtrics first. To do this, open your Qualtrics survey and select the “Data & Analysis” tab along the top, just under the name of your survey.\nIn the Data Table view, look to the right-hand side of the screen. Click on the drop-down menu labelled “Export & Import”, then select the first option, “Export Data…”\n\nIn the “Download a data table” menu, choose “SPSS” from the choices along the top. Make sure “Download all fields” is ticked, then click “Download”.\n\nThe dataset will download automatically to your computer’s Downloads folder. From there, you should rename it to something sensible and move it into a data folder within your project folder. From there, you can read it in using the here::here() %&gt;% haven::read_sav() combo that we saw in the Data section previously.\n\n\n\n\n\n\nSensible Naming Conventions and Folder Structure\n\n\n\n\n\nSensible file and folder names will make your life so much easier for working in R (and generally).\nFor folder structure, make sure you do the following:\n\nAlways always ALWAYS use an R Project for working in R.\nHave a consistent set of folders for each project: for example, images, data, and docs.\nUse sub-folders where necessary, but consider using sensible naming conventions instead.\n\nFor naming conventions, your file name should make it obvious what information it contains and when it was created, especially for datasets like this. I recommend longer and more explicit file names over brevity.\nSo, for a download like this, I’d name it something like qtrics_diss_2024_03_20.sav. The qtrics tells me it’s a Qualtrics export, the diss tells me it’s a dissertation project, and the last bit is the full date in easily machine-readable format. Imagine if I continue to recruit participants and download a new dataset later, say a month from now, and name it qtrics_diss_2024_04_20.sav. I could easily distinguish which dataset was which by the date, but also see that they are different versions of the same thing by their shared prefix.\nThis is a much more reliable system than calling them, say, Qualtrics output.sav and Dissertation FINAL REAL.sav. This kind of naming “convention” contains no information about which is which or when they were exported, or even that they’re two versions of the same study dataset! Future You trying to figure out which dataset to use weeks or months later will feel the difference."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#setup",
    "href": "workshops/dissertations/qualtrics_workshop.html#setup",
    "title": "Working with Qualtrics Data",
    "section": "Setup",
    "text": "Setup\nThe rest of this tutorial walks you through the basics of importing, inspecting, cleaning, and converting your Qualtrics data, including automatically generating a data dictionary for reference. Data is provided to practice with in workshops, but you are welcome to follow along with your own data if you prefer.\n\nPackages\nWe will need the following packages:\n\n{tidyverse} for data wrangling.\n{haven} for importing data. This package is installed with {tidyverse} but not loaded with the core packages so needs to be loaded separately.\n{labelled} for working with labelled data.\n{sjPlot} for a data dictionary convenience function\n\n\n\n\n\n\n\nExercise\n\n\n\nLoad the packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(sjPlot)\n\n\n\n\n\n\n\n\nData\nToday’s example dataset focuses on various aspects of meaning in life (MiL), and has been randomly generated based on a real dataset kindly contributed by Hanna Eldarwish and Vlad Costin. All variables have been randomly generated, but they are based on the patterns in the original dataset. The original, bigger dataset will be made available alongside article publication in the future, so keep an eye out for it!\n\n\n\n\n\n\nNew File Type\n\n\n\nYou might notice that instead of the familiar readr::read_csv(), today we have haven::read_sav(). We need a different function since we are using a different type of data. See the section above on .sav data for more details.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRead in the mil_data.sav object from the data folder, or alternatively from Github via URL, as you prefer.\nURL: https://github.com/de84sussex/data/raw/main/mil_data.sav\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom a folder:\n\nmil_data &lt;- here::here(\"data/mil_data.sav\") %&gt;% haven::read_sav()\n\nFrom URL:\n\nmil_data &lt;- haven::read_sav(\"https://github.com/de84sussex/data/raw/main/mil_data.sav\")\n\n\n\n\n\n\n\nCodebook\nThis codebook is intentionally sparse, because we’ll be generating our own from the dataset in just a moment. This table covers only the demographic and questionnaire measures to help you understand the variables.\n\n\n\n\n\n\nCodebook\n\n\n\n\n\n\n\n\n\n\n\nVariable\nItem/Scale: Subscale\n\n\n\n\nQ1\nHow well can you speak English?\n\n\nQ2\nHow old are you?\n\n\nQ3\nWhat is your gender identity?\n\n\nQ4\nWhat is your annual income?\n\n\nQ5\nWhat is your occupation?\n\n\nQ6_1\nMeaning in Life: Global Meaning (item 1)\n\n\nQ6_2\nMeaning in Life: Global Meaning (item 2)\n\n\nQ6_3\nMeaning in Life: Global Meaning (item 3)\n\n\nQ6_4\nMeaning in Life: Global Meaning (item 4)\n\n\nQ7_1\nMeaning in Life: Mattering (item 1)\n\n\nQ7_2\nMeaning in Life: Mattering (item 2)\n\n\nQ7_3\nMeaning in Life: Mattering (item 3)\n\n\nQ7_4\nMeaning in Life: Mattering (item 4)\n\n\nQ8_1\nMeaning in Life: Coherence (item 1)\n\n\nQ8_2\nMeaning in Life: Coherence (item 2)\n\n\nQ8_3\nMeaning in Life: Coherence (item 3)\n\n\nQ8_4\nMeaning in Life: Coherence (item 4)\n\n\nQ9_1\nMeaning in Life: Purpose (item 1)\n\n\nQ9_2\nMeaning in Life: Purpose (item 2)\n\n\nQ9_3\nMeaning in Life: Purpose (item 3)\n\n\nQ9_4\nMeaning in Life: Purpose (item 4)\n\n\nQ10_1\nSymbolic Immortality (item 1)\n\n\nQ10_2\nSymbolic Immortality (item 2)\n\n\nQ11_1\nBelonging (item 1)\n\n\nQ11_2\nBelonging (item 2)\n\n\nQ11_3\nBelonging (item 3)\n\n\nQ11_4\nBelonging (item 4)\n\n\nQ11_5\nBelonging (item 5)\n\n\nQ11_6\nBelonging (item 6)\n\n\nQ11_7\nBelonging (item 7)\n\n\nQ11_8\nBelonging (item 8)\n\n\nQ11_9\nBelonging (item 9)\n\n\nQ11_10\nBelonging (item 10)\n\n\nQ11_11\nBelonging (item 11)\n\n\nQ11_12\nBelonging (item 12)"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#variable-names",
    "href": "workshops/dissertations/qualtrics_workshop.html#variable-names",
    "title": "Working with Qualtrics Data",
    "section": "Variable Names",
    "text": "Variable Names\nQualtrics datasets are often large and unwieldy. However, they also often have a consistent structure, which we can take advantage of to work with them consistently.\n\nDefault Variable Names\nIn your dataset, you will by default have some variables that are automatically created by Qualtrics, with (somewhat) sensible names, like DistributionChannel and StartDate. You will also have all the questions that you created, and what they are called depends on what you (or, rather, the author of the questionnaire) called them.\nIf you changed the name of the questions, they will have the name that you gave them. If not, they will have a default name from Qualtrics, usually the capital letter “Q” followed by a number, like this: Q15, Q34, etc.\nIf you have matrix questions, the variable names will have a further number indicating which item in the matrix they correspond to. If, for example, your matrix question was Q23, then the responses to the first item in that matrix will be stored in Q23_1, the second in Q23_2, and so on.\nThese default variable names should be changed as a first step, before you carry on with your data processing. This is because they are easy to mix up or mistype, and difficult to remember (was it Q23 or Q32 that contained the question I wanted…?), which will lead to both unnecessary errors and extra time spent fixing problems or cross-checking which question is which.\nTo do this, we’ll get round to the dplyr::rename() function by way of a detour revising dplyr::select().\n\n\nSelecting\nYou have already encountered the function dplyr::select() as a function for keeping or dropping columns in a dataset. As a reminder, there’s some easy notation to use for variable names to quickly select single variables or ranges, or to drop variables.\n\n1dataset_name %&gt;%\n2  dplyr::select(\n3    variable_to_include,\n4    -variable_to_exclude,\n5    keep_this_one:through_this_one,\n6    new_name = variable_to_rename,\n7    variable_number\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n2\n\nSelect the following variables:\n\n3\n\nThe name of a variable to be included in the output. Multiple variables can be selected separated by commas.\n\n4\n\nThe name of a variable to be excluded from the output. Use either an exclamation mark (!) or a minus sign (-) in front of each variable to exclude. Multiple variables can be dropped, separated by commas with a ! (or -) before each.\n\n5\n\nA range of variables to include in the output. All the variables between and including the two named will be selected (or dropped, with !(drop_this_one:through_this_one)).\n\n6\n\nInclude variable_to_rename in the output, but call it new_name.\n\n7\n\nInclude a variable in the output by where it appears in the dataset, numbered left to right. For example, “2” will select the second column in the original dataset.\n\n\n\n\nColumns will appear in the output in the order they are selected in select(), so this function can also be used to reorder columns.\n\nSelection Helpers\nHowever, the real power in this and other {tidyverse} functions is in a system of helper functions and notations collectively called “selection helpers”, or &lt;tidyselect&gt;. The overall goal of “&lt;tidyselect&gt; semantics” (as you will see it referred to in help documentation) is to make selecting variables easy, efficient, and clear.\nThese helper functions can be combined with the selection methods above in any combination. Some very convenient options include:\n\neverything() for all columns\nlast_col() for the last column in the dataset\nstarts_with(), ends_with(), and contains() for selecting columns by shared name elements, which will be our key focus today.\nwhere() for selecting with a function, not described here (see ?where() for more)\n\nFor example, we can select specific groups of variables using the shared portions of their names, such as:\n\n“Date” for the three default Qualtrics variables containing date information\n“Q8” for the four Coherence items\n\n\nmil_data %&gt;% \n  dplyr::select(\n    contains(\"Date\"), starts_with(\"Q8\")\n  )\n\n\n\n  \n\n\n\n\n\n\nRenaming\nNow that we know how to easily select groups of variables, we need sensible names in order to make best use of those selection helpers. There are three main options for renaming variables, depending on access to the original Qualtrics questionnaire, and proficiency in R.\n\n\n\n\n\n\nImportant\n\n\n\nYou are strongly advised not to manually change the names in your dataset, e.g. in a .csv file/Excel. Not only will you lose the labels, but this is very prone to error with no record of the changes made.\n\n\n\nOption 1: Rename in Qualtrics\nThis option requires that you have have access to, and are willing to edit, the original Qualtrics questionnaire. Rather than being a coding option, this entails going back to the Qualtrics questionnaire and changing the question labels before you export the dataset.\nFor more on this, see Setting Up Qualtrics.\n\n\nOption 2: rename()\nThe friendly dplyr::rename() function does exactly what it says on the tin. In general:\n\n1dataset_name %&gt;%\n2  dplyr::rename(\n3    new_name = old_name\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n2\n\nRename the following variables:\n\n3\n\nThe new name (new_name) you would like to give to an existing variable (old_name).\n\n\n\n\nYou can list as many of these new_name = old_name pairs as you like. For example, let’s rename the Global Meaning items so they have sensible prefixes. We should keep the item numbers as they are, so we know which one is which.\n\nmil_data %&gt;% \n  dplyr::rename(\n    meaning_1 = Q6_1,\n    meaning_2 = Q6_2,\n    meaning_3 = Q6_3,\n    meaning_4 = Q6_4,\n  )\n\n\n\n  \n\n\n\nThis option allows you to easily keep track of the renaming you’ve done in your code, but it is very tedious and intensive, especially if you have many variables that need renaming.\n\n\nOption 3: rename_with()\nThis option requires considerable proficiency and experience with R. It is by far the quickest and most efficient of these options, but you must be able to write anonymous functions, use regular expressions and selection helpers, and have good working knowledge of how to debug errors and check output. If any of those things are unfamiliar, use one of the two previous options instead.\n\n\n\n\n\n\nHaRd Mode: Using rename_with()\n\n\n\n\n\nThe versatile dplyr::rename_with() function allows quick, efficient, and accurate renaming of large groups of variables at once. The general form is:\n\ndataset_name %&gt;%\n  dplyr::rename_with(\n     .fn = function_to_apply,\n     .cols = variables_to_rename\n  )\n\n\n\n\n\n\nThe “function to apply” here could be simply the name of an existing function, for example tolower (convert to lowercase). You can also write a “purrr-style lambda” function, which will allow you to write your own custom function to change the variable names however you please.\nAs an example, let’s convert the Q11 variables in the dataset at once. We know from the codebook that these are all items on the Belonging subscale, so we want to replace the string “Q11” in the variable names to “belonging”.\n\n1mil_data %&gt;%\n  dplyr::rename_with(\n2    .fn = ~ gsub(\"Q11\", \"belonging\", .x),\n3    .cols = dplyr::starts_with(\"Q11\")\n  )\n\n\n1\n\nTake the mil_data dataset and then rename variables as follows\n\n2\n\nReplace every instance of the string “Q11” with the string “belonging”\n\n3\n\nDo this for every column that currently starts with the string “Q11”\n\n\n\n\n\n\n  \n\n\n\nIn this command, our “purrr-style lambda” is the anonymous function ~ gsub(\"Q23\", \"belonging\", .x). The ~ (apparently pronounced “twiddle”) at the beginning is a shortcut for the longer function(x) ... notation for creating functions. The .x is a placeholder for each of the variables that the function will be applied to. These are both used in a customised version of the base-R gsub() function, which generally substitutes every match with its first argument with the replacement in its second argument for the vector of possibilities in its third argument; see ?gsub() for details.\nAs you can see from the output, this only replaces the relevant portion of the column name, leaving the numbered item suffixes unchanged. If you are proficient in working with regular expressions and string manipulation, you can use this technique to programmatically rename variables very easily."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#exercises-names",
    "href": "workshops/dissertations/qualtrics_workshop.html#exercises-names",
    "title": "Working with Qualtrics Data",
    "section": "Exercises: Names",
    "text": "Exercises: Names\nBefore we go on, it’s time to get the variables in this dataset sorted out. You must do this, or the solutions further on in the document won’t work!\n\n\n\n\n\n\nExercise\n\n\n\nClean up your dataset by doing the following. You can do the steps in whatever order works for you.\n\nKeep all the demographic questions, items measuring Global Meaning, and Mattering, and all the Belonging items.\nRename any default-named Qualtrics variables (starting with “Q”) to a sensible name using the Codebook to guide you."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#labelled-data",
    "href": "workshops/dissertations/qualtrics_workshop.html#labelled-data",
    "title": "Working with Qualtrics Data",
    "section": "Labelled Data",
    "text": "Labelled Data\nWith the minimal necessary cleaning out of the way, we can now move on to exploring labelled data.\n\n\n\n\n\n\nThe Plan\n\n\n\nOur workflow for this dataset will be slightly different than you may have encountered before.\nWe’ll start by checking the labels and producing a codebook, or “data dictionary”, drawing on the label metadata in the SAV file. For the purpose of practice, we’ll also have a look at how to work with those labels, and optionally manage different types of missing values.\nAs useful as labels are, they will get in the way when we want to work with our dataset further. So, we’ll next convert the variables in the dataset into either factors, for categorical data, or numeric, for continuous data 1. From that point forward, we can work with the dataset using the techniques and functions we’ve covered throughout first and second year.\n\n\n\nWorking with Labels\nThe SAV data we’re using has a special property: labels. Labelled data has a number of features, which we will explore in depth shortly:\n\nVariable labels. The label associated with a whole variable will contain the text of the item that the participants responded to. This is analogous to the “Label” column of the Variable View in SPSS.\nValue labels. The label associated with individual values within a variable will contain the text associated with individual choices, for instance the points on a Likert scale or the options on a multiple-choice question. This is analogous to the “Values” column of the Variable View in SPSS.\nMissing values. Within value labels, you can designate particular values as indicative of missing responses, refusal to respond, etc. This is analogous to the “Missing” column of the Variable View in SPSS.\n\nWe’re first going to look at how you can work with each of these elements. The reason to do this is that once our dataset has been thoroughly checked, we’re going to generate a final data dictionary, then convert any categorical variables into factors, the levels of which will correspond to the labels for that variable. We’ll also convert any numeric variables into numeric data type, which will discard the labels; that will make it possible to do analyses with them, but that’s why we have to create the data dictionary first.\nMost of the following examples are drawn from the “Introduction to labelled” vignette from the {labelled} package. If you want to do something with labelled data that isn’t covered here, that’s a good place to start!\n\n\n\n\n\n\nImportant\n\n\n\nThese features will work optimally only if you have set up your Qualtrics questionnaire appropriately. Make sure to refer to the Setting Up Qualtrics section to get the most out of your labelled data and save yourself data cleaning and wrangling headaches later.\n\n\n\n\nVariable Labels\nVariable labels contain information about the whole variable, and for Qualtrics data, will by default contain either an automatically generated Qualtrics value (like “Start Date”), or the question text that that variable contains the responses to.\n\nGetting Labels\nTo begin, let’s just get out a single variable label to work with using labelled::var_label().\nTo specify the variable we want, we will need to subset it from the dataset, using either $ or dplyr::pull().\n\nlabelled::var_label(mil_data$gender)\n\n[1] \"What is your gender identity?  This question is optional. - Selected Choice\"\n\n\n\n\nCreating/Updating Labels\nIf you’d like to edit labels, you can do it “manually” - that is, just writing a whole new label from scratch.\nThe structure of the following code might look a little unfamiliar. For the most part, we’ve seen code that contains longer and more complex instructions on the right-hand side of the &lt;-, and a single object being created or updated on the left-hand side. In the structure below, the left-hand side contains longer and more complex code that identifies the value(s) to be updated or created, and the right-hand side contains the value(s) to create or update. It’s the same logic, just with a different structure.\n\nlabelled::var_label(mil_data$StartDate) &lt;- \"Date and time questionnaire was started\"\n\nlabelled::var_label(mil_data$StartDate)\n\n\n\n[1] \"Date and time questionnaire was started\"\n\n\n\n\n\n\n\n\nHaRd Mode: Using Regular Expressions\n\n\n\n\n\nRegular expressions are the magic of working with code. They are also fiddly, confusing, and difficult. If you’re not keen on spending a lot of time learning what is in essence a new mini-language, skip this section!\nEditing labels is a good opportunity to start working with regular expressions. For example, if we want to keep only the first bit of the label for gender, then we can keep everything only up to and including the question mark, and re-assign that to the variable label. This style is a bit more dynamic and resilient to changes or updates.\n\nlabelled::var_label(mil_data$gender) &lt;- labelled::var_label(mil_data$gender) %&gt;%\n  gsub(\"(.*\\\\?).*\", \"\\\\1\", x = .)\n\nlabelled::var_label(mil_data$gender)\n\n\n\n[1] \"What is your gender identity?\"\n\n\nLet’s pick apart this gsub() command a bit at a time. First, gsub() has three arguments:\n\npattern, here \"(.*\\\\?).*\", which is the regex statement representing the string to match.\nreplacement, here \"\\\\1\", which is the string that should replace the match in pattern.\nx, the string to look in.\n\nThe pattern has essentially two parts: the bit in the rounded brackets, and the bit outside. The rounded brackets designate a “capturing group” - a portion of the string that should be grouped together as a unit. The benefit of this grouping is in the second argument of gsub(); \\\\1 isn’t the number 1, but rather is a pronoun referring to the first capturing group. In other words, as a whole, this gsub() command captures a subset of the incoming string, and then replaces the entire string with that captured string, essentially dropping everything outside the capturing group.\nTo understand the regex statement \"(.*\\\\?).*\", we need to look at the incoming text, x. In this case, x is being piped in from above and looks like this:\n\nlabelled::var_label(mil_data$gender)\n\n[1] \"What is your gender identity?  This question is optional. - Selected Choice\"\n\n\n.* is a common regex shorthand that means “match any character, as many times as possible.” It’s essentially an “any number of anything” wildcard. This wildcard appears both inside and outside the brackets. So, how does gsub() know which bit should belong in the capturing group?\nThe answer is \\\\?. This is a “literal” question mark. Some symbols, like . and ?, are regex operators, but we might want to also match the “literal” symbols full-stop “.” and question mark “?” in a string. In this case we need an “escape” character “\\\", that escapes regex and turns the symbol into a literal one. So, the capturing group ends with a literal question mark - in the target string, that’s the question mark after”identity”, which is the only one in the string.\nAs an aside, if you’re wondering why there are two escape characters instead of one - i.e., why is it \\\\? and not \\?, well, you and me both. There’s an explanation in vignette(\"regular-expressions\") that never completely makes sense to me. Also, this seems to be an R thing - regex outside of R seems to use only a single escape character, so a literal question mark would be \\?. If you are ever trying to adapt regex from e.g. StackOverflow or regex101 and it isn’t working, check whether the escape characters are right!\nAnyway. We can now read \"(.*\\\\?)\" as “capture all characters up to and including a literal question mark” - which matches the substring “What is your gender identity?” in x. However, we don’t just want to replace that portion of the string - instead, we want to replace the whole string with that bit of it. So, the second .* outside the brackets matches the rest of the string. If we didn’t include this last bit, the capturing group would just be replaced with itself, which would result in the same string as we started with, as below:\n\nlabelled::var_label(mil_data$gender) %&gt;%\n  gsub(\"(.*\\\\?)\", \"\\\\1\", x = .)\n\n[1] \"What is your gender identity?  This question is optional. - Selected Choice\"\n\n\nSo, altogether, we can read this gsub() command as: “Capture everything up to an including the question mark, and replace the entire string with that capturing group.”\nNow. Why, you might wonder, is all this faff better?\nWell, it might not be. You might find it more frustrating or effortful to generate the right regex pattern than to replace the label “manually”, and in that case, there’s nothing wrong with just writing out the label you want.\nOn the other hand, the regex command will always drop everything after the question mark, no matter what that text is. If there is no match, it won’t replace anything. So, unlike the “manual” option, there’s much less danger of accidentally mixing up labels or overwriting the wrong thing; and this regex statement can be generalised to any label that contains a question mark, rather than having to type out each label one by one.\n\n\n\n\n\nSearching Labels\nA very nifty feature of variable labels and {labelled} is the ability to search through them with labelled::look_for(). With the whole dataset, look_for() returns a whole codebook (see Data Dictionaries below for more on this), but given a second argument containing a search term, you get back only the variables whose label contains that term.\nFor example, we can use labelled::look_for() to get only the items in this questionnaire that mentioned family. (I’ve piped into tibble::as_tibble() to make the output easier to read.)\n\nlabelled::look_for(mil_data, \"family\") %&gt;%\n  tibble::as_tibble()\n\n\n\n  \n\n\n\n\n\n\nValue Labels\nValue labels contain individual labels associated with unique values within a variable. It’s not necessary to have a label for every value, but for our purposes, it’s important that all values that represent categories have a label.\n\nGetting Labels\nThere are two functions to assist with this. labelled::val_labels() (with an “s”) returns all of the labels, while labelled::val_label() (without an “s”) will return the label for a single specified value.\n\nlabelled::val_labels(mil_data$english_fluency)\n\n Very well       Well   Not well Not at all \n         1          2          3          4 \n\n\n\nlabelled::val_label(mil_data$english_fluency, 3)\n\n[1] \"Not well\"\n\n\n\n\nCreating/Updating Labels\nThese two functions can also be used to update an entire variable or a single value respectively. The structure of this code is the same as we saw with variable labels previously.\nFor example, let’s get all the value labels for the gender variable, then update the last value to “Other”.\nFirst, return the existing labels:\n\nlabelled::val_labels(mil_data$gender)\n\n                      Male                     Female \n                         0                          1 \n                Non-binary Other (please state below) \n                         2                          3 \n\n\nThen, replace the label associated with the value 3:\n\nlabelled::val_label(mil_data$gender, 3) &lt;- \"Other\"\n\n\n\n\nMissing Values\nThis section is included especially for people who may have previous experience with SPSS, and are learning how to adapt their SPSS knowledge to R. Unless you make regular use of SPSS’s alternative options for managing missing values, you can skip this section.\n\n\n\n\n\n\nHaRd Mode: Missing Values\n\n\n\n\n\nLabelled data allows an extra functionality from SPSS, namely to create user-defined “missing” values. These missing values aren’t actually missing, in the sense that the participant didn’t respond at all. Rather, they might be missing in the sense that a participant selected an option like “don’t know”, “doesn’t apply”, “prefer not to say”, etc.\nLet’s look at an example. As we’ve just seen, we can get out all the value labels in variable with labelled::val_labels():\n\nlabelled::val_labels(mil_data$english_fluency)\n\n Very well       Well   Not well Not at all \n         1          2          3          4 \n\n\nThis variable asked participants to indicate their level of English fluency. Even for participants who have in fact responded to this question, we may want to code “Not well” and “Not as all” as “missing” so that they can be excluded easily. To do this, we can use the function labelled::na_values() to indicate which values should be considered as missing.\n\nlabelled::na_values(mil_data$english_fluency) &lt;- 3:4\n\nmil_data$english_fluency\n\n&lt;labelled_spss&lt;double&gt;[164]&gt;: Please select which box best describes your English fluency. - How well can you speak English?\n  [1] 2 2 2 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 2 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 2 2\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 3 2\nMissing values: 3, 4\n\nLabels:\n value      label\n     1  Very well\n     2       Well\n     3   Not well\n     4 Not at all\n\n\nFor the moment, these values are not actually NA in the data - they’re listed under “Missing Values” in the variable attributes. In other words, the actual responses are still retained. However, if we ask R which of the values in this variable are missing…\n\nis.na(mil_data$english_fluency)\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n…we can see one TRUE corresponding to the 3 above.\nIf we wanted to actually remove those values entirely and turn them into NAs for real, we could use labelled::user_na_to_na() for that purpose. Now, the variable has only two remaining values, and any 3s and 4s have been replaced.\n\nlabelled::user_na_to_na(mil_data$english_fluency)\n\n&lt;labelled&lt;double&gt;[164]&gt;: Please select which box best describes your English fluency. - How well can you speak English?\n  [1]  2  2  2  1  1  1  2  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [26]  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [51]  1  1  2  1  1  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  2  1  1  1  1\n [76]  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1\n[101]  1  1  2  1  1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[126]  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  1  1  1  1  1  1  2  1  1  1  1 NA  2\n\nLabels:\n value     label\n     1 Very well\n     2      Well\n\n\n\n\n\n\n\n\nTip\n\n\n\nSee the {labelled} vignette for more help on working with user-defined NAs, including how to deal with them when converting to other types."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#exercises-labels",
    "href": "workshops/dissertations/qualtrics_workshop.html#exercises-labels",
    "title": "Working with Qualtrics Data",
    "section": "Exercises: Labels",
    "text": "Exercises: Labels\nThe following exercises will help you get some hands-on practice with working with labels. You’re strongly recommended to try them yourself before you carry on.\n\n\n\n\n\n\nExercise\n\n\n\nIdentify the item that mentions ‘job’. Then, change the variable label of this item so that it just says ‘Occupational Status’\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFor the income variable, change the value label ‘I prefer not to disclose information about my annual income as part of this research study.’ to ‘Prefer not to say.’"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#data-dictionaries",
    "href": "workshops/dissertations/qualtrics_workshop.html#data-dictionaries",
    "title": "Working with Qualtrics Data",
    "section": "Data Dictionaries",
    "text": "Data Dictionaries\nOnce our labels have been cleaned and updated, we can finally produce a data dictionary for this dataset.\n\n\n\n\n\n\nWhy a data dictionary?\n\n\n\n\n\nThere’s two key reasons to produce a data dictionary for your dataset.\nFirst, data dictionaries (or “codebooks”) are very useful for understanding datasets, even your own. You may find yourself referring to it frequently when writing your methods and results, to remind yourself what different questions contain, what the text of the question was, etc.\nSecond, data dictionaries are hugely useful for other people. This would be a massive help to, for example, your supervisor who may need to assist you with your data analysis, or to include in your dissertation submission for your markers. If you want to share your data publicly, including a dictionary/codebook is not only a kindness to other users but also helps prevent misuse or misunderstandings.\n\n\n\nIf you primarily need a quick reference as you’re working with your dataset, the delightful sjPlot::view_df() function makes this particularly easy.\nLet’s put mil_data into the sjPlot::view_df() function and see what it does. By default, the document opens in the Viewer, but you can also save the file it creates for further sharing - see the help documentation.\n\nsjPlot::view_df(mil_data)\n\nIf you’re happy with this, this is probably all you need to carry on. If you are keen to create your data dictionary as a dataset that you could further edit - or if you’d like a version of the data dictionary that more closely emulates SPSS’s Variable View - see below.\n\n\n\n\n\n\nHaRd Mode: Editable Data Dictonary\n\n\n\n\n\nUse the generate_dictionary() function from the {labelled} packages to create a data dictionary for mil_data. To have the best look at it, I would recommend using View() to review it.\n\nmil_data %&gt;%\n  labelled::generate_dictionary() %&gt;%\n  View()\n\n\n\n\n\n  \n\n\n\nUnlike the output from sjPlot::view_df(), the output from this function is a dataset that you can work with. This means you can edit it using any of your {dplyr} skills and render it as a table in a document if you like. The sky’s the limit!"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#converting-variables",
    "href": "workshops/dissertations/qualtrics_workshop.html#converting-variables",
    "title": "Working with Qualtrics Data",
    "section": "Converting Variables",
    "text": "Converting Variables\nThe labels have served their purpose helping us navigate and clean up the dataset, and produce a lovely data dictionary for sharing. However, if we want to use the data, we’ll need to convert to other data types that we can use for statistical analysis.\nHow we convert each variable will fall into two main categories:\n\nAny variables containing categorical data, we’ll convert to factors, which will use the value labels as factor levels\nAny variables containing numbers that we want to do maths with, we’ll convert to numeric, which will strip the labels.\n\n\n\n\n\n\n\nImportant\n\n\n\nVariables that will be converted to factor should have labels for all of their levels, whereas variables that will be converted to numeric can have fewer labels, because we will stop using them after the numeric conversion.\n\n\n\nFactors\nFactor variables are R’s way of representing categorical data, which have a fixed and known set of possible values.\nFactors actually contain two pieces of information for each observation: levels and labels. Levels are the (existing or possible) values that the variable contains, whereas labels are very similar to the labels we’ve just been exploring.\nIf you feel confident understanding and working with factors in R, you can skip the box below.\n\n\n\n\n\n\nRevision of Factors\n\n\n\nLet’s start by looking at an example factor to see how it appears. This isn’t in our dataset; instead, we can create factor data using the factor() function.\n\nfactor(c(1, 2, 1, 1, 2),\n       labels = c(\"Male\", \"Female\"))\n\n[1] Male   Female Male   Male   Female\nLevels: Male Female\n\n\nThe underlying values in the factor are numbers, here 1 and 2. The labels are applied to the values in ascending order of those values, so 1 becomes “Male”, “2” becomes “Female”, etc. Here, we don’t need to specify the levels; if you don’t elaborate otherwise, R will assume that they are the same as the unique values.\nYou can also supply additional possible values, even if they haven’t been observed, using the levels argument:\n\nfactor(c(1, 2, 1, 1, 1),\n       levels = c(1, 2, 3),\n       labels = c(\"Male\", \"Female\", \"Non-binary\"))\n\n[1] Male   Female Male   Male   Male  \nLevels: Male Female Non-binary\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFactors are so common and useful in R that they have a whole {tidyverse} package to themselves! You already installed {forcats} with {tidyverse}, but you can check out the help documentation if you’d like to learn more about working with factors.\n\n\n\nConverting to Factors\nLabelled data is very easy to convert into factors, which is what R expects for many different types of analysis and plotting functions. Handy!\nFor an individual variable, we can use labelled::to_factor() to convert to factor.\nFor example, we can convert the gender variable to factor as follows, using the dplyr::mutate() function to make a change to the dataset. Remember that using the same variable name as we have done here means that the existing variable will be replaced (overwritten) in the dataset.\nIf we look at only this particular variable, we can see that its data type is now &lt;fctr&gt;, which is what we wanted.\n\nmil_data %&gt;% \n  dplyr::mutate(\n    gender = labelled::to_factor(gender)\n  ) %&gt;% \n  dplyr::select(gender)\n\n\n\n  \n\n\n\nIf you wanted a specific order of the levels, for plotting or similar, there’s also a sort_levels = argument described in the help documentation for labelled::to_factor().\nThat’s actually it! Whatever the value labels are in the variable, they will be converted into factor labels. Assuming your value labels are correct, no further editing is needed.\n\n\n\nNumeric\nFor continuous variables, we don’t need anything fancy to turn them into numeric data, because they technically already are. Instead, we just need to get rid of the labels using unclass().\nAs an example, we can use unclass() to convert belonging_1 to numeric, using the dplyr::mutate() function to make a change to the dataset again.\nIf we look at only this particular variable, we can see that its data type is now &lt;dbl&gt;, which is again what we wanted.\n\nmil_data %&gt;% \n  dplyr::mutate(\n    belonging_1 = unclass(belonging_1)\n  ) %&gt;% \n  dplyr::select(belonging_1)\n\n\n\n  \n\n\n\nFrom here, you can convert variables one by one as necessary…or, for a (much!) more efficient method, read on.\n\n\nEfficient Conversion\nDepending on the size of your dataset, converting your variables one by one to either factor or numeric might range from mild inconvenience to massive undertaking. In this optional section, we will make use of what we covered previously about selection helpers in combination with a new function, dplyr::across(), to convert multiple variables at once.\nThe general form is:\n\n1dataset_name %&gt;%\n  dplyr::mutate(\n2     dplyr::across(\n3        .cols = variables_to_change,\n4        .fn = function_to_apply\n     )\n  )\n\n\n1\n\nTake the dataset dataset_name, and then make a change to it as follows\n\n2\n\nApply to…\n\n3\n\nThe variables selected to be changed\n\n4\n\nA function to apply to each of the selected variables\n\n\n\n\nIn the first .cols argument, we use &lt;tidyselect&gt; syntax (i.e. selection helpers) to choose which variables we want to change.\nIn the second argument, the function or expression in function_to_apply is applied to each of the variables we’ve chosen.\nAs an example, we can change all of the mattering variables at once as follows:\n\nmil_data %&gt;%\n  dplyr::mutate(\n     dplyr::across(\n        .cols = starts_with(\"mattering\"),\n        .fn = unclass\n     )\n  )\n\n\n\n  \n\n\n\nHere I’ve used the dplyr::starts_with() function to choose which variables I want to change, and then each of those variables will have the unclass() function applied to them, converting them to numeric. This is exactly the same result as:\n\nmil_data %&gt;%\n  dplyr::mutate(\n    mattering_1 = unclass(mattering_1),\n    mattering_2 = unclass(mattering_2),\n    mattering_3 = unclass(mattering_3),\n    mattering_4 = unclass(mattering_4)\n  )\n\n…but with no risk of accidentally replacing variables with the wrong values due to copy/paste or typing mistakes."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#calculating-variables",
    "href": "workshops/dissertations/qualtrics_workshop.html#calculating-variables",
    "title": "Working with Qualtrics Data",
    "section": "Calculating Variables",
    "text": "Calculating Variables\nAs a final topic to get you ahead on your data analysis, this last section is a brief revision/reference of key topics you’ve already covered previously in your core methods modules. First, we’ll revise reverse-coding, to reverse the direction of responses on particular items as necessary. Once that’s done, we can create scores for each group of items that belong to the same subscale to get overall subscale scores to use in analysis.\n\n\n\n\n\n\nTip\n\n\n\nBoth reverse-coding and composite scores, including the underlying concepts and the code, have previously been covered in last year’s QQM Skills Lab.\n\n\n\nReverse Coding\nYou may or not have items in your questionnaire that are reverse-coded. There’s nothing in the data that will tell you this; you have to know what’s in your questionnaire, how the questions were designed, and which item(s) need reverse-coding. Make sure you check this carefully before you go on if working with your own data.\n\n\n\n\n\n\nWhat is reverse-coding?\n\n\n\n\n\nIn many multi-item measures, some items are reversed in the way that they capture a particular construct. For example, items on the State-Trait Inventory of Cognitive and Somatic Anxiety (STICSA, not in this example data) are worded so that a higher numerical response (closer to the “very much so” end of the scale) indicates more anxiety, such as item 4: “I think that others won’t approve of me”.\nHowever, reverse-coded items are intended to capture the same ideas, but in reverse. A reversed version a STICSA item might read, “I can concentrate easily with no intrusive thoughts.” In this case, a higher numerical response (closer to the “very much so” end of the scale) would indicate less anxiety. In order for these reversed items to be aligned with the other items on the scale, so that together they form a cohesive score, the coding of the response scale must be flipped: high becomes low, and low becomes high.\nIf the response scale is a numerical integer sequence, as this one is, then the simplest way to reverse-code the responses is to subtract every response from the maximum possible response plus one. For the STICSA, the response scale ranges from 1 to 4; the maximum possible response is 4, plus one is 5. So, to reverse-code the responses, we would need to subtract each rating on this item from 5. A high score (4) will be become a low score (5 - 4 = 1), and vice versa for a low score (5 - 1 = 4).\n\n\n\nIn order to reverse-code a variable, we will need to make use again of the dplyr::mutate() function for changing variables. For example, let’s reverse-code mattering_32.\nFirst, we need to know the maximum possible value in this variable. Using our data dictionary, we can see that values range from 1 to 7. So, to reverse-code, we should subtract each value from the max value plus one = 7 + 1 = 8.\nThen, we simply overwrite the existing variable with the new scores:\n\nmil_data %&gt;% \n  dplyr::mutate(\n    mattering_3 = 8 - mattering_3\n  )\n\nNote that it is recommended to overwrite the item, rather than create a new variable, so that you don’t accidently include the wrong or multiple versions in the next step.\n\n\nComposite Scores\nOnce all your items are cleaned and reverse-scored, you can finally create a composite score. For example, we have four mattering items, that we can combine into a “composite” score measuring general performance across all items. How this composite is calculated will depend on the questionnaire you’re using, but as many questionnaire subscale scores use mean scores3, we will demonstrate that here.\nTo do this, we need two new functions.\n\nThe first new function, dplyr::c_across(), provides an efficient way to select multiple variables to contribute to the calculation - namely, by using &lt;tidyselect&gt; selection helpers.\nThe second new function is actually a pair of functions, dplyr::rowwise() and dplyr::ungroup(). These two respectively impose and remove an internal structure to the dataset, such that each row is treated like its own group, and any operations are done within those row-wise groups.\n\nLet’s see the combination of these two in action to create a mattering composite score.\n\n\n\n\n\n\nImportant\n\n\n\nThe code below assumes a dataset structured so there is information from each participant on only and exactly one row in the dataset.\nIf your data has observations from the same participants on multiple rows, you will need to reshape your data or otherwise adapt the code to suit your data structure.\n\n\n\n1mil_data |&gt;\n2  dplyr::rowwise() |&gt;\n3  dplyr::mutate(\n    mattering_comp = mean(c_across(starts_with(\"mattering\")),\n                        na.rm = TRUE)\n  ) |&gt;\n4  dplyr::ungroup()\n\n\n1\n\nOverwrite the mil_data dataset with the following output: take the existing mil_data dataset, and then\n\n2\n\nGroup the dataset by row, so any subsequent calculations will be done for each row separately, and then\n\n3\n\nCreate the new mattering_comp variable by taking the mean of all the values in variables that start with the string “mattering” (ignoring any missing values), and then\n\n4\n\nRemove the by-row grouping that was created by rowwise() to output an ungrouped dataset.\n\n\n\n\n\n\n  \n\n\n\nIf you don’t feel comfortable using selection helpers, you can list variables instead inside c_across() using c() to combine them:\n\nmil_data |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate(\n    mattering_comp = mean(c_across(c(mattering_1, mattering_2, mattering_3, mattering_4)),\n                        na.rm = TRUE)\n  ) |&gt;\n  dplyr::ungroup()\n\nHowever, you’re strongly recommended to get the hang of selection helpers, since they are both easy to read and use and extremely versatile!\n\n\n\n\n\n\nRunning Code Out of Order\n\n\n\nUsing selection helpers like this does have a potential issue: it will give you the wrong answer if you run the same code more than once, or out of the order.\nIn the first example above using starts_with(), this command calculates the mean across all of the variables in the data whose names start with the string “mattering”. This will be mattering_1, mattering_2, mattering_3, and mattering_4.\nHowever, if you run the same code a second time, the command will again calculate the mean across all of the variables in the data whose names start with the string “mattering”. This will be mattering_1, mattering_2, mattering_3, mattering_4 - AND mattering_comp, which was created previously.\nAlthough the second example above enumerating individual variable names doesn’t have this danger, it’s still better to use the selection helpers, and simply never run your code out of order."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#exercises-conversion-and-wrangling",
    "href": "workshops/dissertations/qualtrics_workshop.html#exercises-conversion-and-wrangling",
    "title": "Working with Qualtrics Data",
    "section": "Exercises: Conversion and Wrangling",
    "text": "Exercises: Conversion and Wrangling\n\n\n\n\n\n\nExercise\n\n\n\nPrepare the mil_data dataset for analysis.\n\nProduce a final data dictionary and save it.\nConvert all categorical variables to factor, and all scale rating variables to numeric.\nReverse-code global_meaning_2.\nCreate composite scores for all of the subscale variables."
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#well-done",
    "href": "workshops/dissertations/qualtrics_workshop.html#well-done",
    "title": "Working with Qualtrics Data",
    "section": "Well done!",
    "text": "Well done!\nFrom here you can carry on with your data analysis: further cleaning, visualisation, and analysis. You’ve gained quite a few new skills today, so very well done indeed!"
  },
  {
    "objectID": "workshops/dissertations/qualtrics_workshop.html#footnotes",
    "href": "workshops/dissertations/qualtrics_workshop.html#footnotes",
    "title": "Working with Qualtrics Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the purposes of simplicity, we’re going to pretend that Likert and similar rating scales are “continuous”.↩︎\nNote that this item is NOT reversed on the real scale; this is only for practice!↩︎\nNote that averaging Likert data is controversial (h/t Dr Vlad Costin!), but widespread in the literature. We’re going to press boldly onward anyway to not get too deep in the statistical weeds, but if you’re using Likert scales in your own research, it’s something you might want to consider.↩︎"
  }
]