[
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Under Construction\n\n\n\nThis section is still under construction. Check back for more in the future!\n\n\nThis will be the home of any materials for one-off workshops outside of the PsychRlogy series. Workshops may focus on particular tasks, outputs, or analyses and will require some core skills in R (typically through the Essentials section of the PsychRlogy series).\nIf you have a suggestion for a workshop topic, you can drop it in the Suggestion Box on Canvas."
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html",
    "href": "workbooks/01_02_intro_workbook.html",
    "title": "01/02: IntRoduction",
    "section": "",
    "text": "Use the following code chunks to open the accompanying tutorial.\n\n\n\nrstudioapi::viewer('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/01_02_intro')\n\nError: RStudio not running\n\n\n\n\n\n\nutils::browseURL('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/01_02_intro')"
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#open-the-tutorial",
    "href": "workbooks/01_02_intro_workbook.html#open-the-tutorial",
    "title": "01/02: IntRoduction",
    "section": "",
    "text": "Use the following code chunks to open the accompanying tutorial.\n\n\n\nrstudioapi::viewer('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/01_02_intro')\n\nError: RStudio not running\n\n\n\n\n\n\nutils::browseURL('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/01_02_intro')"
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#orientation",
    "href": "workbooks/01_02_intro_workbook.html#orientation",
    "title": "01/02: IntRoduction",
    "section": "Orientation",
    "text": "Orientation\n\nThe RStudio Interface\n\nSource\n\n\nConsole"
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#errors",
    "href": "workbooks/01_02_intro_workbook.html#errors",
    "title": "01/02: IntRoduction",
    "section": "Errors",
    "text": "Errors\nType literally any gibberish, words, keysmashes etc. into the code chunk in the workbook and press Run (or Ctrl/Cmd + Shift + Enter).\n\nGlossoRlia: the Language of Errors"
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#types-of-data",
    "href": "workbooks/01_02_intro_workbook.html#types-of-data",
    "title": "01/02: IntRoduction",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric Data\nType any single number and run the code.\nHow does R handle commas within a number (e.g. to separate the thousands place from the hundreds)? How about full stops for decimals?\nAdd together your shoe size and the number of windows in the room you’re currently in.\nTry subtracting, dividing, and multiplying the same two numbers.\n\nVectors\nPrint out every whole number between 1 and 50.\nPrint out all the numbers 12 through 30; all of the numbers 23 through 55; and 36, all in one command.\n\n\nVector Calculations\nCreate a vector of every whole number between 37 and 63, and subtract 7 from each element.\n\n\n\nCharacter Data\nCreate a vector containing the first five animals you think of, then print the 3rd one.\n\n\nLogical Data\nWrite the following assertions in R:\n\n5 is greater than 10\n6 is less than 12\n27 is less than or equal to 27\n49 does not equal 93\n420 equals 42\n\nUse a single command to ask R whether the numbers 2 through 10 are less than or equal to 6."
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#class-and-coercion",
    "href": "workbooks/01_02_intro_workbook.html#class-and-coercion",
    "title": "01/02: IntRoduction",
    "section": "Class and Coercion",
    "text": "Class and Coercion\nUse the class() function to get R to print the values \"numeric\", \"logical\", and \"character\".\nWhat data type does R give you if you combine numbers and characters in c()?\nUse an as.*() function to convert the following vector of participant ages into numeric data: c(20, \"42\", \"36 years old\"). What do you think will happen to each element?"
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#objects",
    "href": "workbooks/01_02_intro_workbook.html#objects",
    "title": "01/02: IntRoduction",
    "section": "Objects",
    "text": "Objects\n\nCreating an Object\n\nNaming Objects\nThink of a research scenario familiar to you with two independent groups. You’re welcome to draw from your own research or expertise, but you should choose something with numerical scores. Some ideas include:\n\nReaction times on a button-pressing task from a control and an experimental group\nStatistics anxiety scores from first and second year UG students\nQuiz marks from students with practicals scheduled 9am and students with practicals at 6pm\n\nMake a note of the scenario you chose. Then, create two new objects: one that contains a vector of six scores from the first of the two groups, and the second that has six different scores from the second group.\nHint: Just make up some numbers that sound plausible!\n\n\n\nCalling an Object\nCall both of the objects you just created.\n\n\nUsing Objects\nCalculate the mean of each of the two sets of scores you created.\nCalculate the difference in the mean of each of the two sets of scores, and save this difference in a new object called quiz_diff.\nWhat is the class of these objects?\n\n\nOverwriting Objects\nFirst, let’s imagine we get three new participants in each condition of our previous study. Update the same two objects you created previously with three new scores each."
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#functions",
    "href": "workbooks/01_02_intro_workbook.html#functions",
    "title": "01/02: IntRoduction",
    "section": "Functions",
    "text": "Functions\n\nBasics and Help\nTry running the round() function.\nOpen the help documentation for the round() function by running ?round() or help(round) in the Console.\n\n\nArguments\n\nUnnamed Arguments\n\n\nNamed Arguments\n\n\n\nUsing Functions\nUse the round() function to round 64.3333333 to two decimal places.\nIf you prefer, you can do this with one of the means you calculated for your own scores earlier.\n\nPassing Multiple Values to Arguments\nBefore you go on, have a go using a single round() command to round 64.3333333, 59.5452, and 0.198 at once.\nHint: Refer to Vectors."
  },
  {
    "objectID": "workbooks/01_02_intro_workbook.html#quick-test-t-test",
    "href": "workbooks/01_02_intro_workbook.html#quick-test-t-test",
    "title": "01/02: IntRoduction",
    "section": "Quick Test: t-test",
    "text": "Quick Test: t-test\nBring up the help documentation for t.test() and use it to run a t-test comparing your two sets of scores.\nUsing the help documentation, re-run the t-test with equal variances assumed."
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/10_reshape.html",
    "href": "tutorials/psychrlogy/03_improvRs/10_reshape.html",
    "title": "10: Reshaping and Merging",
    "section": "",
    "text": "Under Construction\n\n\n\nThis tutorial is still under construction. Check back another time!"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/08_analysis.html",
    "href": "tutorials/psychrlogy/02_essentials/08_analysis.html",
    "title": "08: Analysis",
    "section": "",
    "text": "Under Construction\n\n\n\nThis tutorial is still under construction. Check back another time!"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/06_changes.html",
    "href": "tutorials/psychrlogy/02_essentials/06_changes.html",
    "title": "06: Mutate and Summarise",
    "section": "",
    "text": "Under Construction\n\n\n\nThis tutorial is still under construction. Check back another time!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "",
    "text": "This tutorial covers how to run, inspect, and report a linear model in R. For the report portion, we will cover some key features of dynamic reporting in Quarto and how to write and render professionally formatted documents."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#overview",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#overview",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "",
    "text": "This tutorial covers how to run, inspect, and report a linear model in R. For the report portion, we will cover some key features of dynamic reporting in Quarto and how to write and render professionally formatted documents."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#the-linear-model",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#the-linear-model",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "The Linear Model",
    "text": "The Linear Model\nIn this section, we will walk through the process of fitting, comparing, and reporting hierarchical linear models in R. This is not a statistics tutorial, so there will be minimal detail about how to understand or interpret the output of these commands. Instead, refer to Prof Andy Field’s {discovr} tutorials, which are the primary teaching materials for the same content in UG teaching at Sussex. All of the code and interpretation in the following section is from discovr_08, the GLM.\nThere are two key goals for this linear model walkthrough:\n\nCreate a detailed “cheatsheet” for a linear model analysis for future reference\nGet familiar with the {discovr} tutorials\n\nOf the two, the first is more important. I’d strongly recommend you open the relevant {discovr} tutorial and skim through it as you go so you’re familiar with what it contains. However, the following sections of this tutorial will present the same code and information with very abbreviated text, to serve as a quick reference.\nYou can also have them both open at once and refer to each!\n\n\n\n\n\n\nInstalling {discovr}\n\n\n\n\n\nIf you are working in the Posit Cloud workspace, the tutorials have already been installed. Please do NOT reinstall them!\nIf working elsewhere, use the following code to install the necessary packages in the Console:\n\nif(!require(remotes)){\n  install.packages('remotes')\n}\n\nremotes::install_github(\"profandyfield/discovr\")\n\nNote that there’s no need to call library(discovr) at the start of your document, since the tutorials don’t work in the document, but rather in RStudio itself.\n\n\n\n\n\n\n\n\n\nUsing the {discovr} tutorials\n\n\n\n\n\nProf Andy Field’s {discovr} tutorials provide detailed walkthroughs of both the R code and the statistical concepts of a variety of statistical analyses. They are a good place to look first to understand what your UG supervisees or advisees have been taught on a particular topic.\nThe tutorials are built in {learnr}, an interactive platform for learning and running R code. So, unlike the tutorial you’re currently reading, they must be run inside an R session.\nTo start a tutorial, open any project and click on the Tutorial tab in the Environment pane. You can run any tutorial from here, but the relevant one for the linear modelling we are working on now is discovr_08, “the GLM”. Scroll down to this tutorial and click the “Start Tutorial ▶️” button to load the tutorial.\nBecause {discovr} tutorials run within R, you don’t need to use any external documents; you can write and run R code within the tutorial itself. However, I strongly recommend that whenever you work with these tutorials, you write and run your code in a separate document, otherwise you will have no record of the code and output.\n\n\n\n\nData and Codebook\nToday’s data is the TeachingRatings dataset from the {AER} package. This dataset is built into the package, so you don’t need to read it in from anywhere. Instead, use the code below to get the data and rename it teach_tib (not necessary, just easier to type!).\n\ndata(\"TeachingRatings\", package = \"AER\")\nteach_tib &lt;- TeachingRatings\n\n\n\n\n\n\n\nInstalling {AER}\n\n\n\n\n\nAs usual, no need to install anything on the Cloud, but elsewhere install {AER} first in the Console:\ninstall.packages(\"AER\")\n\n\n\nYou can load the codebook in the Help viewer by running the following in the Console:\n?AER::TeachingRatings\n\n\nOne Predictor\nNow that we have some data, we can fit our first model with a single predictor. We will do this with the very hardworking lm() function in R, standing for “linear m odel”.\n\nGeneral Format\nThe lm() function has a lot of options (as you might expect, given the versatility and ubiquity of linear models!), but its basic format to fit a linear model with a single predictor is very simple:\nlm(outcome ~ predictor, data = dataset_name)\nIn this function, outcome ~ predictor is a formula expressing a (simplified version of) the linear model equation. Here, outcome is the name of the variable in dataset_name that contains the outcome or dependent variable y, and predictor is the name of the variable that contains the predictor or independent variable x.\n\n\n\n\n\n\nExercise\n\n\n\nUse the Codebook and the lm() function to fit a linear model predicting teaching evaluation score from beauty ratings. Save the resulting model in a new object called eval_lm.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\neval_lm &lt;- lm(eval ~ beauty, data = teach_tib)\n\nThat’s it!\n\n\n\n\n\nNow we have a new object that contains all the information about our model. We could simply call the name of this object, but the output doesn’t tell us anything besides the actual value of the b estimates (try it if you like!). Instead, we’ll use some useful functions from the {tidyverse} package {broom} to get the information we need.\n\n\nModel Fit\nTo get some common measures of model fit, we can use the function broom::glance(). The output includes \\(R^2\\), adjusted \\(R^2\\), and F and accompanying statistics in comparison to the null model (the mean of the outcome alone).\n\n\n\n\n\n\nExercise\n\n\n\nPut the eval_lm object into broom::glance() to get model fit statistics.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbroom::glance(eval_lm)\n\n\n\n  \n\n\n\n\n\n\n\n\nHelpfully, broom::glance() (and many other {tidyverse} functions) output tibbles, which means we can work with them as we already know how to do. In future tutorials, we’ll also learn more about changing and filtering tibbles that will make this even more useful. For now, we can simply note the information we get out of this function for future use.\n\n\nModel Parameters\nThe most common technique for getting a look at the details of the model is to use a function we’ve met before: summary(). The output isn’t great, but it does let us get a whole bunch of useful information quickly and directly.\n\n\n\n\n\n\nExercise\n\n\n\nPut the eval_lm object into summary() to produce the model summary output.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(eval_lm)\n\n\nCall:\nlm(formula = eval ~ beauty, data = teach_tib)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.99827    0.02535 157.727  &lt; 2e-16 ***\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05\n\n\n\n\n\n\n\n\n\n\n\n\n\nSignificance Codes\n\n\n\n\n\nThe output from many base-R {stats} functions, like this summary() output, include a line labeled Signif. codes that provide a key for understanding the notation given for significance levels in p-values.\nReading the key from left to right, we can see that a result is given three asterisks (***) when the p-value is between 0 and .001; two asterisks between .001 and .01; and so on.\nThis can be a useful visual check, especially because p-values that are very, very small are frequently expressed in scientific notation, which can make them more difficult to spot.\n\n\n\nTo get information about the b estimates for individual predictors, we can also use the function broom::tidy(). We could run this function without any other information, as we did with glance() above, but we’ll change one argument here in order to get 95% confidence intervals for b in the output as well.\nThe CIs are an improvement over summary(), and this information is a tibble rather than just text output, which will make it very handy for reporting in the future.\n\n\n\n\n\n\nExercise\n\n\n\nPut the eval_lm object into broom::tidy() and use the argument conf.int = TRUE to obtain confidence intervals.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbroom::tidy(eval_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nHierarchial Models\nNext, we may want to test the addition of further predictors in the model. We can then compare the more complex multi-predictor model to the single-predictor model.\n\n\n\n\n\n\nExercise\n\n\n\nFit a linear model with teaching evaluations as the outcome, and both beauty ratings and gender as predictors. Save the model output in a new object called eval_full_lm.\nThen, obtain model fit statistics and parameters as before.\nHint: to add a new predictor, you will need to literally add it (+) to the formula.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\neval_full_lm &lt;- lm(eval ~ beauty + gender, data = teach_tib)\n\n## Simple output with summary\nsummary(eval_full_lm)\n\n\nCall:\nlm(formula = eval ~ beauty + gender, data = teach_tib)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87196 -0.36913  0.03493  0.39919  1.03237 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.08158    0.03293  123.94  &lt; 2e-16 ***\nbeauty        0.14859    0.03195    4.65 4.34e-06 ***\ngenderfemale -0.19781    0.05098   -3.88  0.00012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5373 on 460 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \nF-statistic: 16.33 on 2 and 460 DF,  p-value: 1.407e-07\n\n## Tibble output and CIs with broom\nbroom::glance(eval_full_lm)\n\n\n\n  \n\n\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat About Interactions?\n\n\n\n\n\nThe models we’re describing here contain only additions, not interactions. So, you may be wondering, “What if I want to model more complex relationships between predictors?” If that’s what you’re trying to do now, you may want to jump ahead to discovr_10 for moderation and mediation.\nOtherwise, we will get there in this course eventually!\n\n\n\n\nComparing Models\nNow we have two models, one simpler with only a single predictor, and the other with two predictors. We might next want to test whether the more complex, two-predictor model is a significant improvement over the simpler model, in order to decide which model to retain. We can do this with the anova() function1 to compare the two models.\n\n\n\n\n\n\nWarning\n\n\n\nThe anova() function will only work for model comparison for particular models.\n\nThe models must be hierarchical. That is, the more complex model(s) must contain all of the predictor(s) present in the less complex model(s).\nAll models must be fit to the same dataset. If, for example, your first predictor has no missing values, but your second predictor had one, the model with using the second predictor would have been fit to a dataset of a different size than the model using only the first, and the anova() function will throw an error to this effect.\n\nIf you encounter this issue, you may need to inspect and clean your dataset before you proceed with analysis!\n\n\n\n\n\n\n\n\nExercise\n\n\n\nPut both model objects into the anova() function to find out which model to retain.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanova(eval_lm, eval_full_lm)\n\n\n\n  \n\n\n\n\n\n\n\n\nThe F-test is significant, indicating that the more complex two-predictor model is a significant improvement over the one-predictor model, so we will proceed with the two-predictor model.\n\n\nStandardised Bs\nYou may have noticed that the output we’ve seen so for only contains unstandardised model parameter estimates. If we want standardised Bs expressing the relationship between predictor(s) and outcome in standard deviation units, we can make use of the model_parameters() function from the {parameters} package to standardise our bs.\n\n\n\n\n\n\nExercise\n\n\n\nUse the standarize = \"refit\" argument in the parameters::model_parameters() function to obtain standardised Bs.\n\n\n\n\n\n\nWarning\n\n\n\nNote the spelling of standardize with a “z”! Spelling it with an “s” will not rescale the parameter estimates.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nparameters::model_parameters(eval_full_lm, standardize = \"refit\")\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAssumptions Checks\nAt Sussex, we teach a range of model diagnostics and robust model sensitivity tests in order to test model assumptions. We will look briefly at each of these in turn.\n\n\n\n\n\n\nTip\n\n\n\nRemember, there are more examples and longer explanations in the discovr_08 tutorial!\n\n\n\nResidual Plots\nTo begin, we can generate nicely formatted, customisable residual plots using the function ggplot2::autoplot(). However, it is essential to load the {ggfortify} package in order for this to work!\n\n\n\n\n\n\nExercise\n\n\n\nLoad the {ggfortify} package and use the autoplot() function to generate residual plots for eval_full_lm. Set the which argument to c(1, 3, 2, 4).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ggfortify)\n\nggplot2::autoplot(eval_full_lm, which = c(1, 3, 2, 4))\n\n\n\n\nIf you’re wondering what’s up with which, the plot() function that autoplot() is based on has a total of six plots available. Here I’ve chosen the two residual plots, the normal Q-Q and the Cook’s distance plot. Plots 5 and 6 have to do with leverage, which we don’t teach at UG level. To see them, simply add them to the which = argument.\n\n\n\n\n\nIf you want to customise the theme or look of these plots further, they are built with {ggplot2} so you can add or change anything about them using that package. We will come round later to a detailed exploration of data visualisations with {ggplot2}.\n\n\nDistribution of Standardised Residuals\nIn the UG core statistics modules at Sussex, we teach that normality is the least important of the assumptions of the linear model - the most important being additivity and linearity - so we do not generally worry too much about normally distributed standardised residuals, especially in large sample sizes. However, we do teach them how to evaluate the proportion of standardised residuals with values above ±1.96 (approximately 5%), ±2.56 (approximately 1%), and above ±3 (likely an outlier). For details on how to use broom::augment() to obtain standardised residuals and other model diagnostic measures like Cook’s distance, see the discovr_08 tutorial.\n\n\nRobust Models\nAt UG level, we teach robust models for two purposes:\n\nAs sensitivity tests to check assumptions. If a robust technique that adjusts for a particular issue, such as heteroscedasticity, results in a model that is substantially different from the unadjusted model, we might conclude that the unadjusted model did in fact have that particular issue.\nAs robust alternatives to the unadjusted model.\n\n\nRobust Parameter Estimates\nOur first robust model re-estimates the parameter estimates using robust techniques with the robust::lmRob() function. We can then compare the robust parameter estimates to the unadjusted ones obtained with lm() to find out if our estimates were biased, and report the robust parameter estimates if they were.\n\n\n\n\n\n\nExercise\n\n\n\nFit the same two-predictor model again with robust::lmRob() and compare the results to the unadjusted two-predictor model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\neval_lm_rob &lt;- robust::lmRob(eval ~ beauty + gender, data = teach_tib)\n\nbroom::tidy(eval_lm_rob, conf.int = TRUE)\n\n\n\n  \n\n\n## Compare to the unadjusted model\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nComparing the values of the two versions of the model, we can see that the parameter estimates have changed very little, so we might conclude that the unadjusted model was fine.\n\n\n\n\n\n\n\nRobust CIs and p-values\nTo test and adjust for heteroscedastic residuals, we can re-estimate the standard error using a robust method. To do this, we’ll use the parameters::model_parameters() function with the argument vcov = \"HC4\" as recommended in the {discovr} tutorial. To do this, use the unadjusted model object eval_full_lm as the first argument.\n\n\n\n\n\n\nExercise\n\n\n\nUse parameters::model_parameters() to re-estimate the SEs, CIs, and p-values, and compare the results to the unadjusted model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nparameters::model_parameters(eval_full_lm, vcov = \"HC4\")\n\n\n\n  \n\n\n## Compare to the unadjusted model\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nThe parameter estimates will not change, but the SEs, CIs, and p-values may. Here, the values are nearly identical and there are no major changes - that is, no predictors have become non-significant that were previously significant - so we might again conclude that the unadjusted model was not unduly biased.\n\n\n\n\n\n\n\nBootstrapping\nIf we had a small sample size, a final option would be to bootstrap the confidence intervals. To do this, we will again use parameters::model_parameters(), but this time with bootstrap = TRUE.\nNote: Sample size is not an issue with this dataset (N = 463).\n\n\n\n\n\n\nExercise\n\n\n\nProduce bootstrapped confidence intervals for the two-predictor model and compare to the unadjusted confidence intervals.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nparameters::model_parameters(eval_full_lm, bootstrap = TRUE)\n\n\n\n  \n\n\n## Compare to the unadjusted model\nbroom::tidy(eval_full_lm, conf.int = TRUE)\n\n\n\n  \n\n\n\nWe could once again note that there are no major changes, as previously, so the evidence of our checks suggests that the original, unadjusted model was not unduly biased."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#quarto",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#quarto",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto documents are a mix of text and code, the next generation of R Markdown documents. For our purposes, we will be using R within Quarto, but Quarto documents support the integration of many different coding languages, including Python, Julia, and Observable. If you’ve previously used Rmd (RMarkdown), Quarto is backwards-compatible and will able to render most documents with no issues.\n\n\n\n\n\n\nHelp with Quarto\n\n\n\nThe official Quarto Guide is extensive, detailed, and extremely helpful. It’s always the best first stop for any questions you have about using Quarto. Quarto also offers detailed tutorials.\nThis quick-reference to Markdown formatting is particularly helpful.\nProf Andy Field has also recorded a series of video guides to using Quarto that are used in UG teaching.\n\n\n\nGetting Started\nTo get some hands-on practice working with Quarto, we will create a new Quarto document from scratch. We will use the analysis code we’ve already written to create a nicely formatted report.\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new Quarto document via File &gt; New File &gt; Quarto Document.\nIf you like, you can give it a title; you will be able to change this later.\nThen, click “Create”.\n\n\nBy default, your new Quarto document will already have some settings and content to demonstrate how it works. Most importantly, you can see that there are three main types of information in this document:\n\nThe YAML header at the top, delineated by ---s, which contains information about how the document will be rendered\nThe body text, which contains regular (i.e. non-code) text\nThe code chunks, which contain code - in this case, specifically, R.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTo complete our setup, take the following steps:\n\nDelete everything in the new Quarto document except for the YAML header (i.e. all the text and code chunks).\nClear your Environment by clicking the broom icon in the Environment tab.\nRestart your R session (via Session &gt; Restart R).\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you have completed and saved your worksheet with all of your code before you do this!\n\n\n\n\n\n\n\n\n\n\nVisual vs Source Mode\n\n\n\n\n\nA new feature with Quarto is Visual mode, which is very like using Word or a similar word processing programme. All formatting can be applied and previewed in the document itself, using keyboard shortcuts or the familiar formatting buttons along the top toolbar.\nVisual mode also has an “insert anything” shortcut, /, that allows you to quickly insert elements into your document. If you type / at the start of a new line (or Ctrl/Cmd + / otherwise), a drop-down list of possible elements will appear, which you can navigate by scrolling or search by typing.\nAlternatively, you can toggle to Source mode by clicking the “Source” button at the very top left of the document pane. In Source mode you can edit the document using Markdown formatting. This allows more fine-tuned control of elements, but automatic formatting and the / shortcut don’t work in this mode.\nWhich you use is entirely personal preference - I toggle regularly between them depending on what I’m trying to do, so pick whichever works for you.\n\n\n\n\n\nCreating a Code Chunk\nTo prepare for our analysis, we will need a place to load packages and read in the data. Any executable code - that is, code that you want to run and do something - must be written in a code chunk and NOT in the body text2.\nThere are several ways to insert a new code chunk.\n\nIn Visual mode, by typing / and then Enter (since “R Code Chunk” is the first option)\nIn either mode, by:\n\nClicking the green “insert code chunk” button in the top right\nTyping out the literal code fencing: ```{r} ```\nUsing a keyboard shortcut.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new code chunk and code that does the following:\n\nLoad all the packages we used in the previous Linear Model section\nRead in the dataset\nCreate the two objects containing the linear models with one predictor and with two predictors.\n\nThen, run the code chunk.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLoad packages:\nIn the previous section, we used the following packages:\n\n{tidyverse}, or specifically {broom}, for tidying up the linear model output\n{ggfortify} for creating diagnostic plots\n{parameters} for standardising bs, re-estimating CIs, and boostrapping\n{robust} for robust parameter estimates\n\nRead in the data:\nUse the code from the Data and Codebook section of this tutorial to read in the dataset and rename it if you want.\nCreate the models:\nCopy and paste the code from your workbook document, or from the previous sections of this tutorial. You can also find all of the commands you have run your History tab (next to Environment).\nAltogether, your new code chunk should look like this:\n```{r}\nlibrary(tidyverse)\nlibrary(ggfortify)\nlibrary(parameters)\nlibrary(robust)\n\ndata(\"TeachingRatings\", package = \"AER\")\nteach_tib &lt;- TeachingRatings\n\neval_lm &lt;- lm(eval ~ beauty, data = teach_tib)\neval_full_lm &lt;- lm(eval ~ beauty + gender, data = teach_tib)\n```\nRun the code chunk by clicking the green “play” arrow, or by pressing Ctrl/Cmd + Shift + Enter while your cursor is inside the chunk.\n\n\n\n\n\n\n\nBody Text\n\nHeadings\nTo begin, we’ll use headings to map out our document. There are several ways to insert a new heading.\n\nIn Visual mode, by:\n\nUsing the text formatting dropdown. Click on “Normal” and select the heading level.\nUsing a keyboard shortcut, Ctrl + Alt + the number of the heading (e.g. 3 for a level 3 heading)\n\nIn either mode, by:\n\nTyping hashes at the start of a new line, followed by a space (e.g. ### creates a level 3 heading)\n\n\n\n\n\n\n\n\nWhy Use Headings?\n\n\n\n\n\nProperly formatted headings are strongly recommended for any documents you write, for a variety of reasons:\n\nThey automatically create the outline (to the right) and navigation menu (to the bottom) of your document for easy navigation\nThey can be automatically converted into a table of contents\nThey are a crucial accessibility feature for navigating the document via keyboard/screenreader, as well as providing clear visual structure.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate 2-3 level 2 headings in your document for the brief report of the linear model. You can choose anything you like, but the three I will refer to will be called “Model Comparison” (constructing and comparing the two models), “Assumptions Checks”, and “The Final Model” (reporting the final model in full).\n\n\n\n\nText\nAny new-line text will automatically be plain text. This can be anything you like, although in our current document, we are writing a mock-formal results section.\nHow to format body text depends on the mode you are in.\n\nIn Visual mode, by:\n\nUsing familiar keyboard shortcuts (e.g. Ctrl/Cmd + B for bold, Ctrl/Cmd + I for italics, etc.)\nUsing the formatting toolbar at the top of the document.\n\nIn Source mode, by using Markdown formatting.\n\n\n\n\n\n\n\nExercise\n\n\n\nUnder the first heading (which I have called “Model Comparison”), write a brief, journal-style description of the two models. Don’t fill in any statistics, but leave placeholders where the statistical reporting will go.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf you aren’t inclined to write your own, here’s a brief sample text to use.\n\nTwo linear models were constructed to investigate the influences on teaching evaluation ratings. The first model contained only instructor beauty ratings as a predictor, and teaching evaluation ratings as the outcome (R2 = ???, F(???, ???) = ???, p = ???). The second model added instructor gender as a second predictor with no interaction, both again predicting teaching evaluation ratings (R2 = ???, F(???, ???) = ???, p = ???). An ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(???, ???) = ???, p = ???).\n\n\n\n\n\n\n\n\n\nDynamic Reporting\nThe paragraph from the previous exercise was very clearly missing some key information: the actual statistical results of the linear model analysis. To start with, the last sentence should report which of the two models was better, based on the result of our F-test, rather than question marks. We could produce the output of this test, read it ourselves with our very own eyes/ears, and then type out the results by hand…but that is definitely not what we are going to do! Instead, we’ll look at a couple options for reporting the results dynamically. (We’ll come back to the R2s and Fs for each model in a moment.)\n\nInline Code\nOur first option is to use inline code to report the numbers. In order to do this, we first need to know what information we have available in the test output, so we can make use of it.\n\n\n\n\n\n\nTip\n\n\n\nThis section will make extensive use of $ subsetting, which was covered in Tutorial 03, and [] subsetting, which was covered in Tutorial 01/02.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIn your report document, in a new code chunk, use the broom::tidy() function to get a nice tibble of the anova() comparison between the two models, and save it in a new object called tidy_f. Then, print out this object in the Console.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe are creating the tidy_f object in a code chunk because we will want to use it again in our report.\nWe are calling this object in the Console because viewing its contents is only for our information/reference, and not something we want to appear or use directly in our finished document.\n\n## In a code chunk\ntidy_f &lt;- broom::tidy(anova(eval_lm, eval_full_lm))\n\n## In the Console\ntidy_f\n\n\n\n  \n\n\n\n\n\n\n\n\nThe tibble we get here is quite different from the “Analysis of Variance Table” text output that we saw earlier when we printed out the results of the anova() function on its own. We now have all the values we need to report these results in a conveniently subsettable tibble with nice R-friendly names.\nSo, let’s have a go getting at some of those values.\n\n\n\n\n\n\nExercise\n\n\n\nGet out the F-ratio of 15.0554899 from this object, and round it to two decimal places.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to get out the statistic variable, which we can do with $.\n\ntidy_f$statistic\n\n[1]       NA 15.05549\n\n\nThis returns a vector of two values, NA and 15.0554899. To index the second of these values, we need [] subsetting and the index number of the correct value.\n\ntidy_f$statistic[2]\n\n[1] 15.05549\n\n\nFinally, we can use the round() function to round to two decimal places.\n\nround(tidy_f$statistic[2], 2)\n\n[1] 15.06\n\n\n\n\n\n\n\nWe now have a bit of R code that produces a single number that we want to report in the text3. The issue is that code chunks contain code, and body text contains text, but we would like the code that we’ve written to print out its value in the text!\nThe solution is inline code, a small bit of R code written in the body text (“inline”) that will, when the document is rendered, automatically insert the right number. Inline code is written as follows, in the text: `r some_r_code`. Note the backticks; these are NOT apostrophes, and are typically located on the top left of a UK keyboard to the lefter of the 1/! key.\nFor our reporting, we will need to insert the inline code in exactly the spot where we would like the output of the code to appear. This particular bit of code produces the F-statistic, so we can replace the “???”s in our reporting where the value of the F-statistic should go:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(???, ???) = `r round(tidy_f$statistic[2], 2)`, p = ???).\n\nYou can check whether this has worked in two ways:\n\nPlace your cursor inside the backticks and press Ctrl/Cmd + Enter, as you would inside a code chunk. This will run the code and a small pop-up will show you what that code will produce when rendered.\nRender your document and see what appears!\n\n\n\n\n\n\n\nExercise\n\n\n\nUse inline code to replace all of the ???s in the F-test reporting to produce a final report in APA style.\nHint: Rounding p-values is a bit tricky. Check out the {papaja} package to see if you can find a function besides round() that will produce the p-value with the correct formatting.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAll of the other numbers should be straightforward, except for the p-value. Rounding to three decimal places with round() will result in a value of 0, which is not what we want. Instead, since the p-value is below .001, we want “&lt; .001”.\n{papaja} has the printp() function, which will do this exactly was we like (as well as containing a lot of other useful functions for rounding and printing in APA style!)\nYour final text may look like this:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(`r tidy_f$df[2]`, `r tidy_f$df.residual[2]`) = `r round(tidy_f$statistic[2], 2)`, p `r papaja::printp(tidy_f$p.value[2]`).\n\nNote that there’s no need for a “&lt;” symbol because papaja::printp() includes it automatically.\nThis will render as:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit for the second model compared to the first (F(1, 460) = 15.06, p &lt; .001).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Create a bit of inline code that will either report a significant or non-significant result depending on the value of p.\nHint: You may need to check out the ifelse() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we really want our reporting to be resilient, we want to remove or replace all the places where we have to manually remember to update the code if our results change. In this case, our reporting reads:\n\nAn ANOVA comparing the models indicated a significant improvement in model fit…\n\nBut if we wanted to reuse this code for another report, we would have to remember to update this depending on the actual value of p. OR, we can have R do it for us.\nFirst, we need to write a bit of R code that will evaluate whether the value of p is above or below a particular threshold, and then output the correct text. We could do this with ifelse(), a handy little base R function with three arguments. The first is a test returning a logical value (either TRUE or FALSE). If the test returns TRUE, the second argument is executed; if the test returns FALSE, the final argument is executed.\n\nifelse(\n1  tidy_f$p.value[2] &lt; .05,\n2  \"significant\",\n3  \"non-significant\"\n)\n\n\n1\n\nThe test: is the p-value for our F-test less than .05? (If you have a different \\(\\alpha\\)-threshold, you could hard-code it here or use an object you’ve defined previously for this comparison.)\n\n2\n\nWhat to do if the test returns TRUE: print “significant”.\n\n3\n\nWhat to do if the test returns FALSE: print “non-significant”.\n\n\n\n\n[1] \"significant\"\n\n\nWe can then take this command and replace the word “significant” in our report with this inline code:\n\nAn ANOVA comparing the models indicated a `r ifelse(tidy_f$p.value[2] &lt; .05, \"significant\", \"non-significant\")` improvement in model fit…\n\nNow we don’t have to worry about getting this right: our document, when rendered, will automatically insert the right word depending on the data.\n\n\n\n\n\n\n\nAutomatic Reporting\nAs helpful as inline code is (and I would recommend reporting all values dynamically/automatically wherever possible, so it is very useful!), you may have noticed that there was a lot of repetitive typing that also made the text itself quite difficult to read, as well lots of opportunities make typos or mistakes. Surely there’s a simpler way to do this sort of thing!4\nThere are, in fact, many simpler ways to do common tasks like this, which take advantage of the fact that an object created by a particular function will always have the same structure. One option is to make further use of the {papaja} package, which is designed for just this purpose.\n\n\n\n\n\n\nExercise\n\n\n\nUse the {papaja} documentation to fill in the statistical reporting for each of the linear models (i.e., R2, F, and p) using only one piece of inline code for each.\nWhen you’re done, render your document to see the results!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe {papaja} documentation illustrates the process using t.test(), which works the same way as lm(). The key here is to use the original objects containing the models you want to report.\nLet’s have a look at the first of the two models and see what the papaja::apa_print() function gives us.\n\npapaja::apa_print(eval_lm)\n\n$estimate\n$estimate$Intercept\n[1] \"$b = 4.00$, 95\\\\% CI $[3.95, 4.05]$\"\n\n$estimate$beauty\n[1] \"$b = 0.13$, 95\\\\% CI $[0.07, 0.20]$\"\n\n$estimate$modelfit\n$estimate$modelfit$r2\n[1] \"$R^2 = .04$\"\n\n$estimate$modelfit$r2_adj\n[1] \"$R^2_{adj} = .03$\"\n\n$estimate$modelfit$aic\n[1] \"$\\\\mathrm{AIC} = 756.65$\"\n\n$estimate$modelfit$bic\n[1] \"$\\\\mathrm{BIC} = 769.06$\"\n\n\n\n$statistic\n$statistic$Intercept\n[1] \"$t(461) = 157.73$, $p &lt; .001$\"\n\n$statistic$beauty\n[1] \"$t(461) = 4.13$, $p &lt; .001$\"\n\n$statistic$modelfit\n$statistic$modelfit$r2\n[1] \"$F(1, 461) = 17.08$, $p &lt; .001$\"\n\n\n\n$full_result\n$full_result$Intercept\n[1] \"$b = 4.00$, 95\\\\% CI $[3.95, 4.05]$, $t(461) = 157.73$, $p &lt; .001$\"\n\n$full_result$beauty\n[1] \"$b = 0.13$, 95\\\\% CI $[0.07, 0.20]$, $t(461) = 4.13$, $p &lt; .001$\"\n\n$full_result$modelfit\n$full_result$modelfit$r2\n[1] \"$R^2 = .04$, $F(1, 461) = 17.08$, $p &lt; .001$\"\n\n\n\n$table\nA data.frame with 6 labelled columns:\n\n       term estimate     conf.int statistic  df p.value\n1 Intercept     4.00 [3.95, 4.05]    157.73 461  &lt; .001\n2    Beauty     0.13 [0.07, 0.20]      4.13 461  &lt; .001\n\nterm     : Predictor \nestimate : $b$ \nconf.int : 95\\\\% CI \nstatistic: $t$ \ndf       : $\\\\mathit{df}$ \np.value  : $p$ \nattr(,\"class\")\n[1] \"apa_results\" \"list\"       \n\n\nWe’ve got a huge number of options here, but for this exercise we wanted R2, F, and p. All three are given under $full_result$modelfit$r2. We will need to save the output from apa_print() into an object, then we can subset it using inline code:\n\neval_lm_out &lt;- papaja::apa_print(eval_lm)\neval_full_lm_out &lt;- papaja::apa_print(eval_full_lm)\n\n\nTwo linear models were constructed to investigate the influences on teaching evaluation ratings. The first model contained only instructor beauty ratings as a predictor, and teaching evaluation ratings as the outcome (`r eval_lm_out$full_result$modelfit$r2`). The second model added instructor gender as a second predictor with no interaction, both again predicting teaching evaluation ratings (`r eval_full_lm_out$full_result$modelfit$r2`).\n\nWhich will render as:\n\nTwo linear models were constructed to investigate the influences on teaching evaluation ratings. The first model contained only instructor beauty ratings as a predictor, and teaching evaluation ratings as the outcome (\\(R^2 = .04\\), \\(F(1, 461) = 17.08\\), \\(p &lt; .001\\)). The second model added instructor gender as a second predictor with no interaction, both again predicting teaching evaluation ratings (\\(R^2 = .07\\), \\(F(2, 460) = 16.33\\), \\(p &lt; .001\\)).\n\n\n\n\n\n\n\n\nTable Formatting\nNext, we will jump to the “Final Model” heading and have a look at how to turn our final model output into a nicely formatted table. Once again, {papaja} provides a quick and beautiful solution for reporting, so let’s use it again.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the {papaja} help documentation, produce a nicely formatted table of the final model, presenting the parameter estimates, p-values etc. for each predictor under the third (“Final Model”) heading.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe already have the necessary object, eval_full_lm_out, from the previous task. We just need to subset it as described in the help documentation.\nThis command should go in a new code chunk, wherever you want the table to appear in your document.\n\npapaja::apa_table(eval_full_lm_out$table)\n\n\n(#tab:unnamed-chunk-21)\n\n\n**\n\n\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n4.08\n[4.02, 4.15]\n123.94\n460\n&lt; .001\n\n\nBeauty\n0.15\n[0.09, 0.21]\n4.65\n460\n&lt; .001\n\n\nGenderfemale\n-0.20\n[-0.30, -0.10]\n-3.88\n460\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: {papaja} isn’t the only package to provide easy formatting for commonly reported tests. Have a go creating this table again using the nice_table() function from the {rempsyc} package.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe nice_table() function can be for tables generally, but it can apply specialised formatting for model tables created with broom, if we use the broom = argument to tell the function what formatting template to apply.\n\nrempsyc::nice_table(broom::tidy(eval_full_lm, conf.int = TRUE), broom = \"lm\")\n\n\nTermbSEtp95% CI(Intercept)4.080.03123.94&lt; .001***[4.02, 4.15]beauty0.150.034.65&lt; .001***[0.09, 0.21]genderfemale-0.200.05-3.88&lt; .001***[-0.30, -0.10]\n\n\n\n\n\n\n\n\n\nCross-Referencing\nAs anyone who has had to create a long document with lots of tables and figures knows, keeping track of the numbering is a huge pain, especially when, for instance, a reviewer asks you to add something partway through and then everything has to be renumbered.\nThe good news is that Quarto can take care of figure and table numbering automatically. There are two steps to this:\n\nInclude a label in the relevant code chunk, using the prefix fig- for figures and tbl- for tables.\nRefer to the figure or table in the text using @.\n\n\n\n\n\n\n\nExercises\n\n\n\nUsing the Quarto help documentation, write a short introductory sentence under the “Final Model” heading and refer to the final model table with a cross-reference.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, add a label and caption to the code chunk from the previous task that produces the model table.\n```{r}\n#| label: tbl-final-model\n#| tbl-cap: \"The final model predicting teaching evaluation ratings from instructor beauty and gender.\"\n\npapaja::apa_table(eval_full_lm_out$table)\n```\nYou can of course write whatever you like, or borrow the text below, but use whatever label you gave the table code chunk to refer to the table.\n\nThe final model with two predictors is presented in full in @tbl-final-model.\n\nAltogether, it should render as follows:\n\nThe Final Model\nThe final model with two predictors is presented in full in Table 1.\n\n\npapaja::apa_table(eval_full_lm_out$table)\n\n\n(#tab:tbl-final-model)\n\n\n**\n\n\n\nTable 1: The final model predicting teaching evaluation ratings from instructor beauty and gender.\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n4.08\n[4.02, 4.15]\n123.94\n460\n&lt; .001\n\n\nBeauty\n0.15\n[0.09, 0.21]\n4.65\n460\n&lt; .001\n\n\nGenderfemale\n-0.20\n[-0.30, -0.10]\n-3.88\n460\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Complete the final “Assumptions Checks” section summarising the checks and using figure cross-referencing to insert and refer to the diagnostic plots.\nHint: To report the exact maximum value of Cook’s distance, you will also need to refer to discovr_08 for how to use broom::augment().\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can write whatever you like, but here’s a suggestion with the plot included.\n\nAssumptions Checks\nWe next assessed the model with two predictors for any evidence of bias. Residual plots (@fig-diag-plots) did not indicate any outstanding issues with normality, linearity, or heteroscedasticity. There was also no evidence of influential cases, as the max value of Cook’s distance was `r round(max(broom::augment(eval_full_lm)$.cooksd), 2)`. Robust models were also fitted as sensitivity checks. Robust parameter estimates estimated using the {robust} package were minimally different from the unadjusted parameter estimates. Similarly, robust HC4 standard errors estimated using the {parameters} package yielded confidence intervals and p-values very similar to the unadjusted values. Therefore, we will proceed with the unadjusted two-predictor model as our final model.\n\n```{r}\n#| label: fig-diag-plots\n#| fig-cap: \"Diagnostic plots for the two-predictor model.\"\n\nggplot2::autoplot(eval_full_lm, which = c(1, 3, 2, 4))\n```\n\n\n\n\n\n\n\n\nRendering\nAfter all this work to analyse, interpret, and report, it’s finally time to produce the final document. The process of turning a Quarto document into some output format - including running all the code and applying all the formatting - is called rendering (previously knitting with RMarkdown).\n\n\n\n\n\n\nExercise\n\n\n\nRender your report document using the “Render” button at the top of the document, or by using the keyboard shortcut Ctrl/Cmd + Shift + K5.\n\n\n\nGlobal Options\nAt the moment, your report may not be as clean as we’d like it to be: there are likely messages from R floating around and code all over the place, whereas for a formal report, we of course only want to show the final output. This is where the YAML header comes in.\nThe default YAML header contains only a few explicit settings, and if you haven’t changed anything, probably looks like this:\n---\ntitle: \"Untitled\"\nformat: html\neditor: visual\n---\nBy adding options to the YAML header, we can determine how the document as a whole is rendered. Here are the common ones that I use on the regular:\n\n---\ntitle: \"Linear Model Report\"\nformat:\n  html:\n1    toc: true\neditor: visual\n2self-contained: true\n3execute:\n4  echo: false\n5  warning: false\n6  message: false\n---\n\n\n1\n\nAutomatically produce a table of contents (ToC) from the document headings.\n\n2\n\nCombine all the files, images, stylesheets etc. into a single output document. Necessary if you want to send an HTML file to someone else and have it look as it should!\n\n3\n\nSet default behaviour for all code chunks as follows\n\n4\n\nRun code and show output, but do not show the code itself.\n\n5\n\nDo not show any warnings produced by code.\n\n6\n\nDo not show any messages produced by code.\n\n\n\n\nThe requirements for each document will change depending on its purpose.\n\n\n\n\n\n\nTip\n\n\n\nThe Quarto help documentation, as usual, has a complete list of YAML options, including how to set default behaviour for figures and other settings.\n\n\n\n\nCode Chunk Options\nGlobal options apply to the entire document, but you may want to change these settings for individual code chunks to override the default settings in the YAML.\nFor example, you may have a code chunk containing some processing code that you used to view and clean your data. If your global execution option is set to echo: false, the code output would still appear, although the code itself would be hidden. If, for this particular code chunk, you don’t want the output to appear either, you can override the global option with a local option for that chunk only.\nLocal code chunk options appear as “hashpipe” (#|) comments, as we have seen earlier with labels. They use the same syntax as YAML, but the settings only apply to individual code chunks. Any settings that aren’t explicitly changed within the chunk are inherited from the YAML settings.\nFor this example, we could set a local code chunk option include: false which will prevent the output from appearing in the document.6\n```{r}\n#| include: false\n\nsome_code_doing_cleaning_and_processing\n```\n\n\nOutput Formats\nFor these tutorials, we will generally stick to HTML, as it’s the most painless of the rendering options. However, you will likely find yourself wanting to produce some other type of document, which you can easily7 do from the same Quarto document.\nTo render to a different format, change the YAML format: setting to a different output.\n\n\n\n\n\n\nTip\n\n\n\nAs per, the Quarto guide on output formats has all the information you need!\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRender your linear model report to a Word document.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSimply update the format: html YAML option to format: docx and render your document. Note that the format options are usually named after the file extension rather than the name of the programme necessarily (e.g. docx rather than word.)\n\n\n\n\n\n \nAnd there we have it! You now have a complete example of a linear model report, rendered into both HTML and Word, to refer to. It’s amazing how much you’re able to do after just a few weeks!\nThis is the end of the FundRmentals section of the course. If you’re so inclined, we’ll see you in the Essentials section, which will cover data wrangling and cleaning, and running and reporting many more statistical analyses."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#footnotes",
    "href": "tutorials/psychrlogy/01_fundRmentals/04_lm.html#footnotes",
    "title": "04: Reporting Linear Models with Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSomewhat confusingly, this is not the function we teach UGs to perform ANOVAs! See discovr tutorials 11, 12, 15, and 16 for a detailed guide through the afex package for running linear models with categorical predictors.↩︎\nMostly true, except for inline code!↩︎\nGenerally, inline code should only ever produce a single value, otherwise the formatting can get interesting. This value, however, could longer than a single number, if you want to get creative with your dynamic reporting!↩︎\nHere we’re going to briefly encounter a principle that will come up again and again, namely: If you have to copy and paste more than once, use a function instead.↩︎\nIt seems obvious that this should be R instead of K, but remember this made perfect sense when it was called “knitting”!↩︎\nFor writing tutorials, I make extensive use of eval: false, which includes the code in the output but does not attempt to run the code. This allows me to write all kinds of nonsense code without R getting stroppy and throwing errors all over the place!↩︎\nDepends crucially on what you consider to be “easy”, especially when dealing with PDFs!↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#orientation",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#orientation",
    "title": "01/02: IntRoduction",
    "section": "Orientation",
    "text": "Orientation\nWelcome to the first PsychRlogy tutorial! Let’s jump right in and get started.\nThis tutorial is designed to accompany a “Workbook” document, which is already available for you on the Posit Cloud workspace for this course. To access it, navigate to Posit Cloud and open the course workspace. If you haven’t joined the workspace yet, use the join link on the Posit Cloud page on Canvas.\nIn the workspace, you will see a list of projects available. These will have an “ASSIGNMENT” banner next to them. When you click on these projects, a new copy of the project will be generated for you to work in.\n\n\n\n\n\n\nClick on the project with the same name as this tutorial: “01/02 IntRoduction”.\n\n\n\n\nThe RStudio Interface\nYou are now looking at the RStudio IDE itself. It is possible to use R directly with minimal interface, but using an integrated development environment like RStudio comes with a lot of additional convenience to make working with R smoother, easier, and more efficient.\n\n\n\n\n\n\nImportant\n\n\n\nIt’s beyond the scope of this tutorial to cover all of the options and tools available in RStudio. Here we’ll focus only on the minimum to get started and build outward from there.\nFor a more complete tour, try this playlist of Andy Field’s RStudio tutorials.\n\n\nBefore anything else, open up the workbook for this tutorial. If you’re a little shaky on where this is, skip down to the screenshot below!\n\n\n\n\n\n\nFind the document named “01_02_intro_workbook.qmd” in the Files window. Click on it to open it.\n\n\n\nYou should now be looking at a dashboard-like interface with four main windows, each with a bunch of tabs across the top, like pictured below. We’ll refer to each window by these names, which come from the most important or commonly used tab in each window.\n\nWe will eventually work with all four of these windows, but if you’ve opened the workbook, you can ignore the Environment and Files windows for now. We’ll be focusing only on the other two: Source and Console.\n\nSource\nThe Source window is where any documents you want to create or work on will open up. What you have open now is a Quarto document, a type of document that integrates regular text and code. A Quarto document has three main elements: the YAML header, body text, and code chunks.\n\n\n\n\n\nIgnore the YAML header for now; we’ll come back to Quarto documents, including the YAML, in depth in Tutorial 04.\nYou can use the body text portion of a Quarto document more or less like you would a document in Word (or your word processor of choice). In the body of the document, you can write and format any text you want - notes, questions, thoughts, ideas, comments, etc. This workbook already contains all the headings from this tutorial to help you get organised, but please do delete or edit as you see fit.\nThe last element is the code chunk, which is where all R code should be written. Code chunks have a contrasting background colour, an {r} in the upper left corner, and two green buttons in the upper right. The one that looks like a green “play” button will run all the code in that chunk. Code chunks will NOT handle any non-code text, unless it’s a comment (i.e. preceded by one or more #s.)\n\n\nConsole\nThe Console is deceptively simple: just some stuff about the R version and acknowledgements, and the &gt; symbol with a flashing cursor after it, waiting for you to type something. However, the Console is the heart of R, where anything you want to do actually happens. Every command that you type, anything you want R to do, goes through here.\nSo, already we have two places we could write R code: in a code chunk or in the Console. How do we know where to start?\n\n\n\n\n\n\nCode Chunk or Console?\n\n\n\nFor the purposes of learning, by default, it’s best that you write all code into the workbook code chunks, so you have a detailed record of everything you’ve tried - even if it doesn’t work!\nOutside of these sessions, whether to write a bit of R code in a code chunk in Quarto, or in the Console, largely depends on a single question: do you want to use this same line of code again in the future?\nIf yes, write the code in a code chunk. By adding the code to a document like Quarto, we are creating a record of all the steps we’ve taken in whatever task we are working on. Assuming we want to be able to use and refer to that code again in the future, it should go into a document.\nIf no, write the code in the Console. Code written directly in the Console isn’t saved or documented anywhere1. Some common uses of the Console are:\n\nInstalling/updating packages\nOpening help documentation\nDrafting or testing code to go into a document.\n\nSo, I often use the Console to test my code, building it up bit by bit, until it does what I want it to do. Then, when I’ve puzzled out the solution, I add it into a code chunk.\n\n\n\n\n\n\nStill Confused? How About a Convoluted Analogy?\n\n\n\n\n\nImagine that working in R is like cooking, and writing a sequence of commands to, for example, clean a new dataset is like developing a new recipe.\nIf you’re developing a recipe, you likely wouldn’t just sit down and write down the final version if you’ve never tried the recipe before. Instead, you might experiment a bit with each step to see what works and what doesn’t.\nAlong the way, you may write notes to yourself: “Maybe try cumin?”, “Buy more kefir”, “This time was 2 tsp salt, too salty!” Those notes are a part of the development process, relevant to what you’re doing now and helpful to try out or note down ideas, but they wouldn’t go in your final recipe. Those behind-the-scenes and under-development bits are the code you’d write in the Console.\nWhen you find a technique or temperature or seasoning that works, you might add it as a step in your recipe. That final recipe, the steps that actually work the way you want, are the code in your code chunks.\n\n\n\n\n\nIf that seems like a lot to remember, don’t worry - we’ll practice both and let you know clearly if it should be one or the other.\nRight, enough orientation - let’s get cracking!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#errors",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#errors",
    "title": "01/02: IntRoduction",
    "section": "Errors",
    "text": "Errors\nBefore we go any further, an affirmation: you will, inevitably, make typos and errors using R. You will write commands that make sense to you that R doesn’t understand; and you will write commands that don’t make sense to you, that R does understand. Errors are an essential and unavoidable part of learning R, so let’s start there.\n\n\n\n\n\n\nExercise\n\n\n\nType literally any gibberish, words, keysmashes etc. into the code chunk in the workbook and press Run (or Ctrl/Cmd + Shift + Enter).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Keysmash!\naslavb;lj aew aljvb\n\nError: &lt;text&gt;:2:11: unexpected symbol\n1: ## Keysmash!\n2: aslavb;lj aew\n             ^\n\n\n\n## Words!\nAm I a coward? Who calls me villain?\n\nError: &lt;text&gt;:2:4: unexpected symbol\n1: ## Words!\n2: Am I\n      ^\n\n\n\n## Emojis! \n¯\\_(ツ)_/¯\n\nError: &lt;text&gt;:2:1: unexpected input\n1: ## Emojis! \n2: ¯\n   ^\n\n\n\n\n\n\n\nWell, that went about as well as expected.\nIf you haven’t tried this yet, and your pristine document is just ominously staring at you, I’m serious - punch your keyboard if you have to, or let your cat walk on it, or play it as if it were a piano, and press Enter. There’s two important things to learn from this:\n\nTo ask R to do something, you must write commands out somewhere (in a code chunk, in the Console) and then run them.\nEventually, inevitably, something that you type WILL produce an error.\n\nFrom our keysmashing above, you will have seen that aslavb;lj aew aljvb, Am I a coward? Who calls me villain?, and ¯\\_(ツ)_/¯ are not valid commands in R. Although each of these has a communicative function for humans, R can’t understand them. In order to get the answer that we want, we have to ask R to do something in a way it can understand, by writing commands it can parse using the R language.\nJust like learning any other language, learning to communicate with R takes time and practice, and it can be very frustrating when you and R can’t seem to understand each other. However, one advantage of learning to talk to R vs learning to speak a human language is that R always works the same way. Even if the response it gives doesn’t make sense to you, there’s always a logical reason for what it does.\n\n\n\n\n\n\nGlossoRlia: the Language of Errors\n\n\n\nVery often, R communicates with you via errors. Unlike many other computer programmes you might be familiar with, an “error” in R doesn’t (usually!) mean a catastrophic failure and/or potential loss of hours of work2. Rather, whatever you’ve asked R to do, it’s essentially replied, “Sorry, I can’t do that.” Two of the most important skills you can develop early on with R is to treat errors as feedback, and to learn to read and recognise errors.\nTreat errors as feedback: Errors aren’t (just) an annoyance, although running into lots of errors, usually just when you don’t want them the most!, can be incredibly frustrating. However, errors are just R’s way of telling you that it can’t do what you’ve asked it to do. If you’re trying to work out how to get R to do something, then the “no” of an error rules out whatever you’ve just tried. Rather than setting out to avoid errors, and thinking of an error as a “failure” to “do it right”, it’s much better to expect errors, and make use of them as part of the code-writing process.\nLearn to read and recognise errors: Errors often contain useful information about what’s gone wrong and how to fix it. At minimum, errors usually contain the following information:\n\nWhere in the document the error occurred, or which command/bit of code produced the error\nSome sort of message about what went wrong.\n\nErrors vary wildly in understandability and helpfulness, from highly technical jargon to friendly and conversational with suggestions for fixing common problems. Even the obtuse ones, though, will become familiar with time.\n\n\nLet’s take a look at how we might interpret the error we produced above.\n\naslavb;lj aew aljvb\n\nError: &lt;text&gt;:1:11: unexpected symbol\n1: aslavb;lj aew\n              ^\n\n\nHere R doesn’t need to tell us where the error is, because there’s only one thing it’s trying to run. We do have the text of the error, though: object 'aslavb' not found. We’ll come to objects later on in this tutorial, but in short R is looking for some information labeled aslavb and can’t find it (because it’s just a keysmash!). This is an example of an error that comes up all the time - often when you’ve made a typo, or forgotten to create or store information in an object properly. For me at this point, as someone who forgets or mistypes things on the reg, this error is a familiar friend!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#types-of-data",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#types-of-data",
    "title": "01/02: IntRoduction",
    "section": "Types of Data",
    "text": "Types of Data\nOne key concept for using R is the different ways it categorises data. “Data” here means any piece of information you put into R - a word, a number, the result of a command or calculation, a dataset, etc. Depending on the type of data you have, R will treat it differently, and some operations only work on certain types of data. So, let’s have a look at how R encodes and deals with different types of data. Here we’ll cover three of the most common and important: numeric, character, and logical. As we do so, we’ll practice some core skills in R.\n\nNumeric Data\nThe first, and perhaps most obvious, type of data in R is numbers. We’ll start by doing some calculations with common mathematical operators.\n\n\n\n\n\n\nExercise\n\n\n\nType any single number and run the code.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Pick any number at random\n\n3958\n\n[1] 3958\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that you can run all the code in a code chunk by pressing Ctrl/Cmd + Shift + Enter on your keyboard, or by clicking the green “play” arrow in the top right corner of the code chunk.\nYou can also run only a particular line of code, or something that you’ve highlighted, by pressing Ctrl/Cmd + Enter.\n\n\nThis might be what you’d expect. We’ve essentially asked R, “Give me 3958” (or whatever number you put in) and R obliges. The only thing that might be a surprise is the [1] marker, called an index. Basically, R has replied, “The first thing ([1]) that you asked me for is 3958.” We’ll come back to indices in a moment.\n\n\n\n\n\n\nExercise\n\n\n\nHow does R handle commas within a number (e.g. to separate the thousands place from the hundreds)? How about full stops for decimals?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n3,958\n\nError: &lt;text&gt;:1:2: unexpected ','\n1: 3,\n     ^\n\n\n\n3.958\n\n[1] 3.958\n\n\n\n\n\n\n\nSo, commas within numbers throw an error. This is because commas have an important role to play in the syntax of R, so long numbers must be inputted into R without any punctuation. However, full stops to mark decimal places are just fine.\n\n\n\n\n\n\nGrammar Check\n\n\n\n\n\nTry for a moment switching to Source mode by clicking the Source button in the upper left hand of your Quarto document. You can see that RStudio helpfully marks out the part of the code that isn’t parsable (not in “grammatical” R) with a red ❌ next to the line number, and squiggly red underlining, likely familiar from word processing programmes, under the part of the code that’s causing the issue. It won’t do this for every error, but it’s very helpful for finding “grammatical” errors like extra or missing brackets or misplaced commas.\n\n\n\nNext, let’s try doing some maths.\n\n\n\n\n\n\nExercise\n\n\n\nAdd together your shoe size and the number of windows in the room you’re currently in.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n40 + 8\n\n[1] 48\n\n\n\n\n\n\n\nImportant to note here is that we don’t need to type an = to get the answer, just the equation we want to solve and then run the code. Again, we’ve asked R, “Give me 40 + 8” (or whatever numbers you chose) and R replies with the answer.\nYou will not be surprised to learn that you can use R as a calculator to subtract, divide, and multiply as well.\n\n\n\n\n\n\nExercise\n\n\n\nTry subtracting, dividing, and multiplying the same two numbers.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n40 - 8\n\n[1] 32\n\n40 * 8\n\n[1] 320\n\n40 / 8\n\n[1] 5\n\n\n\n\n\n\n\nThis exercise also shows us something useful: you can run multiple commands within the same code chunk. While spaces are not meaningful to R (that is, 40 - 8 and 40-8 and 40      -8 are all read the same way), new lines have an important role to play separating out commands. Each command must have its own new line.\n\nVectors\nLet’s imagine I am running a study, and I want to generate some simple participant ID numbers to keep track of the order that they completed my study. I had 50 participants in total. I could do this by typing every number out one by one, but this is exactly the kind of tedious nonsense that R is great at. Instead, we’ll use the operator :, which means “every whole number between”.\n\n\n\n\n\n\nExercise\n\n\n\nPrint out every whole number between 1 and 50.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n1:50\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n\n\n\n\n\n\nNotice that the indices mentioned earlier have come up again. The first element after the [n] index is the nth element. Let’s have a look at this some more.\n\n\n\n\n\n\nExercise\n\n\n\nPrint out all the numbers 12 through 30; all of the numbers 23 through 55; and 36, all in one command.\n\n\nYou may have tried something like this:\n\n12:30\n\n [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n23:55\n\n [1] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n[26] 48 49 50 51 52 53 54 55\n\n36\n\n[1] 36\n\n\nAs you can see from the indices, this is three separate commands, because the numbered indices start over from [1] each time. However, we want all those numbers in a single command. To do this, we’ll use a function called c().\nThis is our first contact with functions in R, and we’ll explore how they work more later on. To use this one, type it out, then inside the brackets, put the numbers you want to collect (or concatenate, or combine), with different groups separated by commas.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc(12:30, 23:55, 36)\n\n [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 23 24 25 26 27 28\n[26] 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n[51] 54 55 36\n\n\n\n\n\nAs you can see from the numbered indices this time, when I put the numbers I want inside the function c(), separated by commas, R collects all of the numbers into a single series of elements, called a vector.\nActually, we’ve been looking at vectors this whole time. Any series of pieces of information in R is a vector (but see Tip box on vectors and elements). When we were looking at single numbers (like 3958 above), we were still getting a vector back from R, but it was a vector with only one element, and thus only [1].\nIf I want the nth element in the vector we’ve just created, (say, the 33rd), I can get it out by indexing with square brackets.\n\nc(12:30, 23:55, 36)[33]\n\n[1] 36\n\n\nWhat I’ve essentially asked R is, “Put all of these numbers into a single vector, and then give me the 33rd element in that vector.” As it turns out, the 33rd element in that vector of numbers is 36.\n\n\n\n\n\n\nDefinition: Vectors\n\n\n\nA vector is essentially a series of pieces of data, or elements. It is a key basic piece of how data is stored in R. When R returns a vector as the output from a command, each element is numbered in square brackets. These square brackets can also be used to index the vector to get the nth element.\nFor atomic vectors created with c() or similar operations, there are some important rules:\n\nEach element must be scalar (i.e. of length 1)\nAll of the elements must have the same data type (or will be coerced)\n\nFor a complete explanation of vectors (and their more versatile siblings, lists) that’s beyond the scope of this tutorial, see:\n\nThis excellent explainer on vectors and lists\nR for Data Science chapter 20\n\n\n\n\n\nVector Calculations\n\n\n\n\n\n\nExercise\n\n\n\nCreate a vector of every whole number between 37 and 63, and subtract 7 from each element.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc(37:63) - 7\n\n [1] 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n[26] 55 56\n\n\n\n\n\n\n\nThis could be a very tedious process, but here we have an example of a vectorised operation. By default, the operation “subtract 7” is automatically applied to each individual element of the vector.\nWe can do a lot more than this with numbers and data in R, but this is an excellent start. Just one note before we move on about the order in which R performs its calculations.\n\n\n\n\n\n\nOrder of Operations\n\n\n\nMathematical expressions are evaluated in a certain order of priority. You can use brackets to tell R which part of a longer calculation to do first, e.g.:\n\n59 * (401 + 821)\n\n[1] 72098\n\n\nWithout the brackets, the expression is evaluated from left to right, which in this case would give a different answer:\n\n59 * 401 + 821\n\n[1] 24480\n\n\nWhenever there’s any chance for ambiguity, always use brackets to make sure the calculation is performed correctly.\n\n\n\n\n\nCharacter Data\nCharacters are a more general data category that also includes letters and words. In R, strings of letters or words must be enclosed in either ‘single’ or “double” quotes, otherwise R will try to read them as code:\n\n\nHello world!\n\nError: &lt;text&gt;:1:7: unexpected symbol\n1: Hello world\n          ^\n\n\n\n\"Hello world!\"\n\n[1] \"Hello world!\"\n\n\nAs you can see here, the first command without quotes throws an error, whereas the second prints out our input just like it did with the single numbers before.\nAn important thing to note is that R sees everything inside a pair of quotes as a single element, regardless of how long it is. You can see this in the indices we saw before:\n\n\n\"Hi!\"\n\n[1] \"Hi!\"\n\n\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\"\n\n[1] \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...\"\n\n\nThe [1] markers also tell us that each of the two strings above already constitute vectors, each of length 1. Just like we saw with numbers, above, any number of character strings can be combined into a vector. You can also use the numbered markers to extract the nth element in that vector.\n\n\n\n\n\n\nExercise\n\n\n\nCreate a vector containing the first five animals you think of, then print the 3rd one.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc(\"bumblebee\", \"squid\", \"falcon\", \"flea\", \"seagull\")[3]\n\n[1] \"falcon\"\n\n\n\n\n\n\n\nThe placement of the quotes is very important - they can’t include the commas. As we saw before, R uses commas to separate different elements. So, if you didn’t enclose each word in quotes separately with commas in between, you would have had this odd message:\n\nc(\"bumblebee, squid, falcon, flea, seagull\")[3]\n\n[1] NA\n\n\nNA is a special value in R. It indicates that something is not available, and it usually represents missing data, or that a calculation has gone wrong or can’t be performed properly.\nHere, we asked R for the third element in a vector that, as far as R can tell, only contained one. This is because there’s only one pair of quotes, so all five animals and the commas between them are considered to be one element. Since there isn’t a third element, R has informed us so accordingly - the answer to our query is NA, doesn’t exist.\n\n\nLogical Data\nThe final type of data that we’ll look at for now is logical data. In addition to performing calculations and printing out words, R can also tell you whether a particular statement is TRUE or FALSE. To do this, we can use logical operators to form an assertion, and then R will tell us the result.\n\n\n\n\n\n\nExercise\n\n\n\nWrite the following assertions in R:\n\n5 is greater than 10\n6 is less than 12\n27 is less than or equal to 27\n49 does not equal 93\n420 equals 42\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n5 &gt; 10\n\n[1] FALSE\n\n6 &lt; 12\n\n[1] TRUE\n\n27 &lt;= 27\n\n[1] TRUE\n\n49 != 93\n\n[1] TRUE\n\n420 == 42\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsserting Equivalence\n\n\n\nThe last few statements above may have caused you some trouble if the notation is unfamiliar.\nFor “less than or equal to”, R won’t recognise the \\(\\le\\) symbol. Instead, we combine two operators, “less than” &lt; and “equal to” =, in the same order we’d normally read them aloud. The same goes for “greater than or equal to”, &gt;=. (It does have to be this way round; try =&lt; and =&gt; to see what happens.)\nFor “does not equal”, ! is common notation in R for “not”, or the reverse of something. So != can be read as “not-equals”.\nFor “equals”, if you tried this with a single equals sign, you would have had a strange error:\n\n420 = 42\n\nError in 420 = 42: invalid (do_set) left-hand side to assignment\n\n\nThe problem is that the single equals sign =, like the comma, has some very specialised syntactic uses, including one equivalent to the assignment operator &lt;-, which we’ll look at shortly. Single equals = also has an important and specific role to play in function arguments. Essentially, = is a special operator that doesn’t assert equivalence. Instead, “exactly equals” in R is “double-equals” (or “exactly and only”), ==.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse a single command to ask R whether the numbers 2 through 10 are less than or equal to 6.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n2:10 &lt;= 6\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n\nHere R prints out a value of TRUE or FALSE for each comparison it’s asked to make. So, the first element in the output (TRUE) corresponds to the statement 2 &lt;= 6, the second to 3 &lt;= 6, and so on. This is a vectorised calculation again, as we saw with numeric data before. These vectorised assertions will be absolutely essential to selecting and filtering data that meet particular requirements, or checking our data to find problems.\n\n\n\n\n\n\nSPSS Data Types\n\n\n\n\n\nIf you’re a regular SPSS user, you might recognise many of these data types from SPSS. See page 7 of this SPSS user guide for a reminder of these types.\nSo far, what we’ve called “numeric” in R is also (broadly) “scale/numeric” in SPSS. What we’ve called “character” in R is “string” in SPSS. As far as I know, SPSS doesn’t have an equivalent of “logical”, but would probably be a “nominal/string” type.\nWe haven’t thus far talked about a few common data types that you might be used to using in SPSS. Ordinal and Nominal data types in SPSS correspond (more or less) to factors in R, which we will come to in a later tutorial. R also has date-time data, which is not covered in this series, but if you need to use it, check out the {lubridate} package."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#class-and-coercion",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#class-and-coercion",
    "title": "01/02: IntRoduction",
    "section": "Class and Coercion",
    "text": "Class and Coercion\nWith these short examples, it may be obvious just by looking that 25 is a number and porcupine is a word. However, this isn’t always so straightforward, and there are some situations - such as data checking/cleaning, or debugging - where we might want to check what type of data a certain thing is. To do this, we’ll need another new function, class(). This function will print out, as a character, the name of the data type of whatever is put into the brackets.\n\n\n\n\n\n\nExercise\n\n\n\nUse the class() function to get R to print the values \"numeric\", \"logical\", and \"character\".\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Any numeric vector will do\nclass(216907)\n\n[1] \"numeric\"\n\n## You can also use a longer vector of numbers\n## as long as they are all numbers!\nclass(c(4:291, -1, 38.7, 100000000))\n\n[1] \"numeric\"\n\n## Logical has two options\n## Create a vector of TRUEs and FALSEs\nclass(TRUE)\n\n[1] \"logical\"\n\n## Create a vector that outputs logical values\n## (now you're thinking with functions!)\nclass(c(6 &gt; 4, 10 == 37, 3 != 8))\n\n[1] \"logical\"\n\n## Character\nclass(\"antidisestablishmentarianism\")\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat data type does R give you if you combine numbers and characters in c()?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Again, anything will do\n\nclass(c(93, -1905, \"avocado\"))\n\n[1] \"character\"\n\n\n\n\n\n\n\nSomething interesting has happened here. Recall that atomic vectors created with c() must all have the same data type. Here, we combined two types of data: numeric and character. We didn’t get an error - instead, without warning or telling us, R quietly converted the entire vector to character type. This forcible conversion is called coersion.\n\n\n\n\n\n\nDefinition: Coersion\n\n\n\nCoersion is when a piece of data is forcibly changed from one data type to another. This is sometimes intentional, but it can happen unintentionally (and without any warning or fanfare!), so is a common source of errors.\nCoersion follows a hierarchy; data types on the left can be coerced into the types further along to the right.\nlogical ==&gt; integer ==&gt; double (numeric) ==&gt; character\nAs we saw previously, you can check the data type of a vector with class(). You can also check if a vector is a particular type (and receive a logical vector in response) with the is.*() family of functions. (The * notation refers to a placeholder for many different options, such as is.numeric, is.character, etc.)\nYou can similarly (try to) coerce a vector into a particular data type with the as.*() family of functions.\n\n\nThis explains why our vector from the last exercise was a character vector - since the vector contained at least one character element, everything else in the vector was coerced to the same type. This can cause problems when, for example, numeric data is coerced into character data, even though it still looks like numbers.\nEven though we can do mathematical operations on numbers, we can’t do them on characters; it should be clear that asking e.g. what is \"tomato\" - 7 is nonsense. However, this is the case even if all of the data are numerals! For example:\n\n## No problem here; all numbers\nc(2:20, 45) - 7\n\n [1] -5 -4 -3 -2 -1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 38\n\n## Doesn't work\nc(2:20, \"45\") - 7\n\nError in c(2:20, \"45\") - 7: non-numeric argument to binary operator\n\n\nEven though “45” looks like a number, because it’s in quotes, R thinks that it’s a character, and will refuse to do the calculation, in the same way that it would refuse to do it with “tomato”.\n\n\n\n\n\n\nExercise\n\n\n\nUse an as.*() function to convert the following vector of participant ages into numeric data: c(20, \"42\", \"36 years old\"). What do you think will happen to each element?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nas.numeric(c(20, \"42\", \"36 years old\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 20 42 NA\n\n\n\n\n\n\n\nHere for the first time we see an example of a warning. Warnings are not errors, even though they get printed out in the same (usually alarming) colour and the same (often unfriendly) curt tone. The key difference is whether the code runs or not. With errors, the code cannot be executed as written, and the error is returned at the point where the execution failed. With warnings, the code CAN be (and has been) executed as written, but R is telling you that it has done something that you might not have expected or wanted along the way.\nWhat we have asked in this command is for three pieces of data to be coerced to numeric type. The first, 20, is already numeric type, so presents no issue. The second, \"42\", is character type (because of the quotes), but is also parsable as a number so similarly presents no issue. The problem is \"36 years old\", which cannot be turned into a number3. Instead of throwing an error, though, R instead replaces \"36 years old\" in the output vector with NA, and prints a warning to let you know it’s done this. If this is what you wanted (and sometimes it might be), you can ignore it, but if you thought that all these ages were already numbers, this warning would be an important flag to investigate your data a bit more thoroughly."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#objects",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#objects",
    "title": "01/02: IntRoduction",
    "section": "Objects",
    "text": "Objects\nR is a programming language, but (being created by speakers of natural language) it has many features similar or analogous to natural languages. In this section, we’ll cover the basic “grammar” of R, including how R understands what you ask it to do.\nIn a similar way that the basic unit of many languages is the word4, the basic unit of the R programming language is the object. This section will explore the basics of what an object is and some of their key features in R.\n\n\n\n\n\n\nDefinition: Objects\n\n\n\nObjects are the basic elements that R is built around - the equivalent of words. An “object” in R is any bit of information that is stored with a particular name. Objects can hold anything, from a single number or word to huge datasets with thousands of data points or complex graphs. These named objects are the main way you, the programmer, can store, retrieve, and interact with information in R.\n\n\n\nCreating an Object\nAlthough we have done quite a bit in R so far - creating vectors, doing calculations, etc. - you may notice that we haven’t stored this information anywhere. To store the output of code for further use, it needs to be assigned to an object using the assignment operator, &lt;-. Once an object is created, it will appear in the Environment pane.\n\n\n\n\n\n\nClear Your Environment\n\n\n\nAt the moment your Environment should be empty. As a reminder, Environment is by default the first (leftmost) tab in one of your four main windows in RStudio, probably the one on the top right.\nIf this window is blank except for “Environment is empty”, you’re ready to go. If for some reason it isn’t empty, click the broom icon to clear everything from your Environment before you get started, as indicted in the image below. (There will be a very ominous-sounding “Are you sure?” pop-up, but just click “Yes”.)\n\n\n\nFirst, let’s look at the foundational structure of almost everything you will do in R:\nobject &lt;- instructions\nThis is “pseudo-code”, or a “general format” for a command in R. It isn’t valid R code, but is rather intended as a midpoint between natural language and R to help make it clear how the code works. You can read this code as, “An object is created from (&lt;-) some instructions.”\n\nobject: Objects can be named almost anything (although see Naming Objects, below). The object name is a label so you, the analyst, can find, refer to, and use the information you need.\n&lt;-: The assignment operator &lt;- has single job: to assign output to names, or in other words, to create objects.\ninstructions: Any amount of code that produces some output, which is what object will contain.\n\n\nNaming Objects\nGenerally, you can name objects pretty freely in R. Object names must be a single sequence of symbols, so can’t include spaces or special operators (like =, &lt;-, ,, etc.). The best idea is to come up with naming conventions that work for you, so you can easily remember what objects contain. R will let you know if you try to name an object something that it doesn’t like:\n\n## This doesn't work because it would frankly be bonkers if it did\n1285 &lt;- \"a\"\n\nError in 1285 &lt;- \"a\": invalid (do_set) left-hand side to assignment\n\n\n\n## Can't start with a number...\n1stletter &lt;- \"a\"\n\nError: &lt;text&gt;:2:2: unexpected symbol\n1: ## Can't start with a number...\n2: 1stletter\n    ^\n\n\n\n## But numbers inside are okay\nletter1 &lt;- \"a\"\nletter1\n\n[1] \"a\"\n\n\n\n## You can really go wild if you want!\nthisis_TheFirstLETTER.of.the.alphabetWOW &lt;- \"a\"\nthisis_TheFirstLETTER.of.the.alphabetWOW\n\n[1] \"a\"\n\n\nIn these tutorials, we will typically stick to so-called “snake case” - lowercase names with underscores. This is generally the style of {tidyverse} as well. However, there’s nothing to stop you from using different conventions such as camelCase, PascalCase, whatever.this.is, or mixing them all at random, except maybe for the fact that your future self, and anyone else who might want to read or use your code, will almost certainly despair.\n\n\n\n\n\n\nNongrammatical Names\n\n\n\n\n\nIt is actually possible to use unallowable symbols, like spaces and punctuation, in some names, by using backticks. You must use the backticks when you create AND every time you use/call the object. It is generally a very bad idea to do this (just use underscores like a reasonable person), but it occasionally comes in handy for formatting tables or figures when the names don’t need to be machine-readable/good R code/easy to work with anymore.\n\nnope this is bad &lt;- \"a\"\n\nError: &lt;text&gt;:1:6: unexpected symbol\n1: nope this\n         ^\n\n\n\n`but this one works!` &lt;- \"a\"\n`but this one works!`\n\n[1] \"a\"\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThink of a research scenario familiar to you with two independent groups. You’re welcome to draw from your own research or expertise, but you should choose something with numerical scores. Some ideas include:\n\nReaction times on a button-pressing task from a control and an experimental group\nStatistics anxiety scores from first and second year UG students\nQuiz marks from students with practicals scheduled 9am and students with practicals at 6pm\n\nMake a note of the scenario you chose. Then, create two new objects: one that contains a vector of six scores from the first of the two groups, and the second that has six different scores from the second group.\nHint: Just make up some numbers that sound plausible!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nChoosing scenario 3, this vector contains some hypothetical quiz marks from each class.\n\nquiz_9am &lt;- c(75, 58, 62, 16, 33, 67)\nquiz_6pm &lt;- c(45, 90, 27, 65, 39, 77)\n\nLet’s have a look at these two commands. On the left side I’ve written the name I want my new object to have, which I’ve called quiz_9am5. Next, the assignment operator &lt;- assigns whatever comes after it to the object label quiz_9am. Finally, I’ve written instructions for what I want this object to contain: in this case, a vector of numbers that I’ve made up, but that reasonably look like quiz scores. The second command is the same as the first, but with a different object name and different numbers.\nIf you haven’t done this yet, do so now, even if you’ve looked at the solution rather than trying it for yourself first. Once you’ve typed the command, there’s a final step to actually create the object: you have to run the command in order for it to take effect. As a reminder, you can do this by clicking the green ▶️ button in the upper right corner of the code chunk, or by pressing Ctrl/Cmd + Enter when your cursor is blinking on the same line as the code you want to run.\n\n\n\n\n\nAssuming your code is valid, you should see a green bar appear along the left-hand side of the code chunk when you run the code, but you might notice that there’s no printout that appears under the code chunk, as there was previously. In fact, if the code ran successfully, it might look like nothing happened at all. To find out what did happen, look your Environment pane. You should now see a new section, “Values”, and underneath the name of your new objects and what they contain. Success!\n\n\n\nCalling an Object\nFor any object, from the most simple to the most complex, you can always see what’s in it by calling the object. This simply means that you type the name of the object and run the code. R will print out whatever is stored in the object.\n\n\n\n\n\n\nExercise\n\n\n\nCall both of the objects you just created.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nReplace with the name of the object you created, if you did something different.\n\nquiz_9am\n\n[1] 75 58 62 16 33 67\n\nquiz_6pm\n\n[1] 45 90 27 65 39 77\n\n\n\n\n\n\n\nThis output looks just like what we saw earlier, when we just asked R to print out a vector of numbers. In essence, the object names are just labels for storing and referring to the information they contain.\n\n\n\n\n\n\nCreating vs Calling\n\n\n\nThese two actions are the essential basis of everything you will do in R. All of your code will, at base, either create an object, or call an object. (Changing an existing object, as we’ll see shortly, is the exact same procedure as creating one from scratch.)\nWhen you create an object using the assignment operator (&lt;-), the object is created but is not printed out. This is because R always does only and exactly what you ask it to do, and using the assignment operator only tells R to assign something to an object, not to print it out6.\nWhen you call an object, the current contents of that object are printed out, but that object is not changed - you only reproduce a copy of its contents for review. To create or change an object, you must use the assignment operator to assign the output to a new (or existing) object name.\n\n\nLet’s make all of this a bit more concrete by seeing how we can use objects effectively.\n\n\nUsing Objects\nSince objects are convenient reference labels for the information they contain, we can work with them as if they were the information they contain. In this case, our objects contain numbers, so we can use them for numerical calculations.\nFor instance, we might want to know what the mean mark was for this sample of quiz marks. To do this, we could make use of a very handy function, mean(), as follows:\n\nmean(quiz_9am)\n\n[1] 51.83333\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the mean of each of the two sets of scores you created.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhether you save the output of the mean() command is up to you!\n\nmean(quiz_9am)\n\n[1] 51.83333\n\nmean(quiz_6pm)\n\n[1] 57.16667\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the difference in the mean of each of the two sets of scores, and save this difference in a new object called quiz_diff.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere are two options for accomplishing this.\nThe first option is to save each mean value in a new intermediate object (if you didn’t do that already), then subtract one mean from the other. This is very easy to read, but a bit inefficient.\n\nmean_9am &lt;- mean(quiz_9am)\nmean_6pm &lt;- mean(quiz_6pm)\n\nquiz_diff &lt;- mean_9am - mean_6pm\n\nThe second option is to do everything in one command, which takes a bit more effort to parse but is more succinct.\n\nquiz_diff &lt;- mean(quiz_9am) - mean(quiz_6pm)\n\nEither way, you will get the same result:\n\nquiz_diff\n\n[1] -5.333333\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the class of these objects?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEither one will do.\n\nclass(quiz_9am)\n\n[1] \"numeric\"\n\n\nSo, an object has the class of the data it contains.\n\n\n\n\n\nYou may have been surprised to see that the class of these objects is numeric, rather than character - even though the name of the object is a character string. To find out the class of the object, R looks at what that object contains, not at the name of the object itself. We already saw that quiz_9am (or whatever your object is called) contains only numbers; so, R tells us that it’s a numeric vector.\nOne more example to emphasize this point, because it’s often a source of confusion when starting out with R. If we want to ask R the class of the string “quiz_9am”, we would need to put it in quotes, and we’d get a different answer:\n\nclass(\"quiz_9am\")\n\n[1] \"character\"\n\n\nThe key thing here is that objects have the class of the data they contain, and are not character data; and whenever you want to use an object, you must not use quotation marks. On the other hand, if you want to input character data into R, you must use quotation marks. Otherwise, R will look for an object or function with that name, which will likely produce a “cannot find object” error.\n\n\nOverwriting Objects\nOur last major point to cover with objects - for now - is how to change what an object contains.\n\n\n\n\n\n\nExercise\n\n\n\nFirst, let’s imagine we get three new participants in each condition of our previous study. Update the same two objects you created previously with three new scores each.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne way - the longer way round - is to type in all the same numbers again from before, and then include three more.\n\n## Example with just one\nquiz_9am &lt;- c(75, 58, 62, 16, 33, 67, 89, 100, 79)\n\nHowever, this is tedious, repetitive, and prone to error. Wherever possible, it’s better to rely on R to do calculations or repetitions for you. So, we could instead just embed the previous quiz_9am object - which already contains the first six numbers - into a c() along with the three new numbers.\n\nquiz_9am &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am\n\n[1]  75  58  62  16  33  67  89 100  79\n\nquiz_6pm &lt;- c(quiz_6pm, 38, 42, 53)\nquiz_6pm\n\n[1] 45 90 27 65 39 77 38 42 53\n\n\n\n\n\n\n\n \nThe command we’ve just written for the task above demonstrates some extremely important properties of how assignment and functions work in R.\nOverwriting objects is accomplished by assigning new output to an existing object name. If you have a look in your Environment, you will see that the previous version of quiz_9am, containing only six values, has been replaced with the new one containing nine values.\nOverwriting objects is silent. Unlike, say, a word processor, that will give you a warning if you try to save two documents in the same folder with the same name, R won’t ask you if you’re sure you want to overwrite an existing object with new information - it will just do it. This can be a good thing, because you can easily update the information stored in an object with changes, edits, or new information. However, it also means that you can overwrite or replace data when you don’t want to, if you use the same object name.\nThis is why it is so important to keep track of all of the commands and changes you make to your data. If you accidentally replace your dataset with, say, a single word, or number with an error in your code, you can easily retrace your steps and avoid redoing work.\nOverwriting objects can be done recursively. In the command we saw above, we took the current quiz_9am object, combined it with some new values, and then overwrote the quiz_9am object with the new values. If we were to run this exact same code again, this means that each time we would add three new values to quiz_9am, over and over and over:\n\nquiz_9am &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am\n\n [1]  75  58  62  16  33  67  89 100  79  89 100  79\n\nquiz_9am &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am\n\n [1]  75  58  62  16  33  67  89 100  79  89 100  79  89 100  79\n\n\nThis is one reason why the decision to overwrite an existing object, vs creating a new one, can make a big difference to your code. This behaviour only happens because the input and output objects are the same. If we name the output object something different, we don’t get the same recursion - the new object quiz_9am_full is recreated from the same input in the same way every time, so it always contains the same thing.\n\n## Recreating the original version of quiz_9am so things don't get out of hand!\nquiz_9am &lt;- c(75, 58, 62, 16, 33, 67)\n\nquiz_9am_full &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am_full\n\n[1]  75  58  62  16  33  67  89 100  79\n\nquiz_9am_full &lt;- c(quiz_9am, 89, 100, 79)\nquiz_9am_full\n\n[1]  75  58  62  16  33  67  89 100  79\n\n\nThere isn’t a right or wrong way to do this - sometimes this recursive property is exactly what you want. (It’s very useful, for example, in loops.) But it is important to be aware of.\nFinally, overwriting objects only changes the overwritten objects, and NOT any other objects created from them. To see this in action, recall that earlier we calculated the mean of the two quiz objects and saved it as quiz_diff. If we do the same calculation now with our updated objects, we can see that the difference in the means is no longer the same.\n\n## Value calculated previously\nquiz_diff\n\n[1] -5.333333\n\n## Value using the updated objects\nmean(quiz_9am) - mean(quiz_6pm)\n\n[1] 11.44444\n\n## Ask R if the two are the same\nquiz_diff == (mean(quiz_9am) - mean(quiz_6pm))\n\n[1] FALSE\n\n\nThis illustrates the importance of writing and running code sequentially, from beginning to end. If you go back and change values created earlier on in your code, the value you currently have in your Environment may not match the value that your code will produce when run.\nIf you are interested in understanding this process of assigning and replacing the contents of objects better, the aside below explains it in more depth.\n\n\n\n\n\n\nCan you actually change an object?\n\n\n\n\n\nThe majority of this aside was originally written by Milan Valášek\nThink of objects as boxes. The names of the objects are only labels, and you can store anything you like inside them. However, unlike in the physical world, objects in R cannot truly change. You can put stuff in and take stuff out, and that’s pretty much it. Unlike boxes, though, when you take stuff out of objects, you only take out a copy of its contents. The original contents of the box remain intact. Of course, you can do whatever you want (within limits) to the stuff once you’ve taken it out of the box, but you are only modifying the copy. The key thing to remember is that unless you put that modified stuff into a box, R will forget about it as soon as it’s done with it. In other words, if you want to “save” any changes you make, you must assign them to an object in order to keep them.\nNow, as you probably know, you can call your boxes (objects) whatever you want (again, within certain limits). This means that that you can call the new box the same as the old one, as we saw with quiz_9am above. When that happens, R basically takes the label off the old box, pastes it on the new one, and burns the old box. So even though some operations in R may look like they change objects, what’s actually happening is that R copies their content, modifies it, stores the result in a different object, puts the same label on it, and discards the original object. Understanding this mechanism will make things much easier!\nPutting the above into practice, this is how you “change” an R object:\n\n# put 1 into an object (box) called a\na &lt;- 1\n\n# copy the content of a, add 1 to it and store it in an object b\nb &lt;- a + 1\n\n# copy what's inside b and put it in a new object called a\n# discarding (\"overwriting\") the old object a\na &lt;- b\n\n# now see what's inside of a\n# (by copying its content and pasting it in the console)\na\n\n[1] 2\n\n\nOf course, you can just cut out the middleman (creating an object b). So to increment a by another 1, we can do:\n\na &lt;- a + 1\n\na\n\n[1] 3"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#functions",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#functions",
    "title": "01/02: IntRoduction",
    "section": "Functions",
    "text": "Functions\nFunctions are like verbs in the R language. We’ve already started using a few functions: c(), class(), mean(), and as.numeric(). As we’ve seen, these functions perform some operation using the input inside their brackets, and produce the output of that operation. So, functions are the main way that R does anything.\nIn this section, we’ll take a systematic look at the process of using a new function - in particular, functions that take multiple inputs, or arguments. As we go, we’ll look at how to “translate” the command you want to give R into a verb (function) it can understand.\n\nBasics and Help\nLet’s look at an example of how this translation might work. For this example, I’m going to use a number I generated earlier: the mean of the quiz_9am group, 64.3333333, which I’d like to round to two decimal places - a common task for reporting results in APA style.\nIf we want R to do this for us, we have to write this command in a way that R can understand. First, we need to know what function corresponds to the English verb “round” - that is, what function will do the same action that we want R to perform. We’re lucky in this case: the function in R is also called round().\nWe know that we’re looking at a function in R because functions often have a name followed by brackets (and nothing else in R does). That is, they have the general form function_name(). Inside the brackets, we can add more information to the function to complete our command, although not all functions require any more information.\n\n\n\n\n\n\nExercise\n\n\n\nTry running the round() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nround()\n\nError in eval(expr, envir, enclos): 0 arguments passed to 'round' which requires 1 or 2 arguments\n\n\nUnsurprisingly, R has given us an error. This is an informative error, though - that is, the error gives of some sort of intelligible clue about what’s gone wrong. Namely, it tells us that round() can’t just work without additional information (i.e. “required arguments”).\n\n\n\n\n\nWhat we want to do, “Round the number 64.3333333 to two decimal places”, has two more important pieces of information that we need to tell R: what number we want to round (64.3333333) and how many decimal places we want to round it to (2). So, how do we say this in R? To find out, let’s look at the help documentation.\n\n\n\n\n\n\nExercise\n\n\n\nOpen the help documentation for the round() function by running ?round() or help(round) in the Console.\n\n\n\n\n\n\n\n\nHelp Documentation\n\n\n\nHelp documentation is information, like instruction manuals, built into R about how individual functions work. Function documentation varies wildly in helpfulness and completeness, but it’s a useful place to check first if you want to find out what a function does. You can access the help documentation in a few different ways: by running ?function_name or help(function_name) in the Console, or by clicking on the “Help” tab in the Files section of RStudio and using the Find box to search for the function.\n\n\nThe first section, “Description”, varies quite a bit in intelligibility, depending on how complex the function is. Here, if we ignore the information about the other function included in this document, we can see that we have a useful description of round() that tells us that it rounds numbers (that’s a good sign) to a certain number of decimal places. That’s exactly what we want, so how do we use it?\nLet’s scroll down to “Usage”, which gives examples of what the function looks like. You can see that the basic structure of this function is round(x, digits = 0). It seems like we need to add some more information in the brackets of our function - but how do we interpret x and digits = 0?\n\n\nArguments\nThe information inside a function’s brackets, which give it the information it needs to work, are called arguments. Each argument in a function is separated by a comma, so we can see from round(x, digits = 0) that the round() function can take two arguments. How many arguments a function has depends on the function; some (like Sys.Date()) don’t need any arguments to run. One of the most useful parts of a function’s help documentation is the “Arguments” section, which tells you what each of the function’s arguments are and how to use them.\nThere are two main types of arguments: named and unnamed arguments. Conveniently, the arguments of round() give us one example of each.\n\nUnnamed Arguments\nThe first argument to round() is simply x. Just like in maths, x is a placeholder for some number or numbers (a “numeric vector”, which should sound familiar now) that the function will work on. This is common notation in many functions: x, often the first argument in a function, typically denotes the placeholder for the information you want to use the function on. In our case, we just have one number we want to round, so that’s what we should replace with x.\nThis argument has no default so it must be provided, or the function won’t run.\n\n\nNamed Arguments\nThe second argument of round() is a named argument, digits = 0. You can think of named arguments like settings that change the way a function works, often with only certain allowable values. Here we can see that the name of the argument is digits; the name before the = sign tells R which setting we want to change.\nThe help documentation tells us that digits should be an “integer indicating the number of decimal places…to be used.” We can also see in “Usage” that this argument has a default value, digits = 0. That means that if we don’t explicitly include the argument digits when we use the function, by default the round() function will round the number you give it to 0 decimal places. Named arguments frequently, but not always, have a default, and it’s important to check so the function doesn’t quietly do something unexpected.\nDefault values of arguments are really useful, because the default is often the most frequently used or safest7 setting. It means you don’t have to specify every single aspect of a function every time you use it, as long as you want the function to work according to its defaults. In our case, we actually wanted round() to round to two decimal places, not 0. So, in our command, we should change the digits setting from the default, 0, to 2.\n\n\n\nUsing Functions\nNow that we know what both of these arguments mean, we can change them to actually translate the sentence “Round the number 64.3333333 to two decimal places” into a command that R can work with. We’ll explicitly write out each argument so we know what they are doing.\n\n\n\n\n\n\nExercise\n\n\n\nUse the round() function to round 64.3333333 to two decimal places.\nIf you prefer, you can do this with one of the means you calculated for your own scores earlier.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Using the actual value from my earlier calculation\nround(64.3333333, 2)\n\n[1] 64.33\n\n## Using a nested function - that is, calculating the mean and then rounding it!\nround(mean(quiz_9am), 2)\n\n[1] 64.33\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrder of Arguments\n\n\n\n\n\nIf you want to, you can achieve the same result by changing the order of the arguments. Because we have written the names of both arguments, R can still do what we want it to do:\n\nround(digits = 2, x = 64.3333333)\n\n[1] 64.33\n\n\nWe can also, to some degree, drop the names of the arguments, as long as R can still understand what we’re trying to do:\n\nround(digits = 2, 64.3333333)\n\n[1] 64.33\n\n\nHere I left out the x =. R can still understand this because round() only takes two arguments, and we explicitly told it what value belongs to digits, so it assumes the second number must be x.\n\nround(64.3333333, 2)\n\n[1] 64.33\n\n\nThis time I dropped both argument names. R can still understand this because when you don’t specify which input goes with which argument, R will assume they should go in the default order given in the help documentation. So, R has automatically assigned 64.3333333 to x and 2 to digits, which is what we wanted.\nAs I use R more and more, I find that I name arguments more consistently, even though I know how the function works and dropping them is more efficient (at least in terms of typing). That’s because when I come back from lunch, or the next day, or six months later to revisit the same code, it’s much easier to recall what it all means when it’s well-annotated. So, I strongly recommend getting in the habit of including argument names in your code as a favour to your future self, and to avoid situations like this:\n\nround(2, 64.3333333)\n\n[1] 2\n\n\nHere, since we didn’t specify, R assumed that 2 was the number we wanted to round. This isn’t what we wanted - but R has no way of knowing this. It always assumes that what we typed was precisely what we intended to ask R to do.\n\n\n\n\nPassing Multiple Values to Arguments\nA last important aspect of using functions is that each argument in a function can only take a single object or value as input. For example, we saw above that we put the single value 64.3333333 into the x argument of round(). But what if we wanted to round more than one number? We don’t want to have to write a new round() command for every number, even though we could do this if we particularly enjoyed doing a lot of tedious and repetitive typing:\n\nround(64.3333333, 2)\n\n[1] 64.33\n\n## ughhhh\nround(59.5452, 2)\n\n[1] 59.55\n\n## noooooo :(\nround(0.198, 2)\n\n[1] 0.2\n\n## thanks I hate it\n\n\n\n\n\n\n\nExercise\n\n\n\nBefore you go on, have a go using a single round() command to round 64.3333333, 59.5452, and 0.198 at once.\nHint: Refer to Vectors.\n\n\nSo what happens if we try to put all of those numbers into round()? We might first try this:\n\nround(64.3333333, 59.5452, 0.198, 2)\n\nError in eval(expr, envir, enclos): 4 arguments passed to 'round' which requires 1 or 2 arguments\n\n\nOnce again, R tells us that this doesn’t work by throwing an error. R has tried to do what we wanted, but the round() function only allows a max of two arguments, and we’ve given it four. Behind the scenes, R has tried to run round(x = 64.3333333, digits = 59.5452... and can’t proceed from there because it doesn’t know what to do with the last two numbers. So, what we need to do is find a way to put all three numbers that we want to round into the first x argument together. If only there was a way to concatenate them together…\nYou may have guessed where this is going: one method we could use would be to put the three numbers we want to round into a single object, and then pass that object to round() as the x argument. We already saw that we can combine any number of things together into a single vector using the c() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Create an intermediate object to contain the numbers\nnumbers &lt;- c(64.3333333, 59.5452, 0.198)\nround(numbers, digits = 2)\n\n[1] 64.33 59.55  0.20\n\n## Put the vector of numbers into round() directly\nround(c(64.3333333, 59.5452, 0.198), digits = 2)\n\n[1] 64.33 59.55  0.20\n\n\n\n\n\nHere we can see a good example of a function inside another function. You can stack, or “nest”, functions inside each other like this as much as you like, although it can become difficult to read the code or keep track of what it’s doing. (There’s a great solution to this problem that we’ll encounter in the next tutorial: the pipe operator.)\nThat’s looking like some proper R code! Very nicely done.\n\n\n\n\n\n\nHelp Documentation, Revisited\n\n\n\nBefore we leave the round() function altogether, let’s take a look at two more useful sections of the help documentation. Depending on what you are trying to do, the “Details” section can tell you more about how exactly the function works - how it behaves in certain situations, or how it handles unusual or difficult cases. If a function isn’t doing what you expect it to, this is a good place to look for an explanation.\nFinally, at the end of the documentation you can find the “Examples” section. If you are learning to use a new function, this section can give you a template for writing your own commands. You can also click the “Run examples” link, which will run the code in the Examples section for you so you can see what the function will do."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#quick-test-t-test",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#quick-test-t-test",
    "title": "01/02: IntRoduction",
    "section": "Quick Test: t-test",
    "text": "Quick Test: t-test\nLet’s put all of this together and have a look at what we can already do with the skills in this tutorial. R has many, many uses, but one of its core purposes is statistical analysis - and we already know more than enough to do this.\n\n\n\n\n\n\nIf we run out of time in the live session, you can attempt this last bit as an optional Challenge task. Don’t worry if you get stuck - we will come back to t-tests later on in the Essentials section of the course.\nIn case you’re not familiar with t-tests, you can find a lecture recording on the Analysing Data 22/23 Canvas site.\n\n\n\nWe’ve created two objects that contain scores from two different groups - scores we made up, but we will get to real data soon (in the next tutorial!). For now, one common statistical test we could run on data like this is a independent-samples t-test, which is a hypothesis test essentially evaluating the probability that two sets of scores come from the same population.\nHelpfully, the function we want is called t.test().\n\n\n\n\n\n\nExercise\n\n\n\nBring up the help documentation for t.test() and use it to run a t-test comparing your two sets of scores.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCall up the help documentation in the Console:\n\n?t.test\nhelp(t.test)\n\nRun the test:\n\nt.test(quiz_9am, quiz_6pm)\n\n\n    Welch Two Sample t-test\n\ndata:  quiz_9am and quiz_6pm\nt = 1.0264, df = 15.081, p-value = 0.3209\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12.30962  35.19851\nsample estimates:\nmean of x mean of y \n 64.33333  52.88889 \n\n\n\n\n\n\n\nThere are a lot of options in the t.test() function, which can be used, through different arguments, to run almost any variety of t-test you can think of. In this case, though, the code is quite simple, because we want all the default settings (for a two-sample, independent test), so we only need to provide x and y, our two numeric vectors.\nNote that the output mentions “Welch Two Sample t-test”, which is a version of the test that does not assume equal variances. This is the version that is taught to undergraduates, because we have not at this point introduced the process of assumption testing. If you definitely know that the variances are equal and you definitely want Student’s t-test, you can instead change the default setting.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the help documentation, re-run the t-test with equal variances assumed.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nt.test(quiz_9am, quiz_6pm, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  quiz_9am and quiz_6pm\nt = 1.0264, df = 16, p-value = 0.32\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12.19206  35.08095\nsample estimates:\nmean of x mean of y \n 64.33333  52.88889 \n\n\n\n\n\n\n\nIn future tutorial, we’ll see how to turn this rather ugly R output automatically into beautifully formatted reporting like this:\n\nWe compared mean scores between two groups, one who took the quiz in a 9am practical session (M = 64.33) and the other who took the quiz in a 6pm practical session (M =52.89, Mdiff = 11.44). There was no statistically significant difference in scores between practical groups (t(16) = 1.03, p = 0.32, 95% CI [-12.19, 35.08]).\n\n \n \n\n\n\n\n\n\nWell Done!\n\n\n\nThat’s the end of the first tutorial. Very well done on all your hard work!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#footnotes",
    "href": "tutorials/psychrlogy/01_fundRmentals/01_02_intro.html#footnotes",
    "title": "01/02: IntRoduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis isn’t technically true - have a look at the “History” tab in the Environment window. However, the commands stored here can’t be run or used - they have to be copied into the Console or Source windows in order to run. The History tab provides an exhaustive record of the things you’ve typed, not a cohesive or meaningful series of steps.↩︎\nDon’t get me wrong - crashes do happen! But they often look like a “fatal error” popup message, the programme freezing, or other obvious breakdowns of the programme itself. “Errors” in code as we’re seeing here are just a part of the normal functioning of R and don’t usually mean anything particularly horrific is occurring.↩︎\nWell, not in the single command we’re using here. We can certainly get out the age 36, but it will take a bit more work. We’ll come back to this problem in the Essentials section of the course.↩︎\nAs a linguist I have to note, one, words don’t exist, and two, the closest linguistic term for what an object is is probably “lexeme”. “Word” will get you in the right vicinity, though, conceptually. If you’d like to dive down this rabbit hole (rabbit-hole?) this Crash Course video on morphology is a good place to start.↩︎\nAgain, I could have called this object anything, like the_first_example_of_an_object_InThisSection.so.far or made_upQuizScores.fornineamclass or anything else that follows R’s naming conventions. However, it’s a good idea to name your objects something brief and obvious, so you can remember what they contain and work with them easily.↩︎\nThere is a way to do this - you can enclose the entire expression in round brackets, e.g. (object &lt;- instructions), which will BOTH create the object AND print out what that object contains at the same time. I’m not using this method in these tutorials because I think it will be confusing, since it’s primarily for demonstration purposes and not necessary the sort of thing you’d want to use in your own analysis code.↩︎\nBy “safest” setting, I mean that the function makes the fewest assumptions about what you intended.↩︎"
  },
  {
    "objectID": "quick_ref.html",
    "href": "quick_ref.html",
    "title": "Quick Reference",
    "section": "",
    "text": "Looking for a function you can’t quite remember how to use? You’re in the right place! The table below is arranged alphabetically by function name, and the linked full name (including relevant package calls) will take you to the help documentation.\n\n\n\n\n\nFunction Name\n\n\nLink to Help Documentation\n\n\nUsed In…\n\n\n\n\n\n\nall()\n\n\nall()\n\n\n05: Filter and Select\n\n\n\n\nanova()\n\n\nanova()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nany()\n\n\nany()\n\n\n05: Filter and Select\n\n\n\n\napa_print()\n\n\npapaja::apa_print()\n\n\n04: Reporting Linear Models with Quarto05: Filter and Select\n\n\n\n\napa_table()\n\n\npapaja::apa_table()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nas.numeric()\n\n\nas.numeric()\n\n\n01/02: IntRoduction\n\n\n\n\nautoplot()\n\n\nggplot2::autoplot()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nboxplot()\n\n\nboxplot()\n\n\n03: Datasets\n\n\n\n\nc()\n\n\nc()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select\n\n\n\n\nclass()\n\n\nclass()\n\n\n01/02: IntRoduction\n\n\n\n\ncontains()\n\n\ndplyr::contains()\n\n\n05: Filter and Select\n\n\n\n\ncor.test()\n\n\ncor.test()\n\n\n05: Filter and Select\n\n\n\n\ncorrelation()\n\n\ncorrelation::correlation()\n\n\n05: Filter and Select\n\n\n\n\ncount()\n\n\ndplyr::count()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\ndata()\n\n\ndata()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\ndescribe_distribution()\n\n\ndatawizard::describe_distribution()\n\n\n03: Datasets\n\n\n\n\nfilter()\n\n\ndplyr::filter()\n\n\n05: Filter and Select\n\n\n\n\nggpairs()\n\n\nGGally::ggpairs()\n\n\n05: Filter and Select\n\n\n\n\nggscatmat()\n\n\nGGally::ggscatmat()\n\n\n05: Filter and Select\n\n\n\n\nglance()\n\n\nbroom::glance()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nglimpse()\n\n\ndplyr::glimpse()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nhere()\n\n\nhere::here()\n\n\n03: Datasets\n\n\n\n\nhist()\n\n\nhist()\n\n\n03: Datasets\n\n\n\n\nis.factor()\n\n\nis.factor()\n\n\n05: Filter and Select\n\n\n\n\nis.na()\n\n\nis.na()\n\n\n05: Filter and Select\n\n\n\n\nkable_styling()\n\n\nkableExtra::kable_styling()\n\n\n03: Datasets\n\n\n\n\nlibrary()\n\n\nlibrary()\n\n\n03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select\n\n\n\n\nlm()\n\n\nlm()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nlmRob()\n\n\nrobust::lmRob()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nmean()\n\n\nmean()\n\n\n01/02: IntRoduction03: Datasets\n\n\n\n\nmedian()\n\n\nmedian()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nmodel_parameters()\n\n\nparameters::model_parameters()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nnames()\n\n\nnames()\n\n\n03: Datasets\n\n\n\n\nncol()\n\n\nncol()\n\n\n03: Datasets\n\n\n\n\nnice_table()\n\n\nrempsyc::nice_table()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nnrow()\n\n\nnrow()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nplot()\n\n\nplot()\n\n\n03: Datasets\n\n\n\n\nprint()\n\n\nprint()\n\n\n05: Filter and Select\n\n\n\n\npull()\n\n\ndplyr::pull()\n\n\n03: Datasets\n\n\n\n\nrange()\n\n\nrange()\n\n\n03: Datasets\n\n\n\n\nread_csv()\n\n\nreadr::read_csv()\n\n\n03: Datasets\n\n\n\n\nround()\n\n\nround()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto\n\n\n\n\nsd()\n\n\nsd()\n\n\n03: Datasets\n\n\n\n\nselect()\n\n\ndplyr::select()\n\n\n05: Filter and Select\n\n\n\n\nstarts_with()\n\n\ndplyr::starts_with()\n\n\n05: Filter and Select\n\n\n\n\nsum()\n\n\nsum()\n\n\n05: Filter and Select\n\n\n\n\nsummary()\n\n\nsummary()\n\n\n03: Datasets\n\n\n\n\nt.test()\n\n\nt.test()\n\n\n01/02: IntRoduction\n\n\n\n\ntidy()\n\n\nbroom::tidy()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nwhere()\n\n\ndplyr::where()\n\n\n05: Filter and Select"
  },
  {
    "objectID": "quick_ref.html#index-of-functions",
    "href": "quick_ref.html#index-of-functions",
    "title": "Quick Reference",
    "section": "",
    "text": "Looking for a function you can’t quite remember how to use? You’re in the right place! The table below is arranged alphabetically by function name, and the linked full name (including relevant package calls) will take you to the help documentation.\n\n\n\n\n\nFunction Name\n\n\nLink to Help Documentation\n\n\nUsed In…\n\n\n\n\n\n\nall()\n\n\nall()\n\n\n05: Filter and Select\n\n\n\n\nanova()\n\n\nanova()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nany()\n\n\nany()\n\n\n05: Filter and Select\n\n\n\n\napa_print()\n\n\npapaja::apa_print()\n\n\n04: Reporting Linear Models with Quarto05: Filter and Select\n\n\n\n\napa_table()\n\n\npapaja::apa_table()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nas.numeric()\n\n\nas.numeric()\n\n\n01/02: IntRoduction\n\n\n\n\nautoplot()\n\n\nggplot2::autoplot()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nboxplot()\n\n\nboxplot()\n\n\n03: Datasets\n\n\n\n\nc()\n\n\nc()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select\n\n\n\n\nclass()\n\n\nclass()\n\n\n01/02: IntRoduction\n\n\n\n\ncontains()\n\n\ndplyr::contains()\n\n\n05: Filter and Select\n\n\n\n\ncor.test()\n\n\ncor.test()\n\n\n05: Filter and Select\n\n\n\n\ncorrelation()\n\n\ncorrelation::correlation()\n\n\n05: Filter and Select\n\n\n\n\ncount()\n\n\ndplyr::count()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\ndata()\n\n\ndata()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\ndescribe_distribution()\n\n\ndatawizard::describe_distribution()\n\n\n03: Datasets\n\n\n\n\nfilter()\n\n\ndplyr::filter()\n\n\n05: Filter and Select\n\n\n\n\nggpairs()\n\n\nGGally::ggpairs()\n\n\n05: Filter and Select\n\n\n\n\nggscatmat()\n\n\nGGally::ggscatmat()\n\n\n05: Filter and Select\n\n\n\n\nglance()\n\n\nbroom::glance()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nglimpse()\n\n\ndplyr::glimpse()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nhere()\n\n\nhere::here()\n\n\n03: Datasets\n\n\n\n\nhist()\n\n\nhist()\n\n\n03: Datasets\n\n\n\n\nis.factor()\n\n\nis.factor()\n\n\n05: Filter and Select\n\n\n\n\nis.na()\n\n\nis.na()\n\n\n05: Filter and Select\n\n\n\n\nkable_styling()\n\n\nkableExtra::kable_styling()\n\n\n03: Datasets\n\n\n\n\nlibrary()\n\n\nlibrary()\n\n\n03: Datasets04: Reporting Linear Models with Quarto05: Filter and Select\n\n\n\n\nlm()\n\n\nlm()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nlmRob()\n\n\nrobust::lmRob()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nmean()\n\n\nmean()\n\n\n01/02: IntRoduction03: Datasets\n\n\n\n\nmedian()\n\n\nmedian()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nmodel_parameters()\n\n\nparameters::model_parameters()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nnames()\n\n\nnames()\n\n\n03: Datasets\n\n\n\n\nncol()\n\n\nncol()\n\n\n03: Datasets\n\n\n\n\nnice_table()\n\n\nrempsyc::nice_table()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nnrow()\n\n\nnrow()\n\n\n03: Datasets05: Filter and Select\n\n\n\n\nplot()\n\n\nplot()\n\n\n03: Datasets\n\n\n\n\nprint()\n\n\nprint()\n\n\n05: Filter and Select\n\n\n\n\npull()\n\n\ndplyr::pull()\n\n\n03: Datasets\n\n\n\n\nrange()\n\n\nrange()\n\n\n03: Datasets\n\n\n\n\nread_csv()\n\n\nreadr::read_csv()\n\n\n03: Datasets\n\n\n\n\nround()\n\n\nround()\n\n\n01/02: IntRoduction03: Datasets04: Reporting Linear Models with Quarto\n\n\n\n\nsd()\n\n\nsd()\n\n\n03: Datasets\n\n\n\n\nselect()\n\n\ndplyr::select()\n\n\n05: Filter and Select\n\n\n\n\nstarts_with()\n\n\ndplyr::starts_with()\n\n\n05: Filter and Select\n\n\n\n\nsum()\n\n\nsum()\n\n\n05: Filter and Select\n\n\n\n\nsummary()\n\n\nsummary()\n\n\n03: Datasets\n\n\n\n\nt.test()\n\n\nt.test()\n\n\n01/02: IntRoduction\n\n\n\n\ntidy()\n\n\nbroom::tidy()\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nwhere()\n\n\ndplyr::where()\n\n\n05: Filter and Select"
  },
  {
    "objectID": "quick_ref.html#index-of-topics",
    "href": "quick_ref.html#index-of-topics",
    "title": "Quick Reference",
    "section": "Index of Topics",
    "text": "Index of Topics\nIf you’re looking for a particular section of a tutorial, use this handy summary to jump straight to the section you want.\n\n\n\n\n\nTopic\n\n\nSub-Topic\n\n\n\n\n\n\n01/02: IntRoduction\n\n\n\n\nOrientation\n\n\nThe RStudio Interface\n\n\n\n\nErrors\n\n\nGlossoRlia: the Language of Errors\n\n\n\n\nTypes of Data\n\n\nNumeric Data, Character Data, Logical Data\n\n\n\n\nClass and Coercion\n\n\n\n\n\n\nObjects\n\n\nCreating an Object, Calling an Object, Using Objects, Overwriting Objects\n\n\n\n\nFunctions\n\n\nBasics and Help, Arguments, Using Functions\n\n\n\n\nQuick Test: t-test\n\n\n\n\n\n\n03: Datasets\n\n\n\n\nSetup\n\n\nProjects, Documents, Installing and Loading Packages\n\n\n\n\nReading In\n\n\nReading from File\n\n\n\n\nCodebook\n\n\n\n\n\n\nViewing\n\n\nCall the Object, A Glimpse of the Data, View Mode\n\n\n\n\nOverall Summaries\n\n\nBasic Summary, Other Summaries\n\n\n\n\nThe Pipe\n\n\n\n\n\n\nDescribing Datasets\n\n\n\n\n\n\nDescribing Variables\n\n\nCounting, Subsetting, Descriptives, Visualisations\n\n\n\n\n04: Reporting Linear Models with Quarto\n\n\n\n\nThe Linear Model\n\n\nData and Codebook, One Predictor, Hierarchial Models, Assumptions Checks\n\n\n\n\nQuarto\n\n\nGetting Started, Creating a Code Chunk, Body Text, Dynamic Reporting, Rendering\n\n\n\n\n05: Filter and Select\n\n\n\n\nSetup\n\n\nPackages, Data\n\n\n\n\nFilter\n\n\nGeneral Format, Filtering with Assertions, Multiple Assertions, Data Cleaning\n\n\n\n\nSelect\n\n\nGeneral Format, Selecting Directly, Using {tidyselect}\n\n\n\n\nQuick Test: Correlation\n\n\nVisualisation, Testing Correlation"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jennifer Mankin",
    "section": "",
    "text": "Jennifer is a senior teaching-focused lecturer in Psychology at the University of Sussex. She is committed to making statistics and coding accessible, enjoyable, and engaging, particularly for learners who aren’t that enthusiastic about the whole endeavour.\nJennifer began teaching at the University of Sussex in 2016, originally teaching cognitive psychology and research methods and statistics with SPSS. In the 2019/2020 academic year, Psychology at Sussex switched to R for undergraduate methods and statistics teaching1. Since then, Jennifer has been just on the bright side of being hell-bent on helping anyone who will sit still long enough experience the joy of R.\nThe materials on this website are an attempt to do that: to share some of the hard-won skills and knowledge to support others to get excited about the potential of coding in R.\nIn her non-coding time, Jennifer does cross-stitch, plays D&D, potters about the garden, and collects oversized earrings."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "Jennifer Mankin",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYep, the timing was just about as bad as it could have been!↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Training at Sussex",
    "section": "",
    "text": "This is the website for training courses in the programming language R, run by the Methods Teaching Team in the School of Psychology, University of Sussex.\n\n\nAt the moment, our training sessions are only open to members of staff in the School of Psychology at Sussex, or by invitation.\nIf you are staff in the School of Psychology, see the School Bulletin for how to join the Canvas site and live sessions. Otherwise, you can email the training lead, but be aware that places are only available a case-by-case, exceptional basis for University of Sussex staff (academic or Professional Services) and only if there is space in the planned sessions."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "R Training at Sussex",
    "section": "",
    "text": "This is the website for training courses in the programming language R, run by the Methods Teaching Team in the School of Psychology, University of Sussex.\n\n\nAt the moment, our training sessions are only open to members of staff in the School of Psychology at Sussex, or by invitation.\nIf you are staff in the School of Psychology, see the School Bulletin for how to join the Canvas site and live sessions. Otherwise, you can email the training lead, but be aware that places are only available a case-by-case, exceptional basis for University of Sussex staff (academic or Professional Services) and only if there is space in the planned sessions."
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "R Training at Sussex",
    "section": "Materials",
    "text": "Materials\n\nTutorials\nThe Tutorials section of the site contains tutorial documents designed to accompany live training sessions. They provide explanations, examples, and exercises designed for complete beginners through improvers.\n\n\nWorksheets\nWorksheets are hosted on the Posit Cloud workspace for the training course. You can join the workspace via Canvas.\n\n\nRecordings\nLive sessions are recorded and made available, with automatically-generated captions, as soon as possible after the sessions are complete. View session recordings on Canvas."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Under Construction\n\n\n\nMany of the tutorials are still incomplete or under construction. If you encounter a banner like this, just check back another time!"
  },
  {
    "objectID": "tutorials.html#using-the-tutorials",
    "href": "tutorials.html#using-the-tutorials",
    "title": "Tutorials",
    "section": "Using the Tutorials",
    "text": "Using the Tutorials\nThese tutorials are designed to accompany live training sessions, but they also serve as quick-reference guides for all the material covered in those sessions.\n\nOpening the Tutorials\nIn live sessions, it is recommended to open the corresponding tutorial in the Viewer pane in Posit Cloud so that solutions and explanations are easily available. The workbook documents provided for each week will already contain the code to do this.\nHowever, the tutorials can also be easily accessed at any time through this website, so it isn’t necessary to open Posit Cloud to view them - simply use the sidebar to jump to the tutorial you want!\n\n\nExercises\nThe exercises are designed to build your skills in R. Some will ask you to try something you might not know how to do. Just give it your best shot, and if you get stuck, solutions and explanations are always provided!\n\n\n\n\n\n\nDon’t Skip the Exercises!\n\n\n\nIt is strongly recommended that you don’t skip the exercises. Try each one, even if it seems completely trivial, or too hard. You will learn R much faster and more thoroughly by getting your hands dirty.\n\n\nAll data and workbooks will be provided on Posit Cloud for completing the exercises.\n\nChallenges\nSome exercises will be clearly labeled as “Challenges”. These exercises are optional and are meant to go beyond the core tutorial material. However, if you skip them, you will still be able to understand everything that follows; you won’t need to complete them in order to proceed."
  },
  {
    "objectID": "tutorials.html#content",
    "href": "tutorials.html#content",
    "title": "Tutorials",
    "section": "Content",
    "text": "Content\n\n\n\n\n\n\nTip\n\n\n\nLooking for a particular topic or function? Use the Quick Reference to find what you need!\n\n\nTutorials are divided into three sections.\n\nFundRmentals\nThe three-part FundRmentals series covers essential basic skills in R, and is designed for absolute beginners who have never seen R before and who have little to no coding experience of any kind.\nBy the end of this series, you will be able to:\n\nNavigate the RStudio IDE\nCreate and work with different types of data\nWork with objects and functions\nPerform calculations and logical tests on single values and vectors\nRead in data from a .csv file into a tibble\nView, summarise, and arrange the order of a tibble\nCreate and render Quarto documents\nPerform and report t-test and linear model analyses\n\n\n\nEssentials\nThe four-part Essentials series is designed for novices with some basic skills in R, and follows on from the FundRmentals series. It covers the core data wrangling and analysis skills that we teach throughout the first year of the undergraduate Psychology course at Sussex, along with extra tips and techniques for efficient and transparent analysis to support dissertation supervisors to help their students.\nBy the end of this series, you will be able to:\n\nFilter cases and select variables, including efficient &lt;tidyselect&gt; semantics\nCreate new variables in a dataset, or change/recode existing ones\nCreate a variety of customised data visualisations\nPerform and efficiently report the results of t-tests, chi-squared tests, correlations, and simple and hierarchical linear models\n\n\n\nImprovRs\nThe four-part ImprovRs series is designed for those with a strong foundation in R who want to move to using R in part or entirely for their data management and analysis process, and follows on from the Essentials series. It covers specific skills in working with questionnaire data, advanced data wrangling, and an introduction to writing functions, with the aim of building a diverse toolbox of R skills.\nBy the end of this series, you will be able to:\n\nWork with labelled Qualtrics data and factors\nCreate standardised subscale scores\nGenerate automatic codebooks for Qualtrics datasets\nReshape (wide/long format) and merge/separate datasets\nWrite custom anonymous and named functions"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html",
    "title": "03: Datasets",
    "section": "",
    "text": "This tutorial is focused on working with datasets. It covers key functions and tips for reading in, viewing, and summarising datasets. It also introduces the pipe operator and a variety of common descriptive functions for investigating both whole datasets and individual variables, and concludes with a brief look at data visualisations with base R."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overview",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overview",
    "title": "03: Datasets",
    "section": "",
    "text": "This tutorial is focused on working with datasets. It covers key functions and tips for reading in, viewing, and summarising datasets. It also introduces the pipe operator and a variety of common descriptive functions for investigating both whole datasets and individual variables, and concludes with a brief look at data visualisations with base R."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#setup",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#setup",
    "title": "03: Datasets",
    "section": "Setup",
    "text": "Setup\nIn each session, we will always follow the same steps to set up. We’ll walk through the key elements here in detail and then provide a brief summary in future tutorials.\n\n\n\n\n\n\nSetup Steps\n\n\n\n\nCreate or open a project in RStudio\nCreate or open a document to work in\nLoad the necessary packages\n\n\n\n\nProjects\nProjects are the main way that RStudio organises related files and information. Projects are associated with a working directory; everything in that directory, including all the sub-folders, are part of the same project.\nIt is highly recommended that you create a new project for each separate thing you want to work on in R. Among other advantages, it makes navigating folders much easier (see Reading In), allows you to easily pick up work from where you left off, and retain all the settings and options you have set each time you work on the same project.\n\n\n\n\n\n\nCreating a Project\n\n\n\n\n\nOn Posit Cloud, you don’t really have a choice in the matter - you must create or open a project in the Cloud workspace in order to do anything.\nOn a desktop installation, you can create a new directory as a project or associate a project file with an existing directory.\nSee Posit’s Using RStudio Projects guide or Andy Field’s video guide to RStudio Desktop for more information.\n\n\n\n\n\nDocuments\nAs we discussed in the previous tutorial, one of the key strengths of doing your work using R (or any other programming language) is reproducibility - in other words, every step of the process from raw file to output is documented and replicable. However, this is only the case if you do in fact write your code down somewhere! To do that, you’ll need to create some kind of document to record your code. There are two main types of documents you might consider using: a script or a Quarto document.\n\nQuarto documents\nQuarto documents contain a combination of both non-code text and code. The main strength of these documents is that they can be rendered to some other output format, such as HTML, PDF, or Word, by executing all of the code and combining it with the text to create a nicely formatted document.\nWe will investigate the options for Quarto documents in depth in the next tutorial. For now, use the Quarto document in your project on Posit Cloud for your work in this tutorial.\n\n\nScripts\nScripts are text files that RStudio knows to read as R code. They have a .R file extension and can ONLY contain code. They are very useful for running code behind the scenes, so to speak, but not great for reviewing or presenting results.\n\n\n\n\n\n\nQuarto or Script?\n\n\n\nWhen deciding what kind of document to create, think about what you want to do with the output of your work.\n\nUse Quarto if the document needs to contain any amount of text, or will be used to share the output of your code in a presentable way, such as notes for yourself, reports, articles, websites, etc.\nThe page you’re reading now is (or was!) a Quarto document.\nUse a script if the document only needs to contain code and has a solely functional purpose, such as cleaning a dataset, manipulating files, defining new functions, etc.\nI use a script to process all of the tutorial documents and generate the Quick Reference page.\n\nIn this series, we will almost always use Quarto documents, but scripts are an essential part of the development side of R.\n\n\n\n\n\nInstalling and Loading Packages\nIn the previous tutorial, we saw how the main way that R does anything is via functions. All functions belong to a package, which are extensions to the R language. Packages can contain functions, documentation for those functions, datasets, sample code, and more. Some packages, like the {base} and {stats} packages that contain the mean() and t.test() functions that we saw previously, are included by default with R. However, you will often want to use functions from packages that aren’t included by default, so you must do this explicitly.\nIn order to utilise the functions in a package, you must do two things:\n\nInstall the package (only once per device, or to update the package) using install.packages(\"package_name\") in the Console\nLoad the package (every time you want to use it) using library(package_name) at the beginning of each new document\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you are working on these tutorials on the Posit Cloud workspace, all of the packages you need have been installed already. Please do not try to install any packages, as this could cause unexpected conflicts or errors.\n\n\n\n\n\n\n\n\nMore About Installing vs Loading Packages\n\n\n\n\n\nWhen you install R and RStudio for the first time on a device, this is like buying a new mobile phone. When you get a new phone, it comes with some apps pre-installed, like a messaging app, a camera, a calculator, etc. If you only ever wanted to take pictures and do basic maths with your phone, you could probably leave it at that. Most likely, though, you want to use other apps that don’t come with the phone - like WhatsApp, or Facebook. Let’s say you’ve just got a new phone and you want to use WhatsApp. To do this, you’ll need to:\n\nGo to your phone’s app store and download WhatsApp (only once per device, or to update the app)\nOpen the app (every time you want to use it)\n\nAs you can see, these steps correspond almost exactly to the installing vs loading steps described above. In order to use a package that doesn’t come pre-installed with R, you have to do both of these things.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLoad the {tidyverse} package in your Quarto document.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\nWhen you load {tidyverse} for the first time, quite a lot of extra stuff gets printed along with it. All this output looks alarming, but these aren’t errors or warnings - they’re just messages. Messages are like warnings, but neutral: they just contain information that you might find helpful.\nThe usual {tidyverse} message contains two parts:\n\nAttaching core tidyverse packages tells you which packages have just been loaded. Essentially, library(tidyverse) is a shortcut for loading all of these packages individually. Somewhat confusingly, installing {tidyverse} installs more packages than are in this list (for example, {magrittr} and {rlang}), many of which other {tidyverse} packages rely on to function. If you want to load them, you can use library() to do this - but you don’t need to unless you’re using those packages explicitly. For our purposes now, just the default {tidyverse} packages are fine.\nConflicts tells you about any package conflicts as a result of loading the packages. If you’re curious, conflicts are explained further in the callout box below.\n\n\n\n\n\n\n\nConflicts\n\n\n\n\n\nThere are lots and lots of packages for R. At the time of this writing, CRAN (the repository for R packages) contains just shy of 20,000 R packages, with many, many more on Github and elsewhere. Although people generally try to avoid it, it is necessarily the case that sometimes, people give the same name to two different functions from two different packages.\nSo, if you have those packages both loaded, how does R know which one to use? This situation is called a conflict, and is resolved in a few different methods.\nMethod 1: Recency\nIn the absence of any other information, R will use the function from the package that was loaded most recently. This is exactly what’s happening in the {tidyverse} message above.\nThere are two conflicts mentioned. one of which reads:\nx dplyr::filter() masks stats::filter()\n{stats}, you might remember, is a package that is always installed with R and is loaded by default. So, the {stats} package has a function called filter() that is already loaded to begin with. When we loaded {tidyverse}, one of the new packages, {dplyr}, also contains a function called filter(). Because {dplyr} has been loaded more recently, if you write a bit of code using filter(), the one you will get is dplyr::filter()1. In other words, the more recently loaded dplyr::filter() covers over, steps in front of, or (in R terminology) “masks” stats::filter().\nNow, what if you actually want to use filter() from {stats} instead? Well, in that case you might want to use…\nMethod 2: Explicit style\nAbove we saw several examples of the package::function() notation, called “explicit” or “verbose” coding style. With explicit style, there isn’t actually a conflict between stats::filter() and dplyr::filter() anymore, because their package calls are clearly stated so R doesn’t have to guess which filter() you want. So, if you had loaded {tidyverse}, you could write stats::filter() in your code, and you would still get the function from the {stats} package even with {dplyr} loaded.\nAnother, secret benefit of explicit style is that as long as you have a package installed, you can use a function from that package without having to load it. Imagine I start a new project (so I have stats::filter() already loaded by default). If I only want to quickly use dplyr::filter(), I can use explicit style to use that function without having to run library(tidyverse).\n\n\n\nThe style you’ll see in these tutorials is a pretty devotedly explicit style: that is, I’ll always write dplyr::filter() instead of just filter(). I only leave out the package name in a few situations:\n\nWhen the function is from a default-loaded package, like {base} or {stats} (so I write mean() instead of base::mean()). This is mostly just convenience!\nWhen there are lots of functions from the same package in row, all of which have very distinctive names, that would make reading the code very difficult and writing the code very repetitive. This is the case, for example, with {ggplot2}, which we will encounter in the next section of the course. For cases like this, I make sure to explicitly load the relevant package and then leave off the package name.\n\nI like explicit style because I never have to deal with package conflicts; I rarely have to load packages; and it helps me understand better how my code works. You don’t have to use it, and most of the time it won’t make that much difference, so do what makes sense to you. Just to be safe, though, we’ll load {tidyverse} regularly."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#reading-in",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#reading-in",
    "title": "03: Datasets",
    "section": "Reading In",
    "text": "Reading In\nNow that we’ve completed our core setup, we’re ready to get stuck in working with datasets. For the purposes of practicing, we’re going to use some real, open-source data from a real paper by Mealor et al. from their (2016) paper developing the Sussex Cognitive Styles Questionnaire (SCSQ). The SCSQ is a six-factor cognitive styles questionnaire validated by comparing responses from people with and without synaesthesia.\nWe’re going to start by importing, or “reading in”, the data from a location outside of R. The first job is to work out how that data is stored: the file format that it’s in, and the location we need to give to R to look for it.\nFor the purposes of these tutorials, we’ll primarily make use of .csv file types when reading in. CSV (Comma-Separated Values) is a common, programme-agnostic file type without any fancy formatting, just plain text. To practice this, we’ll use the read_csv() function from the {readr} package (part of {tidyverse}).\nOne advantage of readr::read_csv() is that it will output a special kind of dataset, called a tibble. Tibbles are a fundamental component of {tidyverse}. They are a sort of embellished dataframe or table (“table” &gt; “tbl” &gt; “tibble”) with some extra bells and whistles for convenience. We’ll discover their features as we go, but you can get a quick overview of tibbles here.\n\n\n\n\n\n\nReading Other Filetypes\n\n\n\n\n\nIf you have data stored in other types of files, you may need other functions from other packages to read them in. It will depend substantially on what’s in the file and how the data are structured, so you will likely need to do some experimentation to find the best option.\nHere are some possibilities to get you started. All of them (except the last) output a tibble.\n\nExcel (xlsx): readxl::read_xlsx()\nSPSS (.sav): haven::read_sav()\nSAS (.sas): haven::read_sas()\nJSON: rjson::fromJSON()\n\n\n\n\nOur next job is to figure out where the data is stored. For the purposes of practice, we’ll look at two possibilities. First, that the data is stored in a local file on your computer; and second, that the data is hosted online somewhere, accessible via URL.\n\nReading from File\nThe scenario you are most likely to encounter is that you have some data in a folder on your computer, and you’d like to import, or “read in”, this data to R so you can work with it. To practice this, in your Posit Cloud project there is a folder named “data” that contains a file called “syn_data.csv”. (If you are not on the Cloud, skip down to the next section.)\nIn order to use the read_csv() function, we need to give it the file path as a string (i.e. in \"quotes\"). Let’s make this easier by using a helper function: here::here().\n\n\n\n\n\n\nExercise\n\n\n\nRun the here::here() function to see what it does.\n\n\nWhat you should get is a string - a file path to the project you are currently in. On Posit Cloud, this will always be “/cloud/project” (unless you change it). In any case, it will always point to the location of the .Rproj file that denotes your current project.\nWhy is this useful? Instead of having to write out long file paths (“C/Users/my_folder/What Was It called-again?/…”), or trying to figure out where your current file is relative to the data (or image, or whatever) that you are trying to find, here::here() uses the project file as a fixed point. So, all file paths can be written starting from the same point.\n\n\n\n\n\n\nExercise\n\n\n\nUse here::here() to generate a file path to the syn_data data file.\nThen, use readr::read_csv() to read in the syn_data.csv file and store the result in an object called syn_data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming you are on the Cloud, your here::here() command should look like this. The first part of the file path will be generated by here::here() up to the project file; from there, we look in the data folder, in which we can find the syn_data.csv file. (Don’t forget the .csv file extension!)\n\nhere::here(\"data/syn_data.csv\")\n\nTo read in the file, we add two things. First, we put the here::here() command - which outputs the file path - into readr::read_csv(), which actually imports the data at that file path into a tibble. Then, we save that tibble into an object called syn_data using the assignment operator, &lt;-.\n\nsyn_data &lt;- readr::read_csv(here::here(\"data/syn_data.csv\"))\n\n\n\n\n\n\n\nReading from URL\nIf the data is hosted somewhere online, you can give the hosting URL to R as a string. Assuming you have an Internet connection (!), R will go to that URL and parse the data.\n\n\n\n\n\n\nExercise\n\n\n\nRead the CSV file hosted at https://raw.githubusercontent.com/drmankin/practicum/master/data/syn_data.csv and save it to the object name syn_data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/syn_data.csv\")"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#codebook",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#codebook",
    "title": "03: Datasets",
    "section": "Codebook",
    "text": "Codebook\nAs this dataset is likely unfamiliar, the codebook below explains what the variables in this dataset represent.\n\n\n\n\n\n\nMore About Synaesthesia\n\n\n\n\n\nThis dataset focuses on cognitive styles, particularly in people with and without a neuropsychological condition called synaesthesia. Synaesthesia is colloquially referred to as a “blurring of the senses” that can manifest in many different ways. For example, some people with synaesthesia may percieve colours associated with letters or words, or see shapes when they hear music. These additional perceptions are typically automatic and consistent across time.\nThis particular study focused on two different types of synaesthesia: grapheme-colour and sequence-space. People with grapheme-colour synaesthesia experience colour associated with written language, i.e. graphemes. For instance, the letter “Q” may be purple, or the word “cactus” may be red (or a combination of colours).\nPeople with sequence-space synaesthesia associate sequences, such as numbers, days of the week, or months of the year, with particular locations in physical space. For instance, Monday may be located up and to the right, or July near the hip. Sequence-space synaesthetes can often precisely describe and point to the specific location of each element of the sequence.\nThere are also a variety of qualities associated with having synaesthesia of any type, so this dataset also includes a variable coding for having either (or both) types.\n\n\n\n\n\n\n\n\nVariable Name\nType\nDescription\n\n\n\n\nid_code\nfactor\nParticipant ID number\n\n\ngender\nfactor\nParticipant gender, 0 = female, 1 = male\n\n\ngc_score\nnumeric\nScore on the grapheme-colour test of the Synesthesia Battery (Eagleman et al., 2007). Scores of 1.43 or lower indicate genuine synaesthesia (Rothen et al., 2013)\n\n\nsyn\nfactor\nWhether the participant is a synaesthete (Yes) or not (No), regardless of type of synaesthesia\n\n\nsyn_graph_col\nfactor\nWhether the participant has grapheme-colour synaethesia (Yes) or not (No)\n\n\nsyn_seq_space\nfactor\nWhether the participant has sequence-space synaethesia (Yes) or not (No)\n\n\nscsq_imagery\nnumeric\nMean score on the Imagery Abilitysubscale of the SCSQ\n\n\nscsq_techspace\nnumeric\nMean score on the Technical/Spatial subscale of the SCSQ\n\n\nscsq_language\nnumeric\nMean score on the Language and Word Forms subscale of the SCSQ\n\n\nscsq_organise\nnumeric\nMean score on the Organisation subscale of the SCSQ\n\n\nscsq_global\nnumeric\nMean score on the Global Bias subscale of the SCSQ\n\n\nscsq_system\nnumeric\nMean score on the Systemising Tendency subscale of the SCSQ"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#viewing",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#viewing",
    "title": "03: Datasets",
    "section": "Viewing",
    "text": "Viewing\nWe’ve now got some data to work with! Before we jump into doing anything with it, though, we should take a look at it. This is always a good idea to check that our data has read in correctly without any parsing errors. But our data is tucked away in an object! How can we take a look at it?\n\nCall the Object\nOur first option is to call the object that contains our data. This is almost always an easy and straightforward way to get an instant look at what’s in our data.\n\n\n\n\n\n\nExercise\n\n\n\nCall the syn_data object to see what it contains.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data\n\n# A tibble: 1,211 × 12\n   id_code gender gc_score syn   syn_graph_col syn_seq_space scsq_imagery\n     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;                &lt;dbl&gt;\n 1       1      0    NA    Yes   No            Yes                   4.88\n 2       2      0    NA    Yes   No            Yes                   4.82\n 3       3      1    NA    Yes   No            Yes                   3.59\n 4       4      0    NA    Yes   No            Yes                   4.71\n 5       5      1     1.4  Yes   Yes           Yes                   4.65\n 6       6      0     1.34 Yes   Yes           No                    4.06\n 7       7      0     1.3  Yes   Yes           Yes                   4.71\n 8       8      0     1.19 Yes   Yes           No                    4.47\n 9       9      0     1.03 Yes   Yes           Yes                   4.12\n10      10      0     1.02 Yes   Yes           No                    3.71\n# ℹ 1,201 more rows\n# ℹ 5 more variables: scsq_techspace &lt;dbl&gt;, scsq_language &lt;dbl&gt;,\n#   scsq_organise &lt;dbl&gt;, scsq_global &lt;dbl&gt;, scsq_system &lt;dbl&gt;\n\n\n\n\n\n\n\nYou may notice a few of those “bells and whistles” I mentioned earlier here.\n\nBy default, tibbles like this one only print out up to the first ten rows at a time, and as many columns as conveniently fit in your current window size.\nYou can scroll through this printout by clicking the numbers at the bottom (to move through rows) or the left and right arrows at the top (to scroll through columns).\nEach column has a little tag underneath it to tell you what kind of data is currently stored in it, for example &lt;dbl&gt; for numeric/double and &lt;chr&gt; for character.\nIn the top left, the little box tells you what it is (“A tibble”) and the size of the dataset (“1211 x 12”).\n\n\n\n\n\n\n\nWarning\n\n\n\nThere’s a big caveat here: this works great with tibbles. For data stored in other formats, like matrices, there’s no preset formatting like this. If you accidentally call the name of an object that contains thousands of rows, R will try to print them all, which can lead to crashes. So, avoid calling very large objects directly like this if they aren’t tibbles.\n\n\n\n\nA Glimpse of the Data\nAs mentioned above, just calling the dataset isn’t always super helpful - it depends on the size of your screen and even the width of your current window! As the next step, let’s get an overview of this dataset using the glimpse() function from the {dplyr} package.\n\n\n\n\n\n\nExercise\n\n\n\nUse dplyr::glimpse() to get a glimpse of your dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndplyr::glimpse(syn_data)\n\nRows: 1,211\nColumns: 12\n$ id_code        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ gender         &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ gc_score       &lt;dbl&gt; NA, NA, NA, NA, 1.40, 1.34, 1.30, 1.19, 1.03, 1.02, 1.0…\n$ syn            &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\",…\n$ syn_graph_col  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n$ syn_seq_space  &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"…\n$ scsq_imagery   &lt;dbl&gt; 4.88, 4.82, 3.59, 4.71, 4.65, 4.06, 4.71, 4.47, 4.12, 3…\n$ scsq_techspace &lt;dbl&gt; 2.06, 2.18, 3.35, 2.82, 3.47, 2.88, 2.76, 2.76, 2.00, 2…\n$ scsq_language  &lt;dbl&gt; 3.33, 5.00, 2.83, 4.67, 4.83, 4.67, 3.67, 3.67, 4.00, 4…\n$ scsq_organise  &lt;dbl&gt; 2.67, 4.00, 2.33, 3.33, 2.00, 4.00, 2.83, 1.83, 3.33, 3…\n$ scsq_global    &lt;dbl&gt; 1.50, 1.13, 2.13, 2.25, 2.50, 4.13, 3.00, 2.50, 3.88, 1…\n$ scsq_system    &lt;dbl&gt; 2.00, 4.83, 3.50, 3.83, 4.50, 2.00, 2.83, 2.33, 1.50, 2…\n\n\n\n\n\n\n\nThis gives us a nice overview of all the variables we have in the data (each preceded by $, which we’ll come back to in the second half); what kind of data they are (e.g. &lt;chr&gt;, &lt;dbl&gt;, and so on); and a look at the first few values in each variable. This is a great way to check that all the variables you expect to be there are there, and that they contain (more or less) what you thought they should.\nBut what if we really want to get a look at the entire dataset? For that we need…\n\n\nView Mode\nWe can have a look at the whole dataset more easily - and interact with it to some degree - by viewing it, which opens a copy of the dataset in the Source window to look through. We can do this with the View() function (note the capital “V”!).\n\n\n\n\n\n\nExercise\n\n\n\nOpen the syn_data dataset using the View() function in the Console.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nView(syn_data)\n\n\n\n\n\n\nThis View mode has a few very handy features. Take a moment now to explore and work out how to do the following.\n\n\n\n\n\n\nExercise\n\n\n\nUsing only View mode, figure out the following:\n\nWhat is the range of the variable gc_score?\nHow can you arrange the dataset by score in scsq_imagery?\nHow many participants had “Yes” in the variable syn_graph_col?\nWhich gender category had more participants?\nOf the participants who said “Yes” to syn_seq_space, what was the highest SCSQ technical-spatial score?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHover your mouse over the variable label gc_score to see a tooltip reporting the range.\nUse the small up/down arrows next to each variable label to reorder the dataset by the values in that variable.\nClick on the “Filter” button in the top left to open filter view. Then, click on the text box under syn_graph_col and type “Yes”. You can now see a dataset with only the Yes responses.\nIn filter view, click on the text box under gender to open a histogram of the values in this variable.\nIn filter view, click on the text box under syn_seq_space and type “Yes”. Then, use the up and down buttons to arrange in descending order for the variable scsq_techspace.\n\n\n\n\n\n\nThese features are really useful to have a quick poke around the data or check that everything is in order. However, keep in mind an important point: None of the changes made in View mode affect the data. View mode is essentially read-only; there’s no way to actually change the dataset or extract the values (like the range or max value) outside of copying them down by hand2. We’ll have to use R to work with the data in order to do that.\n\n\n\n\n\n\nNo Touching\n\n\n\nIf you are used to programmes like SPSS or Excel, where you can directly edit or work with the data in the spreadsheet, switching to R can be quite a frustrating change. Even though View mode looks similar, it’s like the dataset is behind glass - you can’t affect it directly. As we start working with the data via objects and functions, it may feel a bit like you are trying to work blindfolded - you can’t actually “see” what you are doing as you do it.\nIf you feel that way, be reassured that it’s normal. Working with objects rather than with spreadsheets or data directly takes some getting used to, and it will get easier with practice. Use View() freely to check your work - I do!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overall-summaries",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#overall-summaries",
    "title": "03: Datasets",
    "section": "Overall Summaries",
    "text": "Overall Summaries\nWe’ve now gained some confidence that our data looks like data should. We got a look at some summary information in View mode, but although this might have been useful for us in our initial checks, we can’t easily record or reproduce that information. Next, we’re going to look at some options for getting summary information about the whole dataset.\n\nBasic Summary\nThe quickest and easiest check for a whole dataset is the base R function summary(). This function doesn’t do anything fancy (at all) but it does give you a very quick look at how all the variables have been read in, and an early indication if there’s anything exciting wonky going on.\n\n\n\n\n\n\nExercise\n\n\n\nPrint out a summary of syn_data using the summary() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(syn_data)\n\n    id_code           gender          gc_score          syn           \n Min.   :   1.0   Min.   :0.0000   Min.   :0.3500   Length:1211       \n 1st Qu.: 303.5   1st Qu.:0.0000   1st Qu.:0.5600   Class :character  \n Median : 606.0   Median :0.0000   Median :0.7200   Mode  :character  \n Mean   : 606.0   Mean   :0.1982   Mean   :0.7558                     \n 3rd Qu.: 908.5   3rd Qu.:0.0000   3rd Qu.:0.8750                     \n Max.   :1211.0   Max.   :1.0000   Max.   :1.4000                     \n                                   NA's   :1168                       \n syn_graph_col      syn_seq_space       scsq_imagery   scsq_techspace\n Length:1211        Length:1211        Min.   :1.240   Min.   :1.24  \n Class :character   Class :character   1st Qu.:3.410   1st Qu.:2.41  \n Mode  :character   Mode  :character   Median :3.760   Median :2.82  \n                                       Mean   :3.715   Mean   :2.86  \n                                       3rd Qu.:4.060   3rd Qu.:3.29  \n                                       Max.   :5.000   Max.   :4.88  \n                                                                     \n scsq_language   scsq_organise    scsq_global     scsq_system   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:2.670   1st Qu.:2.500   1st Qu.:2.170  \n Median :3.670   Median :3.170   Median :2.880   Median :2.670  \n Mean   :3.581   Mean   :3.109   Mean   :2.939   Mean   :2.672  \n 3rd Qu.:4.170   3rd Qu.:3.670   3rd Qu.:3.380   3rd Qu.:3.170  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :4.830  \n                                                                \n\n\n\n\n\n\n\nHere, for example, notice the gender variable. This is intended to be a categorical variable, but clearly something has gone pear-shaped, because it has read in as a numeric variable. We have a related, but different issue with the syn_* variables, which also should be categorical (“Yes” and “No”), but instead have been read as numeric. Our other variables, gc_score and the scsq variables, should contain numeric information and it appears they do; for them, we get some helpful information and measures of central tendancy.\nWe will ignore the categorical issue for now until we cover how to make changes to the dataset in the next tutorial.\nsummary() is quick, and because it’s a base-R function, it doesn’t need any package installations to work. However, it’s also of limited use: its output is ugly, and it would be pretty difficult to get any of those values out of that output for reporting!\n\n\nOther Summaries\nBesides the basic summary, there are many ready-made options in various packages to quickly produce summary tables. At the UG level, students are introduced datawizard::describe_distribution(), which is one such function. To use it, simply put the name of the dataset object inside the brackets.\n\n\n\n\n\n\nTip\n\n\n\nBesides its default settings, the output can be further customised to add or remove particular statistics; see the help documentation.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nPrint out a summary of syn_data using the datawizard::describe_distribution() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndatawizard::describe_distribution(syn_data)\n\n# A tibble: 9 × 10\n  Variable          Mean      SD     IQR   Min     Max Skewness Kurtosis     n\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1 id_code        606     350.    606      1    1211      0        -1.2    1211\n2 gender           0.198   0.399   0      0       1      1.52      0.299  1211\n3 gc_score         0.756   0.253   0.33   0.35    1.4    0.747     0.251    43\n4 scsq_imagery     3.72    0.535   0.650  1.24    5     -0.493     0.834  1211\n5 scsq_techspace   2.86    0.632   0.88   1.24    4.88   0.134    -0.217  1211\n6 scsq_language    3.58    0.755   1.17   1       5     -0.221    -0.426  1211\n7 scsq_organise    3.11    0.722   1      1       5     -0.152    -0.239  1211\n8 scsq_global      2.94    0.601   0.88   1       5      0.0858    0.156  1211\n9 scsq_system      2.67    0.720   1      1       4.83   0.157    -0.377  1211\n# ℹ 1 more variable: n_Missing &lt;int&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: There are some variables missing from this output. What are they? Why aren’t they included?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe two character variables, syn_seq_space and syn_graph_col, are missing from the output. Under Details, the help documentation says: “If x is a data frame, only numeric variables are kept and will be displayed in the summary.” Since these are not numeric variables, they’ve been dropped."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#the-pipe",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#the-pipe",
    "title": "03: Datasets",
    "section": "The Pipe",
    "text": "The Pipe\nBefore we go on, we’re going to meet a new operator that will form the core of our coding style from this point onwards: the pipe. We’ll begin working with it a bit today, so let’s first explore why it’s so useful.\nIn this and the previous tutorial, we’ve seen some examples of “nested” code - functions nested within functions, as below.\nround(mean(quiz_9am), digits = 2)\nTo read this code, you have to start at the innermost level of nesting and work outwards. So, first R gets the quiz_9am object; then calculates the mean using mean(); then the output of mean() is the input to round(). For one or two levels of nesting, this is still legible, but can quickly become very difficult to track.\nOne solution is to use the pipe operator, |&gt;. The pipe “chains” commands one after the other by taking the output of the preceding command and “piping it into” the next command, allowing a much more natural and readable sequence of steps - sequentially, rather than nested. The pipe version of the above might look like this:\nquiz_9am |&gt; \n  mean() |&gt; \n  round(digits = 2)\nThis style maps on a lot more naturally to how we would read or understand the steps in this command in natural language.\n\n\n\n\n\n\nDefinition: Pipe\n\n\n\nThe pipe operator may appear in two formats.\n\nThe native pipe, |&gt;. This is the pipe we will use throughout these tutorials. It is called the “native” pipe because it is inbuilt into R and doesn’t require any specific package to use.\nThe {magrittr} pipe, %&gt;%. This pipe comes from {tidyverse}, and in particular requires the {magrittr} package to use. You will very commonly see this pipe in scripts, Stack Overflow posts, from ChatGPT, etc. as until the native pipe was introduced to R in 2022, the {magrittr} pipe was “the pipe” for R.\n\nIn most cases, including almost all of the code we will learn in these tutorials, the two pipes are interchangeable and will result in the same output.\n\n\nConceptually, the pipe works taking the output of the code on the left and passing it into the first argument of whatever comes after it on the right. Many functions - both from the {tidyverse} and not - are already set up so that the first argument is the data, and {tidyverse} functions are explicitly designed this way in order to work best with the pipe.\nFor functions where this is not the case, you can determine where the piped-in information should go using a “placeholder”. The most noticeable difference is that the native pipe placeholder is _, while the magrittr pipe placeholder is . There’s an example using placeholders below to help make this clearer.\nFrom this point forward, we’ll start working with the native pipe. The following sections will have specific examples on using the pipe to practice. Working with the pipe is also a good chance to practice “translating” R code into English, which we’ll do as we go. To give you an idea, here’s an example of how we can read the following code.\n\n1some_numbers &lt;- c(3.52, 7.03, 9.2, 10.11, 2)\n\n2some_numbers |&gt;\n3  mean() |&gt;\n4  round(2)\n\n\n1\n\nCreate a new object, some_numbers, that contains a vector of some numbers.\n\n2\n\nTake some_numbers, and then…\n\n3\n\nCalculate the mean of those numbers, and then…\n\n4\n\nRound that mean to two decimal places, which outputs:\n\n\n\n\n[1] 6.37\n\n\nSo, we’ll typically read |&gt; as “and then”, taking the output of whatever the preceding line produces and passing it on to the next line. To illustrate this more clearly, here’s the same code explicitly including the placeholders to indicate where the piped-in information goes:\n\nsome_numbers &lt;- c(3.52, 7.03, 9.2, 10.11, 2)\n\nsome_numbers |&gt;\n  mean(x = _) |&gt;\n  round(x = _, 2)\n\nThink of the placeholder like a bucket or landing pad, where the information coming in from the pipe falls. In this example, the placeholders aren’t necessary, because at each step, we already want the piped-in information to go into the first argument; but including the placeholders helps to see what’s going on.\nTo make this fully explicit, this illustration shows the progress of information through the same pipe. The orange arrows show how the piped-in information is passed from one function to the next. The teal arrows show what is produced at each step of the pipe - which is what is passed on to the next function via the orange arrows/pipe.\n\n\n\n\n\n\n\nA Sweet Pipe Example\n\n\n\n\n\nImagine we wanted to bake a Victoria sponge cake using R. Translating the steps into R, we might get something like this:\ningredients |&gt; \n  mix(order = c(\"wet\", \"dry\")) |&gt; \n  pour(shape = \"round\", number = 2, lining = TRUE) |&gt; \n  bake(temp = 190, time = 20) |&gt; \n  cool() |&gt; \n  assemble(filling = c(\"buttercream\", \"jam\"), topping = \"icing_sugar\") |&gt; \n  devour()\nAt each step, |&gt; takes whatever the previous step produces and passes it on to the next step. So, we begin with ingredients - presumably an object that contains our flour, sugar, eggs, etc - which is “piped into” the mix() function. The output of that function might be all our ingredients mixed together in a bowl, which is then piped into the pour() function, and so on.\nNotice for example, the function cool(), which doesn’t appear to have anything in it. It actually does: the cool() function would work with whatever the output of the bake() function was above it.\nWithout the pipe, our command might look something like this, which must be read from the inside out rather from top to bottom:\ndevour(\n  assemble(\n      cool(\n        bake(\n          pour(\n            mix(ingredients, \n                order = c(\"wet\", \"dry\")),\n            shape = \"round\", number = 2, lining = TRUE),\n          temp = 190, time = 20)\n      ),\n    filling = c(\"buttercream\", \"jam\"), topping = \"icing_sugar\"\n  )\n)\nThis is, I am sure you will agree, as absolutely horrifying as a soggy bottom on a cake."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-datasets",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-datasets",
    "title": "03: Datasets",
    "section": "Describing Datasets",
    "text": "Describing Datasets\nTo start, we’ll work again with the whole dataset and look at some helpful functions that are often important for validating our data processing.\n\nnrow(): Returns the number of rows as a numeric value.\nncol(): Returns the number of columns as a numeric value.\nnames(): Returns a character vector of the names of the columns of a dataset (and also the names of elements for other types of input).\n\nIf your dataset is structured like this one is - with a single participant per row - then nrow() is a common stand-in for counting participants.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the native pipe, print out the number of columns and the names of those columns in the syn_data dataset.\nHint: This will be two separate commands!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data |&gt; \n  ncol()\n\n[1] 12\n\nsyn_data |&gt; \n  names()\n\n [1] \"id_code\"        \"gender\"         \"gc_score\"       \"syn\"           \n [5] \"syn_graph_col\"  \"syn_seq_space\"  \"scsq_imagery\"   \"scsq_techspace\"\n [9] \"scsq_language\"  \"scsq_organise\"  \"scsq_global\"    \"scsq_system\"   \n\n\nThe new line after the pipe isn’t essential (it will run exactly the same way) but it is highly recommended. Although it doesn’t make much of a difference here, we will shortly get to longer commands where the new line for each new function will make a big difference to legibility!\n\n\n\n\n\n\n\n\n\n\n\nWhat’s Going On With the Pipe?\n\n\n\n\n\nIf a command like syn_data |&gt; names() looks a bit strange, let’s take a closer look at it.\nThis command is equivalent to names(syn_data), which might look a bit more familiar based on what we’ve done so far. The pipe takes whatever comes before it - in this case, the dataset syn_data - and pipes it into the first argument of the function that comes after it. The names() function only accepts one object as input, so syn_data is passed to names() as that single object. It looks like the names() function is empty, because there’s nothing in the brackets, but that’s because the dataset is being “piped in” from above.\nWe can make this a bit more explicit using the placeholder:\n\nsyn_data |&gt; \n  names(x = _)\n\n [1] \"id_code\"        \"gender\"         \"gc_score\"       \"syn\"           \n [5] \"syn_graph_col\"  \"syn_seq_space\"  \"scsq_imagery\"   \"scsq_techspace\"\n [9] \"scsq_language\"  \"scsq_organise\"  \"scsq_global\"    \"scsq_system\"   \n\n\nThe underscore is the “placeholder” for the native pipe; in other words, it explicitly indicates where the object should be placed that is being piped in, like a “bucket” that catches whatever comes out of the pipe! This makes it a bit clearer to see that the object syn_data is going into the names() function, and specifically the x argument.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUsing the native pipe, save the number of participants in the syn_data dataset in a new object of your choice.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npx_initial &lt;- syn_data |&gt; \n  nrow()\n\nThis format takes a bit of getting used to. The new object, which I’ve called px_initial3, is created at the first line of the command by the &lt;-. However, this object contains whatever the final output of this pipe is at the end - in this case, the number of rows as a numeric value produced by nrow()."
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-variables",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#describing-variables",
    "title": "03: Datasets",
    "section": "Describing Variables",
    "text": "Describing Variables\nOnce we’ve had a look at the whole dataset, it’s time to drill down into individual variables. We may want to calculate quick descriptives or investigate what’s going on with particular variables that seem to have issues.\nTo do this, we’ll start working quite a bit with the {dplyr} package. {dplyr} is a core part of the {tidyverse}, and the package generally is focused on user-friendly and easily readable tools for data manipulation. The essential {dplyr} functions will form a core part of the Essentials part of the course, when we really get into to working with data.\n\nCounting\nWe’ll start by having a look at character or categorical variables. Here we’ll meet our first {dplyr} function: dplyr::count(). This function is a friendly way to obtain (as you might expect!) counts of the number of times each unique value appears in a variable. As with just about everything in {dplyr}, it takes a tibble as input and produces a new tibble as output.\nUsing the pipe structure we’ve seen previously, the general form is:\ndataset_name |&gt; \n  dplyr::count(variable_to_count, optionally_another, ...)\nMinimally you need to provide a single variable to count the values in, but you can add more, separated by commas, to further subdivide the counts.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the syn_data dataset, produce a tibble of counts of how many participants had any kind of synaesthesia. Then, produce a second tibble, adding in gender as well.\nHint: Use the codebook to find the variables to use.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsyn_data |&gt; \n  dplyr::count(syn)\n\n# A tibble: 2 × 2\n  syn       n\n  &lt;chr&gt; &lt;int&gt;\n1 No     1090\n2 Yes     121\n\nsyn_data |&gt; \n  dplyr::count(syn, gender)\n\n# A tibble: 4 × 3\n  syn   gender     n\n  &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;\n1 No         0   874\n2 No         1   216\n3 Yes        0    97\n4 Yes        1    24\n\n\nAs you can see, the output from this function is a new summary tibble containing only the unique values in each variable, and a count, in the new “n” variable, of how many times that value (or combination of values) appeared.\nNote that this does not change or add anything to your original dataset! Instead, this function creates a brand-new tibble with the requested information.\n\n\n\n\n\n\n\nSubsetting\nTo work with individual variables, we need to get them out of the dataset. Specifically, for many of the functions we’re about to use, we will need the values stored in those variables as vectors. We can do this in two ways: $ notation, or the function dplyr::pull()4.\nSubsetting with $ is the base-R method, and it takes the general form:\ndataset_name$variable_to_subset\nSubsetting with dplyr::pull() is a {tidyverse} method of accomplishing the same thing. Using the pipe structure we’ve seen previously, the general form is:\ndataset_name |&gt; \n  dplyr::pull(variable_to_subset)\n\n\n\n\n\n\nExercise\n\n\n\nSubset syn_data using $ to get out all the values stored in the scsq_organise variable.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo keep this tutorial legible, I’ve only printed out the first 10 values.\n\nsyn_data$scsq_organise\n\n [1] 2.67 4.00 2.33 3.33 2.00 4.00 2.83 1.83 3.33 3.17\n [ reached getOption(\"max.print\") -- omitted 1201 entries ]\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSubset syn_data using dplyr::pull() to get out all the values stored in the gc_score variable. How would you read this code?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAgain, I’ve only printed out the first 10 values.\n\nsyn_data |&gt; \n  dplyr::pull(gc_score)\n\n [1]   NA   NA   NA   NA 1.40 1.34 1.30 1.19 1.03 1.02\n [ reached getOption(\"max.print\") -- omitted 1201 entries ]\n\n\nYour exact translation may vary, but one option is:\n\nTake the syn_data dataset, and then pull out all the values in the gc_score variable.\n\n\n\n\n\n\nIf you’re wondering when to use $ and when to use dplyr::pull(), the answer depends on what you want to do! We’ll see some examples of both in just a moment.\n\n\nDescriptives\nNext up, we can start working with these values in the dataset. The base-R {stats} package contains a wide variety of very sensibly-named functions that calculate common descriptive statistics. These include:\n\nmean() and median() (there is a function mode(), but it doesn’t do what we’d like it to here!)\nmin() for minimum value, max() for maximum value\nrange() for both minimum and maximum value in a single vector\nsd() for standard deviation\n\nA key feature of all of these functions is that, by default, they return NA if there are any NAs (missing values) present. (This is very sensible behaviour by default, but is frequently not the information we want when we use them.) So, they all have an argument, na.rm =, which determines whether NAs should be removed. By default this argument is set to FALSE (NAs should NOT be removed), but if you want to get the calculation ignoring any NAs, you can set this argument to TRUE instead.\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the mean, standard deviation, and median of the SCSQ global subscale, and the range of the grapheme-colour synaesthesia score.\nTry using each subsetting method at least once.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe’ll start by using $ subsetting for the first bit, and pull() for the second. Not for any principled reason - feel free to try it either way.\n\nmean(syn_data$scsq_global)\n\n[1] 2.939389\n\nsd(syn_data$scsq_global)\n\n[1] 0.6011931\n\nmedian(syn_data$scsq_global)\n\n[1] 2.88\n\n\nThe gc_score variable has a large number of NAs. If we use the range() function without any other changes, we’ll just get NAs back as output. To remove NAs and work only with the non-missing values, we have to include the argument na.rm = TRUE.\n\nsyn_data |&gt; \n  dplyr::pull(gc_score) |&gt; \n  range(na.rm = TRUE)\n\n[1] 0.35 1.40\n\n\n\n\n\n\n\nAs a quick check to get an individual number, this method is quite useful. However, you may have noticed that if we wanted this information for lots of variables, this would be repetitive, laborious, and prone to error. We’ve already seen how to use existing summary functions, but we will also look at creating custom summary tables in a future tutorial.\n\n\nVisualisations\nThe final piece we will look at today will be base-R data visualisations in the {graphics} package. These built-in graphics functions are particularly helpful for quick spot checks during data cleaning and manipulation. For high-quality, fully customisable, professional data visualisations, we will use the {ggplot2} package, covered in depth in the next section of the course.\nTo get the idea, there are a few options for common plots:\n\nhist() for histograms\nboxplot() and barplot()\nplot() for scatterplots\n\nFor more help and examples with base R graphics, try this quick guide.\n\n\n\n\n\n\nExercise\n\n\n\nTry making a histogram and a boxplot, using any of the variables in the syn_data dataset. Try using $ and pull() once each.\nOptionally, if you feel so inclined, use the help documentation to spruce up your plots a bit, such as changing the title and axis labels.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHistogram:\n\nhist(syn_data$scsq_language)\nhist(syn_data$scsq_language,\n     main = \"Histogram of the Language subscale of the SCSQ\",\n     xlab = \"SCSQ Language score\")\n\n\n\n\n\n\n\n\n\n\n\nBoxplot:\n\nsyn_data |&gt; \n  dplyr::pull(gc_score) |&gt; \n  boxplot()\nsyn_data |&gt; \n  dplyr::pull(gc_score) |&gt; \n  boxplot(\n     main = \"Boxplot of grapheme-colour score\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Try making a barplot and a scatterplot.\nFor the barplot, make a visualisation of how many people are synaesthetes or not (regardless of synaesthesia type).\nFor the scatterplot, choose any two SCSQ measures.\nBoth of these require some creative problem-solving using the help documentation and the skills and functions covered in this tutorial.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe following solutions are options - if you found another way to make the same or similar plots, well done!\nBarplots require two sets of values: a categorical one on the horizontal x axis and a continuous one on the vertical y axis. For something like frequency counts, then, we have to first do the counting, then pass those counts onto barplot(). Luckily we already know how to count categorical variables.\nThe help documentation is most helpful in the Examples section, where it shows actual examples of how the function works. There we can see an example of the formula method, y ~ x, which I’ve used below. Since we’re piping in the data to an argument that is not the first, I’ve used the placeholder in the data = _ argument to finish the command.\n\nsyn_data |&gt;\n  dplyr::count(syn) |&gt;\n  barplot(\n    n ~ syn,\n    data = _)\n\n\n\n\n\n\n\n\n\nFor the scatterplot, there are a couple of options. We can either supply x and y separately using $ subsetting, or use the same y ~ x formula we saw for barplots previously.\n\n## Using subsetting\nplot(syn_data$scsq_techspace, syn_data$scsq_imagery)\n## Using a formula\nsyn_data |&gt; \n  plot(scsq_imagery ~ scsq_techspace, data = _)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nWell Done!\n\n\n\nThat’s the end of this tutorial. Very well done on all your hard work!"
  },
  {
    "objectID": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#footnotes",
    "href": "tutorials/psychrlogy/01_fundRmentals/03_datasets.html#footnotes",
    "title": "03: Datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can read this notation as “the filter function from the dplyr package”, or just “dplyr filter”. As for how to pronounce “dplyr”, the official pronunciation is “dee-ply-er”, with “plier” like the tool for which it’s named. I have heard other people say “dipler”. Since code is always a bit tricky to read aloud, just go with whatever sounds good to you.↩︎\n[](https://media.giphy.com/media/12XMGIWtrHBl5e/giphy.gif, fig-alt=‘A gif of Michael from the Office (US) shouting “Oh God no! Please no! NOOOOO”’)↩︎\nYou can call this object anything you like; I use “px” as shorthand for “participant.”↩︎\nThis function always makes me think of one of those arcade claw machines reaching into the dataset to grab the values you want!↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html",
    "title": "05: Filter and Select",
    "section": "",
    "text": "This tutorial covers two important {dplyr} functions: filter() and select(). Easy to confuse, filter() uses logical assertions to return a subset of rows (cases) in a dataset, while select() returns a subset of the columns (variables) in the dataset.\n\n\n\n\n\n\nTip\n\n\n\nTo remember which does which:\n\nfilter() works on rows, which starts with “r”, so it contains the letter “r”.\nselect() works on columns, which starts with “c”, so it contains the letter “c”."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#overview",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#overview",
    "title": "05: Filter and Select",
    "section": "",
    "text": "This tutorial covers two important {dplyr} functions: filter() and select(). Easy to confuse, filter() uses logical assertions to return a subset of rows (cases) in a dataset, while select() returns a subset of the columns (variables) in the dataset.\n\n\n\n\n\n\nTip\n\n\n\nTo remember which does which:\n\nfilter() works on rows, which starts with “r”, so it contains the letter “r”.\nselect() works on columns, which starts with “c”, so it contains the letter “c”."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#setup",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#setup",
    "title": "05: Filter and Select",
    "section": "Setup",
    "text": "Setup\n\nPackages\nWe will be focusing on {dplyr} today, which contains both the filter() and select() functions. You can either load {dplyr} alone, or all of {tidyverse} - it won’t make a difference, but you only need one or the other.\nWe will also make use of the {GGally} package later on for some snazzy visualisations and {correlation} for…well, I’ll give you three guesses!\n\n\n\n\n\n\nExercise\n\n\n\nLoad the necessary packages.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(dplyr)\n## OR\nlibrary(tidyverse)\n\nlibrary(GGally)\nlibrary(correlation)\n\n\n\n\n\n\n\n\nData\nToday we’re going to start working with a dataset that we’re going to get familiar with over the next few weeks. Courtesy of fantastic Sussex colleague (Jenny Terry)[https://www.jennyterry.co.uk/], this dataset contains real data about statistics and maths anxiety.\n\n\n\n\n\n\nExercise\n\n\n\nRead in the dataset and save it in a new object, anx_data.\nOn the Cloud, you can read in this dataset from the data folder using here::here().\nElsewhere, you can download or read in the dataset from this URL:\n\nhttps://raw.githubusercontent.com/drmankin/practicum/master/data/anx_data.csv\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRead in from file:\n\nanx_data &lt;- readr::read_csv(here::here(\"data/anx_data.csv\"))\n\nRead in from URL:\n\nanx_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/drmankin/practicum/master/data/anx_data.csv\")\n\n\n\n\n\n\n\nCodebook\nThere’s quite a bit in this dataset, so you will need to refer to the codebook below for a description of all the variables.\nThis study explored the difference between maths and statistics anxiety, widely assumed to be different constructs. Participants completed the Statistics Anxiety Rating Scale (STARS) and Maths Anxiety Rating Scale - Revised (R-MARS), as well as modified versions, the STARS-M and R-MARS-S. In the modified versions of the scales, references to statistics and maths were swapped; for example, the STARS item “Studying for an examination in a statistics course” became the STARS-M item “Studying for an examination in a maths course”; and the R-MARS item “Walking into a maths class” because the R-MARS-S item “Walking into a statistics class”.\nParticipants also completed the State-Trait Inventory for Cognitive and Somatic Anxiety (STICSA). They completed the state anxiety items twice: once before, and once after, answering a set of five MCQ questions. These MCQ questions were either about maths, or about statistics; each participant only saw one of the two MCQ conditions.\n\n\n\n\n\n\nImportant\n\n\n\nFor learning purposes, I’ve randomly generated some additional variables to add to the dataset containing info on distribution channel, consent, gender, and age. Especially for the consent variable, don’t worry: all the participants in this dataset did consent to the original study. I’ve simulated and added this variable in later to practice removing participants.\n\n\n\nreadr::read_csv(here::here(\"data/anx_codebook.csv\")) %&gt;%\n  kableExtra::kbl(\n    col.names = stringr::str_to_title(names(.)),\n    html = TRUE\n  ) |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nid\nCategorical\nUnique ID code\n\n\ndistribution\nCategorical\nChannel through which the study was completed, either \"preview\" or \"anonymous\" (the latter representing \"real\" data). Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nconsent\nCategorical\nWhether the participant read and consented to participate (\"Yes\") or not (\"No\"). Note that this variable has been randomly generated and does NOT reflect genuine responses; all participants in this dataset did originally consent to participate.\n\n\ngender\nCategorical\nGender identity, one of \"female\", \"male\", \"non-binary\", or \"other/pnts\". \"pnts\" is an abbreviation for \"Prefer not to say\". Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nage\nNumeric\nAge in years. Note that this variable has been randomly generated and does NOT reflect genuine responses.\n\n\nmcq\nCategorical\nIndependent variable for MCQ question condition, whether the participant saw MCQ questions about mathematics (\"maths\") or statistics (\"stats\").\n\n\nstars_[sub][number]\nNumeric\nItem on the Statistics Anxiety Rating Scale. There are three subscales, denoted with [sub] in the name:&lt;br&gt;- [test]: Test anxiety&lt;br&gt;- [help]: Asking for Help&lt;br&gt;- [int]: Interpretation Anxiety.&lt;br&gt;[num] corresponds to the item number. Responses given on a Likert scale from 1 (no anxiety) to 5 (a great deal of anxiety), so higher scores reflect higher levels of anxiety.\n\n\nstars_m_[sub][number]\nNumeric\nItem on the Statistics Anxiety Rating Scale - Maths, a modified version of the STARS with all references to statistics replaced with maths. There are three subscales, denoted with [sub] in the name:&lt;br&gt;- [test]: Test anxiety&lt;br&gt;- [help]: Asking for Help&lt;br&gt;- [int]: Interpretation Anxiety.&lt;br&gt;[num] corresponds to the item number. Responses given on a Likert scale from 1 (no anxiety) to 5 (a great deal of anxiety), so higher scores reflect higher levels of anxiety.\n\n\nrmars_[sub][number]\nNumeric\nItem on the Revised Maths Anxiety Rating Scale. There are three subscales, denoted with [sub] in the name:&lt;br&gt;- [test]: Test anxiety&lt;br&gt;- [num]: Numerical Task Anxiety&lt;br&gt;- [course]: Course anxiety.&lt;br&gt;[num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 5 (very much), so higher scores reflect higher levels of anxiety.\n\n\nrmars_s_[sub][number]\nNumeric\nItem on the Revised Maths Anxiety Rating Scale - Statistics, a modified version of the MARS with all references to maths replaced with statistics. There are three subscales, denoted with [sub] in the name:&lt;br&gt;- [test]: Test anxiety&lt;br&gt;- [num]: Numerical Task Anxiety&lt;br&gt;- [course]: Course anxiety.&lt;br&gt;[num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 5 (very much), so higher scores reflect higher levels of anxiety.\n\n\nsticsa_trait_[number]\nNumeric\nItem on the State-Trait Inventory for Cognitive and Somatic Anxiety, Trait subscale. [num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 4 (very much so), so higher scores reflect higher levels of anxiety.\n\n\nsticsa_[time]_state_[number]\nNumeric\nItem on the State-Trait Inventory for Cognitive and Somatic Anxiety, State subscale. [time] denotes one of two times of administration: before completing the MCQ task (\"pre\"), or after (\"post\"). [num] corresponds to the item number. Responses given on a Likert scale from 1 (not at all) to 4 (very much so), so higher scores reflect higher levels of anxiety.\n\n\nmcq_stats_[num]\nCategorical\nCorrect (1) or incorrect (0) response to MCQ questions about statistics, covering mean ([number] = 1), standard deviation (2), confidence intervals (3), beta coefficient (4), and standard error (5).\n\n\nmcq_maths_[num]\nCategorical\nCorrect (1) or incorrect (0) response to MCQ questions about maths, covering mean ([number] = 1), standard deviation (2), confidence intervals (3), beta coefficient (4), and standard error (5)."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#filter",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#filter",
    "title": "05: Filter and Select",
    "section": "Filter",
    "text": "Filter\nThe filter() function’s primary job is to easily and transparently subset the rows within a dataset - in particular, a tibble. filter() takes one or more logical assertions and returns only the rows for which the assertion is TRUE. Columns are not affected by filter(), only rows.\n\nGeneral Format\n\n1dataset_name |&gt;\n  dplyr::filter\n3    logical_assertion\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n3\n\nA logical assertion about the variable(s) in dataset_name that returns logical (TRUE or FALSE) values.\n\n\n\n\n\n\nFiltering with Assertions\nThe logical_assertion in the general format above is just like the assertions we saw in the first tutorial. The rows where the assertion returns TRUE will be included in the output; those that return FALSE will not. Inside the filter() command, use the names of the variable in the piped-in dataset to create the logical assertions.\nAs a first example, let’s use some of our familiar operators from the first tutorial. To retain only people who completed the maths MCQs, we can run:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    mcq == \"maths\"\n    )\n\n\n1\n\nTake the dataset anx_data, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the mcq variable is exactly and only equal to \"maths\".\n\n\n\n\nSo, the tibble we get as output contains cases that have the value \"maths\", and NOT \"stats\", nor any NAs (because NA does not equal \"maths\"!).\n\n\n\n\n\n\nError Watch: Detected a named input\n\n\n\n\n\nRemember that for exact matches like this, we must use double-equals == and not single-equals =. If you use single equals, you’re not alone - this is such a common thing that the (incredibly friendly and helpful) error message tells you what to do to fix it!\n\nanx_data |&gt; \n  dplyr::filter(mcq = \"maths\")\n\nError in `dplyr::filter()`:\n! We detected a named input.\nℹ This usually means that you've used `=` instead of `==`.\nℹ Did you mean `mcq == \"maths\"`?\n\n\n\n\n\nNaturally, we can also filter on numeric values. If we wanted to keep only participants younger than 40 years old, we can filter as follows:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    age &lt; 40\n    )\n\n\n1\n\nTake the dataset anx_data, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the age variable is less than 40.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProduce a subset of anx_data that doesn’t contain any male participants.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::filter(\n    gender != \"male\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProduce a subset of anx_data that contains only participants as old as the median age, or younger.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere we can take advantage of the fact that we can use variable names as objects inside {dplyr} functions like filter()1. Then we write a logical assertion just like we have done in previous tutorials.\n\nanx_data |&gt; \n  dplyr::filter(\n    age &lt;= median(age, na.rm = TRUE)\n  )\n\nIf you mysteriously got an empty tibble, you may have missed out the na.rm = TRUE argument to median().\n\n\n\n\n\nAs a final example, let’s consider a situation where we want to retain only participants that gave a gender identity of either “male” or “female”.2\nTo do this, we need a new operator: %in%, which God knows I just pronounce as “in” (try saying “percent-in-percent” three times fast!). This looks for any matches with any of the elements that come after it:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    gender %in% c(\"female\", \"male\")\n    )\n\n\n1\n\nTake the dataset anx_dat, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the gender variable matches any of the values “female” or “male”.\n\n\n\n\n\n\n\n\n\n\nWhy not ==?\n\n\n\nWhat follows here is a rabbit hole that gets into some gritty detail. If you’re happy to take my word for it that you absolutely, definitely needed %in% and not == in the previous exercise, you can skip the explanation below. If you’re keen to understand all the nuance, click to expand and read on!\n\n\n\n\n\n\nThe Danger of == vs %in%\n\n\n\n\n\nFor this matching task, you might have thought we’d use gender == c(\"female\", \"male\"), which runs successfully and sure looks okay. So why isn’t this right?\n\n## DO NOT DO THIS\nanx_data |&gt; \n  ## THIS DOES NOT DO WHAT WE WANT!!\n  dplyr::filter(gender == c(\"female\", \"male\"))\n\n\n\n  \n\n\n## DANGER WILL ROBINSON\n\nAt a glance it looks like this produces the same output as the solution above - gender now contains only male or female participants. As you might have gathered from the all-caps comments above - intended to prevent you from accidentally using this code in the future for tasks like this - this is NOT what this code does.\nTo demonstrate what it does do, I need the dplyr::mutate() function from the next tutorial to create some new variables. The first new variable, double_equals, contains TRUEs and FALSEs for each case using the assertion with ==. The second is exactly the same, but reverses the order of the genders - something that should NOT make a difference to the matching! (We want either female OR male participants, regardless of which we happen to write first.) The third, in_op, contains the same again but this time with %in%. The final arrange() line sorts the dataset by gender to make the output easier to read.\n\nanx_data |&gt; \n  dplyr::mutate(\n    double_equals = (gender == c(\"female\", \"male\")),\n    double_equals_rev = (gender == c(\"male\", \"female\")),\n    in_op = (gender %in% c(\"female\", \"male\")),\n    .keep = \"used\"\n  ) |&gt; \n  dplyr::arrange(gender)\n\n\n\n  \n\n\n\nNotice anything wild?\nFor participants with the same value in gender, the assertions with == both flip between TRUE and FALSE, but in the reverse pattern to each other. The assertion with %in% correctly labels them all as TRUE. WTF?\nWhat’s happening is that because the vector c(\"female\", \"male\") contains two elements, the assertion with == matches the first case to the first element - female - and returns TRUE. Then it matches the second case to the second element - male - and this time returns FALSE. Then because there are more cases, it repeats: the next (third) case matches female and returns TRUE, the next male and FALSE, and so forth. The == assertion with the gender categories reversed does the same, but starts with male first and female second. Only %in% actually does what we wanted, which was to return TRUE for any case that matches female OR male.\nThis is a good example of what I think of as “dangerous” code. I don’t mean “reckless” or “irresponsible” - R is just doing exactly what I asked it to do, and it’s not the job of the language or package creators to make sure my code is right. I mean dangerous because it runs as expected, produces (what looks like) the right output, and even with some brief checking, would appear to contain the right cases - but would quietly result in a large chunk of the data being wrongly discarded. If you didn’t know about %in%, or how to carefully double-check your work, you could easily carry on from here and think no more about it.\nSo, how can we avoid a problem like this? Think of any coding task - especially new ones, where you’re not completely familiar with the code or functions you’re working with - as a three-step process3.\n\nAnticipate. Form a clear picture of the task you are trying to achieve with your code. What do you expect the output of the code to look like when it runs successfully?\nExecute. Develop and run the code to perform the task.\nConfirm. Compare the output to your expectations, and perform tests to confirm that what you think the code has done, is in fact what it has done.\n\nSo, what might the Confirm step look like for a situation like this?\nOne option is the code I created above, with new columns for the different assertion options - but this might be something you’d only think to do if you already knew about %in% or suspected there was a problem. A more routine check might look like:\n\nI expect that when my filtering is accomplished, my dataset will contain all and only the participants who reported a gender identy of female or male, and no others. I will also have the same number of cases as the original dataset, less the number of other gender categories.\n\nFirst, I’ll create a new dataset using the filtered data.\n\n## SERIOUSLY THIS IS BAD\nanx_data_bd &lt;- anx_data |&gt; \n## DON'T USE THIS CODE FOR MATCHING\n  dplyr::filter(gender == c(\"female\", \"male\"))\n## STOP OH GOD PLEASE JUST DON'T\n\nCheck 1: Filtered data contains only male and female participants.\n\nanx_data_bd |&gt; \n  dplyr::count(gender)\n\n\n\n  \n\n\n\nOnly female and male participants! Tick ✅\nAt this point, though, I might become suspicious. The original dataset contained 465 cases - we’ve lost more than half! Can that be right?? Better check the numbers.\n\n## Get the numbers from the original dataset\nanx_data |&gt; \n  dplyr::count(gender)\n\n\n\n  \n\n\n\nUh oh. Already we can see that something’s wrong with the numbers. But instead of relying on visual checks, let’s let R tell us.\n\n## Calculate how many cases we expect if the filtering had gone right\nexpected_n &lt;- anx_data |&gt; \n  dplyr::count(gender) |&gt; \n  ## This isn't the best way to filter\n  dplyr::filter(gender != \"non-binary\") |&gt;\n  ## The next section on multiple assertions has a much better method!\n  dplyr::filter(gender != \"other/pnts\") |&gt; \n  dplyr::pull(n) |&gt; \n  sum()\n\n## Ask R whether the expected number of rows is equal to the actual number of rows in the filtered data\nexpected_n == nrow(anx_data_bd)\n\n[1] FALSE\n\n\nNow we know for sure there’s a problem and can investigate what happened more thoroughly.\nAs a final stop on this incredibly lengthy detour (are you still here? 👋), you might wonder whether the check above would give me the wrong answer, because I used two filter()s in a row, and the whole point of this goose chase is how to accomplish that exact filtering task. First, this is NOT the way I would do this (as the comments suggest), but I’m really trying to stick to ONLY what we’ve already covered wherever possible. But let’s say I’d tried to do this with the bad == filtering that caused all this faff in the first place.\nFor this particular case there are four values in gender. If I try gender == c(\"female\", \"male\") here, this DOES actually work fine - because the categories are in the right order and are a multiple of the length of the dataset 🤦 But at least the numbers still wouldn’t match, which would tell me that something went wrong with filtering the whole dataset.\n\nanx_data |&gt; \n  dplyr::count(gender) |&gt; \n  dplyr::filter(gender == c(\"female\", \"male\"))\n\n\n\n  \n\n\n\nIf I happened to have had the genders the other way round, I would have got an empty tibble, and hopefully that also would have clued me in that there was a problem with the original filtering.\n\nanx_data |&gt; \n  dplyr::count(gender) |&gt; \n  dplyr::filter(gender == c(\"male\", \"female\"))\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMultiple Assertions\nLogical assertions can also be combined to specify exactly the cases you want to retain. The two most important operators are:\n\n& (AND): Only cases that return TRUE for all assertions will be retained.\n| (OR): Any cases that return TRUE for at least one assertion will be retained.\n\n\n\n\n\n\n\nMore on AND and OR\n\n\n\n\n\nLet’s look at a couple minimal examples to get the hang of these two symbols. For each of these, you can think of the single response R gives as the answer to the questions, “Are ALL of these assertions true?” for AND, and “Is AT LEAST ONE of these assertions true?” for OR.\nFirst, let’s start with a few straightforward logical assertions:\n\n\"apple\" == \"apple\"\n\n[1] TRUE\n\n23 &gt; 12\n\n[1] TRUE\n\n42 == \"the answer\"\n\n[1] FALSE\n\n10 &gt; 50\n\n[1] FALSE\n\n\nNext, let’s look at how they combine.\nTwo true statements, combined with &, return TRUE, because it is true that all of these assertions are true.\n\n\"apple\" == \"apple\" & 23 &gt; 12\n\n[1] TRUE\n\n\nTwo true statements, combined with |, also return TRUE, because it true that at least one of these assertions is true.\n\n\"apple\" == \"apple\" | 23 &gt; 12\n\n[1] TRUE\n\n\nTwo false statements, combined with &, return FALSE, because it is NOT true that all of them are true.\n\n42 == \"the answer\" & 10 &gt; 50\n\n[1] FALSE\n\n\nTwo false statements, combined with |, return FALSE, because it is NOT true that at least one of them is true.\n\n42 == \"the answer\" | 10 &gt; 50\n\n[1] FALSE\n\n\nOne true and one false statement, combined with &, return FALSE, because it is NOT true that all of them are true.\n\n23 &gt; 12 & 42 == \"the answer\"\n\n[1] FALSE\n\n\nOne true and one false statement, combined with |, return TRUE, because it is true that at least one of them is true.\n\n23 &gt; 12 | 42 == \"the answer\"\n\n[1] TRUE\n\n\n\n\n\nTo see how this works, let’s filter anx_data to keep only cases that saw the stats MCQs, OR that scored 3 or higher on the first STARS test subscale item.\nThis requires two separate statements, combined with | “OR”:\n\n1anx_data |&gt;\n2  dplyr::filter(\n3    mcq == \"stats\" |\n4     stars_test1 &gt;= 3\n    ) \n\n\n1\n\nTake the dataset anx_data, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the mcq variable is only and exactly equal to \"stats\", OR\n\n4\n\nThe value in stars_test1 is greater than or equal to 3.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFilter anx_data to keep only cases where the value of rmars_s_test2 is between 2 and 4.\nHint: You can use two separate assertions to do this, or check out dplyr::between().\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the first solution, we must use & “AND” to ensure that both these conditions are met simultaneously.\nFor the second solution, the dplyr::between() function does the same operation, without having to worry about getting AND vs OR right.\n\nanx_data |&gt; \n  dplyr::filter(\n    rmars_s_test2 &gt;= 2 & rmars_s_test2 &lt;= 4\n  )\n\nanx_data |&gt; \n  dplyr::filter(\n    dplyr::between(rmars_s_test2, 2, 4)\n  )\n\n\n\n\n\n\n\n\nData Cleaning\nFiltering is absolutely invaluable in the process of data cleaning. In order to practice this process, I’ve introduced some messy values into the data, so let’s have a look at a method of cleaning up the dataset and documenting our changes as we go.\n\nPre-Exclusions\nFor data collected on platforms like Qualtrics, you can frequently test out your study via a preview mode. Responses completed via preview are still recorded in Qualtrics, but labeled as such in a variable typically called “DistributionChannel” or similar. In this dataset, we have a similar variable, distribution, that labels whether the data was recorded in a preview (\"preview\") or from real participants (\"anonymous\").\nYour method may vary, but I wouldn’t bother to document these cases as “exclusions” because they aren’t real data. I would just drop them from the dataset - but of course make sure to record the code that does so!\n\n\n\n\n\n\nExercise\n\n\n\nRemove any preview runs from the dataset, keeping only real data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data &lt;- anx_data |&gt; \n  dplyr::filter(distribution == \"anonymous\")\n\n\n\n\n\n\n\n\nRecording Exclusions\nAs a part of complete and transparent reporting, we will want to report all of the reasons we excluded cases from our dataset, along with the number excluded. We can build this counting process into our workflow so that at the end, we have a record of each exclusion along with initial and final numbers.\n\n\n\n\n\n\nExercise\n\n\n\nFollow along with the following data cleaning steps, trying them out in a code chunk for yourself as you go. You’ll need them at the end!\n\n\nFor each check below, our recording process will have two steps:\n\nProduce a dataset of the cases you will exclude, and count the number of rows (cases).\nRemove the cases and overwrite the old dataset with the new one.\n\nIn my process, I’m going to keep anx_data as the original, “raw” version of the dataset. So, I’ll create a copy in a new dataset object to use while “processing” that I will update as I go.\n\nanx_data_proc &lt;- anx_data\n\nTo begin, we will count the initial number of cases before any exclusions.\n\nn_initial &lt;- nrow(anx_data_proc)\n\n(Remember that we can use nrow() because there is only one participant per row. If we had long-form data with observations from the same participant across multiple rows, we would have to do something a bit different!)\n\n\nConsent\nFor many datasets, you would likely have a variable with responses from your participants about informed consent. How you filter this depends on what that variable contains, of course. However, we’ve already seen examples of this kind of operation earlier in this tutorial, and it would probably look something like consent == \"Yes\". As we saw before, this would discard cases that answered “No” (along with any other value not exactly matching “Yes”) and cases with NAs from people who didn’t answer.\n\nn_no_consent &lt;- anx_data_proc |&gt; \n  dplyr::filter(consent != \"Yes\") |&gt; \n  nrow()\n\nn_no_consent\n\n[1] 15\n\n\nThen, we remove all participants who did not actively consent and assign the resulting dataset to the same name, overwriting the previous version.\n\nanx_data_proc &lt;- anx_data_proc |&gt; \n  dplyr::filter(consent == \"Yes\")\n\n\n\nAge\nFor low-risk ethics applications, you may want to exclude people who reported an age below the age of informed consent (typically 18). This may look like age &gt;= 18 or similar in your dataset. However, it’s also important to check for errors or improbable ages, or to remove any participants that are too old if your study has an upper age limit. In this case, my hypothetical study didn’t have an upper age limit, but I’ll designate any ages as 100 or above as unlikely to be genuine responses\nSince these are removed for two different reasons, I’ll save them as two separate objects.\n\n## Store the number to be removed\nn_too_young &lt;- anx_data_proc |&gt; \n  dplyr::filter(age &lt; 18) |&gt; \n  nrow()\nn_too_young\n\n[1] 22\n\nn_too_old &lt;- anx_data_proc |&gt; \n  dplyr::filter(age &gt;= 100) |&gt; \n  nrow()\nn_too_old\n\n[1] 5\n\n## Remove them\nanx_data_proc &lt;- anx_data_proc |&gt; \n  dplyr::filter(\n    dplyr::between(age, 18, 99)\n  )\n\n\n\nMissing Values\nFinally (for now), just about any study will have to decide how to deal with missing values. The possibilities for your own work are too complex for me to have a guess at here, so for now we’ll only look at how to identify and remove missing values.\n\nSingle Variable\nLet’s look at a single variable to begin with - for example, sticsa_trait_3. We can confirm that this variable has a/some NAs to consider by counting the unique values:\n\nanx_data |&gt; \n  dplyr::count(sticsa_trait_3)\n\n\n\n  \n\n\n\nThe first thing you might think to try is to filter on sticsa_trait_3 == NA, but weirdly enough this doesn’t work. Instead, we need to use a function from a family we met all the way back in Tutorial 01/02, namely is.na().\nYou can think of is.na() as a question about whatever is in its brackets: “Is (this) NA?” If the value IS an NA, R will return TRUE; if it’s anything else at all, R will return FALSE. Let’s see this in action:\n\n1anx_data_proc |&gt;\n2  dplyr::filter(\n3    is.na(sticsa_trait_3)\n  )\n\n\n1\n\nTake the dataset anx_data_proc, and then\n\n2\n\nFilter it keeping only the cases where the following assertion is true:\n\n3\n\nThe value in the sticsa_trait_3 variable IS missing (is NA).\n\n\n\n\nThese are the cases we want to remove, so we count how many there are and assign that number to a useful object name, as we did before.\n\nn_sticsa_t3_missing &lt;- anx_data_proc |&gt;\n  dplyr::filter(\n    is.na(sticsa_trait_3)\n  ) |&gt; \n  nrow()\n\nn_sticsa_t3_missing\n\n[1] 3\n\n\nNext, we need to actually exclude these cases. This time, we want to retain the inverse of the previous filtering requirement: that is, we only want to keep the cases that are NOT missing a value in sex, the opposite of what we got from is.na(sticsa_trait_3). You may recognise “the inverse” or “not-x” as something we’ve seen before with !=, “not-equals”. For anything that returns TRUE and FALSE, you can get the inverse by putting an ! before it. (Try running !TRUE, for example!)\nSo, to create my clean anx_data_final dataset, I can use the assertion !is.na(sticsa_trait_3) to keep only the participants who answered this question - who do NOT have a missing value.\nFinally, I can store the actual number of usable cases, according to my cleaning requirements, in a final object to use when reporting.\n\nanx_data_final &lt;- anx_data_proc |&gt;\n  dplyr::filter(\n    !is.na(sticsa_trait_3)\n  )\n\nn_final &lt;- nrow(anx_data_final)\nn_final\n\n[1] 390\n\n\n\n\nAll Variables\nRemoving NAs is a tricky process, but if you’re sure that you want to drop all cases with missing values in your dataset, there are few helper functions to make this easy.\nFor this, we’re going to leave filter() for a moment at look at a different function, tidyr::drop_na(). This function takes a tibble as input, and returns the same tibble as output, but with any rows that had missing values removed.\n\n\n\n\n\n\nWarning\n\n\n\nThis is a pretty major step and should be used with caution! If we didn’t check our data carefully, we could easily end up dropping a bunch of cases we didn’t want to get rid of.\nFor example, if we apply it uncautiously here:\n\nanx_data_proc |&gt; \n  tidyr::drop_na()\n\nWell, there goes all our data!\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Why has every single row in the dataset been dropped? Using any method you like, investigate what’s happened.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is something we could work out without any R whatsoever, just using the codebook and a bit of View mode to confirm. The Codebook tells us that participants were in one of two independent conditions: \"maths\" or \"stats\". Because of the wide format of the data, there are mcq_maths questions that are always NA for people in the statistics-MCQ condition, and vice versa for the mcq_stats questions and people in the maths-MCQ condition. So, every single participant - even those who answered every question - has at least some missing values, and dropping NAs without checking just bins the whole dataset.\nIf I wanted to check this with R, I’d be hard pressed to do it with only what we’ve covered so far. Using the some extra challenge functions from the next tutorial, though, I’d do this:\n\nanx_data |&gt; \n  dplyr::mutate(\n    ## Create a new variable containing the number of missing values in each row\n    number_nas = rowSums(is.na(pick(everything())))\n  ) |&gt; \n  ## Count how many missing values there are\n  dplyr::count(number_nas)\n\n\n\n  \n\n\n\nSo, there’s at least 5 NAs in every single row, and when we call tidyr::drop_na(), every single row is dropped.\n\n\n\n\n\n\n\n\n\n\nReporting\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Using the objects counting intial, final, and excluded cases and what we covered last time about inline code, write a brief journal-style description of your exclusion process.\nWhat is the benefit of taking the extra effort to store these counts in objects? Under what circumstances might this be (particularly) useful?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can write whatever you like, but here’s an example using inline code.\n\nThe initial sample consisted of `r n_initial` cases. We removed `r n_no_consent` cases that did not consent, `r n_too_young` cases that reported an age below the ethical age of consent, and `r n_too_old` cases that reported improbable ages (100 years old or older). This left us with a final sample of `r n_final` cases.\n\nWhen you render your document, this should come out as:\n\nThe initial sample consisted of 453 cases. We removed 15 cases that did not consent, 22 cases that reported an age below the ethical age of consent, and 5 cases that reported improbable ages (100 years old or older). This left us with a final sample of 390 cases.\n\nThere’s a huge advantage of this, namely ease of change. Imagine you had researchers from labs all over the world join the study and add a huge amount of new data to a massive collaborative dataset. In order to update all your numbers, all you have to do is update your initial anx_data dataset with the new cases, and then re-run all your code as is. Because these objects count whatever is in the data, they will automatically contain and record the correct numbers for the data you put into them4.\nThere are other advantages too - like confidence that you, a human person who may occasionally make errors (sorry, no offence meant!), won’t misread, mistype, or otherwise mistake the numbers, because at no point do you actually type a particular number yourself.\nNifty, eh?"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#select",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#select",
    "title": "05: Filter and Select",
    "section": "Select",
    "text": "Select\nThe select() function is probably the most straightforward of the core {dplyr} functions. Its primary job is to easily and transparently subset the columns within a dataset - in particular, a tibble. Rows are not affected by select(), only columns.\n\nGeneral Format\nTo subset a tibble, use the general format:\n\n1dataset_name |&gt;\n2  dplyr::select(\n3    variable_to_include,\n4    -variable_to_exclude,\n5    keep_this_one:through_this_one,\n6    new_name = variable_to_rename,\n7    variable_number\n  )\n\n\n1\n\nTake the dataset dataset_name, and then\n\n2\n\nSelect the following variables:\n\n3\n\nThe name of a variable to be included in the output. Multiple variables can be selected separated by commas.\n\n4\n\nThe name of a variable to be excluded from the output. Use either an exclamation mark (!) or a minus sign (-) in front of each variable to exclude. Multiple variables can be dropped, separated by commas with a ! (or -) before each.\n\n5\n\nA range of variables to include in the output. All the variables between and including the two named will be selected (or dropped, with !(drop_this_one:through_this_one)).\n\n6\n\nInclude variable_to_rename in the output, but call it new_name.\n\n7\n\nInclude a variable in the output by where it appears in the dataset, numbered left to right. For example, “2” will select the second column in the original dataset.\n\n\n\n\nColumns will appear in the output in the order they are selected in select(), so this function can also be used to reorder columns.\n\n\nSelecting Directly\nThe best way to get the hang of this will be to give it a go, so let’s dive on in!\n\n\n\n\n\n\nExercise\n\n\n\nCreate a subset of anx_data that contains the following variables:\n\nThe participant’s age\nThe first variable in the original dataset\nAll of the STARS variables\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    age, 1,\n    stars_test1:stars_help4\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a subset of anx_data that contains the following variables:\n\nAll of the original variables but NOT distribution\nmcq renamed condition\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    -distribution,\n    condition = mcq\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\nThat’s really all there is to it!\n…Or is it?5\n\n\nUsing {tidyselect}\nThe real power in select(), and in many other {tidyverse} functions, is in a system of helper functions and notations collectively called &lt;tidyselect&gt;. The overall goal of “&lt;tidyselect&gt; semantics” (as you will see it referred to in help documentation) is to make selecting variables easy, efficient, and clear.\n\n\n\n\n\n\nNew to UGs\n\n\n\nAt UG level at Sussex, students are not taught about &lt;tidyselect&gt; in core modules. However, &lt;tidyselect&gt; is desperately useful and makes complex data wrangling/cleaning a lot faster and more efficient, especially (for instance) for questionnaires with similarly-named subscales, so would make for a great collaborative activity with supervisors!\n\n\nThese helper functions can be combined with the selection methods above in any combination. Some very convenient options include:\n\neverything() for all columns\nstarts_with(), ends_with(), and contains() for selecting columns by shared name elements\nwhere() for selecting with a function, described in the next section\n\n\n\n\n\n\n\nExercise\n\n\n\nOpen the help documentation by running ?dplyr::select in the Console to see examples of how to use all of the &lt;tidyselect&gt; helper functions.\n\n\nRather than list examples of all the helper functions here, it’s best to just try them out for yourself!\n\n\n\n\n\n\nExercise\n\n\n\nSelect the variables in anx_data that have to do with state anxiety.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    contains(\"state\")\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\nSelect all the variables in anx_data that are NOT the R-MARS, R-MARS-S, or STICSA.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanx_data |&gt; \n  dplyr::select(\n    ## contains() also fine (in this case)\n    !starts_with(c(\"rmars\", \"sticsa\"))\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\nCHALLENGE: Select all the stars variables but NOT the stars_m variables.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis one’s a bit tricky because both starts_with() and contains() will return both types of STARS variables, because of the way the variables are named. We’ll have to provide multiple assertions, like we did earlier with filter().\n\nanx_data |&gt; \n  dplyr::select(starts_with(\"stars\") & !contains(\"_m_\"))\n\n\n\n\n\n\n \n\nUsing Functions\nLet’s say we want to generate a summary table of the variables in our dataset. Before we can create our summary in the next tutorial, we may first want to produce a subset of our dataset that only contains numeric variables.\nTo do this, we can use the &lt;tidyselect&gt; helper function where(). This helper function lets us use any function that returns TRUE and FALSE to select columns. Essentially, we don’t have to select columns using name or position - we can use any criteria we want, as long as we have (or can create…!) a function that expresses that criteria.\nEspecially helpful here is the is.*() family of functions in base R. This group of functions all have the same format, where the * is a stand-in for any type of data or object, e.g. is.logical(), is.numeric(), is.factor() etc. (The very useful is.na() that we’ve seen with filter() above is also a member of this family.) These functions work like a question about whatever you put into them - for example, is.numeric() can be read as, “Is (whatever’s in the brackets) numeric data?”\n\n\n\n\n\n\nTip\n\n\n\nYou can quickly find all of the functions in this family by typing is. in a code chunk and pressing Tab.\n\n\nPutting these two together, we could accomplish the task of selecting only numeric variables as follows:\n\nanx_data |&gt; \n  dplyr::select(\n    where(is.numeric)\n  )\n\n\n\n  \n\n\n\nThis command evaluates each column and determines whether they contain numeric data (TRUE) or not (FALSE), and only returns the columns that return TRUE.\n\n\nUsing Custom Functions\n\n\n\n\n\n\nHere There Be Lambdas\n\n\n\nThe following material in this section isn’t covered in the live workshops. It’s included here for reference because it’s extremely useful in real R analysis workflows, but it won’t be essential for any of the workshop tasks.\n\n\nThe function in where() that determines which columns to keep doesn’t have to be an existing named function. Another option is to use a “purrr-style lambda” or formula (a phrase you may see in help documentation) to write our own criteria on the spot.\nFor example, let’s select all of the numeric variables that had a mean of 3 or higher:\n\nanx_data |&gt;\n  dplyr::select(\n    where(~is.numeric(.x) & mean(.x, na.rm = TRUE) &gt;= 3)\n  )\n\nInstead of just the name of a function, as we had before, we now have a formula. This formula has a few key characteristics:\n\nThe ~ (apparently pronounced “twiddle”!) at the beginning, which is a shortcut for the longer function(x) ... notation for creating functions.\nThe .x, which is a placeholder for each of the variables that the function will be applied to.\n\nSo, this command can be read: “Take my tibble and select all the columns where the following is true: the data type is numeric AND the mean value in that column is greater than or equal to 3 (ignoring missing values).”\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Select the variables in anx_data that are character type, or that do NOT contain any missing values.\nHint: You may need to use function(s) that we haven’t covered in the tutorials so far to solve this.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis one is a doozy! Very well done if you worked it out, either using your own solution or one like this, or if you got partway there.\n\nanx_data |&gt; \n  dplyr::select(\n    where(~ is.character(.x) | all(!is.na(.x)))\n  )\n\n\n\n  \n\n\n\nHere’s the process to understand/solve this using this particular solution.\nThe first half of the formula in where() should be okay - you may have noticed the &lt;chr&gt; label in the tibble output and/or guessed that there might be an is.*() function for this purpose.\nThe second half is a bit rough. You may have tried !is.na(.x) and got an error, namely: Predicate must return TRUE or FALSE, not a logical vector. In other words, this has to return a SINGLE logical value, and is.na() will return a vector containing logical values for each individual value in the variable.\nTo solve this - at least the way I’ve done - you need the {base} function all(), which answers the question, “Are all of these values TRUE?” It also has a (non-identical) twin any(), which (as you might guess) answers the question, “Are any of these values TRUE?” So, all() does a similar job as AND, and any() a similar job as OR.\nTo see what I mean, let’s just try it out:\n\nall(TRUE, TRUE)\n\n[1] TRUE\n\nall(TRUE, FALSE)\n\n[1] FALSE\n\nall(FALSE, FALSE)\n\n[1] FALSE\n\nany(TRUE, TRUE)\n\n[1] TRUE\n\nany(TRUE, FALSE)\n\n[1] TRUE\n\nany(FALSE, FALSE)\n\n[1] FALSE\n\n\nLike AND and OR, all() and any() only give different responses when there are a mix of TRUEs and FALSEs. For this task, we only wanted to retain variables where ALL of the values produced by !is.na(x) were TRUE - that is, it was true that ALL of the values in that variable do NOT contain NAs. So, we wanted all(). This returns a single TRUE or FALSE value for each variable that dplyr::select() can use."
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#quick-test-correlation",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#quick-test-correlation",
    "title": "05: Filter and Select",
    "section": "Quick Test: Correlation",
    "text": "Quick Test: Correlation\nWhew! Had enough logical assertions to last a lifetime? Great - let’s cool down with a some snazzy plots and a nice gentle correlation analysis.\nThis bit is meant to be quick, so we’ll only look briefly at what we teach in UG at Sussex. If you want more correlation fun, check out discovr tutorials 07 and 18.\n\nVisualisation\nIn first year, we teach the function GGally::ggscatmat(), which is a quick way to generate a complex plot with lots of useful info, relatively painlessly. However, ggscatmat() (if you’re wondering, that’s G-G-scat-mat, like “scatterplot matrix”) will only work on numeric variables, so we’ll need to select() the ones we want first.\n\n\n\n\n\n\nTip\n\n\n\nFunctions like ggscatmat() output a special kind of plot created with {ggplot2}, another core {tidyverse} package. The lovely thing about ggplot-creating functions like this is that they do a lot of the heavy lifting of plot creation for you - getting a bunch of the complicated structure and setup out of the way - and then you can customise the plot further using {ggplot2}.\nIf you haven’t used {ggplot2} before, we’ll work through it systematically in an upcoming tutorial. If you can’t wait, check out:\n\ndiscovr_05 on data visualisation with {ggplot2}\nR for Data Science chapter 3\nThis list of {ggplot2} resources\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSelect at least three numeric variables from peng_dat and pipe into ggscatmat().\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npeng_dat |&gt; \n  dplyr::select(where(is.numeric), -year) |&gt; \n  GGally::ggscatmat()\n\n\n\n\n\n\n\n\n\nSo, this single function gets a pretty complex plot: a matrix containing all of our variables along the top and side, with density plots on the diagonal, scatterplots on one pairwise intersection, and correlation coefficients on the other.\nThis is the only {GGally} function we teach in UG at Sussex, but to go a bit further, there’s another example that might be useful in the future.\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Use GGally::ggpairs() on the same numeric variables, but split up all the plots by species as well.\nHint: The {palmerpenguins} intro has this code!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe tidyr::drop_na() line isn’t essential, but there will be a lot of howling and gnashing of teeth about missing data and non-finite values if you don’t include it.\n\npeng_dat |&gt; \n  select(species, body_mass_g, ends_with(\"_mm\")) |&gt; \n  tidyr::drop_na() |&gt; \n  GGally::ggpairs(aes(color = species))\n\n\n\n\n\n\nThe example of this in the {palmerpenguins} intro document also changes the default colours, which is something we’ll look at when we take a tour through {ggplot2} ourselves.\n\n\n\n\n\n\n\nTesting Correlation\nIf we wanted to perform and report a detailed correlation analysis on a single pair of variables, the easiest function to use is cor.test(). Like t.test() (which we encountered in Tutorial 01/02), this is a {stats} package that works in a very similar way.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the help documentation for cor.test(), perform a correlation analysis between any two numeric variables of your choice in the peng_dat dataset. The solution will use the formula option, but if you get it to run, you’re doing good!\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun ?cor.test in the Console.\nI chose bill length and flipper length, but whatever you chose is fine!\n\ncor.test(~ bill_length_mm + flipper_length_mm, data = peng_dat)\n\n\n    Pearson's product-moment correlation\n\ndata:  bill_length_mm and flipper_length_mm\nt = 16.034, df = 340, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5912769 0.7126403\nsample estimates:\n      cor \n0.6561813 \n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCHALLENGE: Using what we learned in the last tutorial, report the results of this analysis without typing any of the results out by hand.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npeng_cor &lt;- cor.test(~ bill_length_mm + flipper_length_mm, data = peng_dat)\n\npeng_cor_out &lt;- papaja::apa_print(peng_cor)\n\n\nA Pearson’s pairwise correlation between bill length and flipper length indicated a very strong, significant positive correlation between the two measurements (`r peng_cor_out$full_result`).\n\nWhich will render as:\n\nA Pearson’s pairwise correlation between bill length and flipper length indicated a very strong, significant positive correlation between the two measurements (\\(r = .66\\), 95% CI \\([.59, .71]\\), \\(t(340) = 16.03\\), \\(p &lt; .001\\)).\n\n\n\n\n\n\nIn second year, UGs are also introduced to the (more {tidyverse}-friendly) function correlation::correlation().\nWhy would you use this one vs cor.test()? On the good side, this function scales up to pairwise tests between as many variables as you give it. This means if you want, for instance, multiple pairwise correlations within a dataset, this is the way to go, since it will apply a familywise error rate correction by default (Holm, to be precise).\nOn the other hand, it’s a right pain to type and doesn’t play ball with {papaja}. The family of packages that {correlation} belongs to, collectively called {easystats}, has its own reporting package, appropriately called {report} - so pick your poison I guess 🤷4\n\n\n\n\n\n\nExercise\n\n\n\nSelect at least three variables from peng_dat, including the ones you used with cor.test() above, and get pairwise correlations between all of them with correlation::correlation().\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npeng_dat |&gt; \n  dplyr::select(where(is.numeric), -year) |&gt; \n  correlation::correlation()\n\n# A tibble: 6 × 11\n  Parameter1    Parameter2      r    CI CI_low CI_high      t df_error         p\n  &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 bill_length_… bill_dept… -0.235  0.95 -0.333  -0.132  -4.46      340 1.12e-  5\n2 bill_length_… flipper_l…  0.656  0.95  0.591   0.713  16.0       340 8.72e- 43\n3 bill_length_… body_mass…  0.595  0.95  0.522   0.660  13.7       340 1.52e- 33\n4 bill_depth_mm flipper_l… -0.584  0.95 -0.650  -0.509 -13.3       340 3.70e- 32\n5 bill_depth_mm body_mass… -0.472  0.95 -0.550  -0.385  -9.87      340 4.55e- 20\n6 flipper_leng… body_mass…  0.871  0.95  0.843   0.895  32.7       340 2.62e-106\n# ℹ 2 more variables: Method &lt;chr&gt;, n_Obs &lt;int&gt;\n\n\n\n\n\n\n\n \nWe made it to the end, if you can believe it!"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#footnotes",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#footnotes",
    "title": "05: Filter and Select",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis incredibly useful property is called “data masking”. If you want to know more, run vignette(\"programming\") in the Console.↩︎\nI’m not wild about this example - the experiences of non-binary and other genders are just as important! Unfortunately it’s the only variable in the dataset with the right number of categories.↩︎\nI did try to think of a snazzy acronym here, but all I came up with is AEC (yikes). I’ll keep thinking and try to update this with something better, and I welcome suggestions if you’ve made it this far!↩︎\nI’m sure Jenny would tell you there’s a little more to it than that, especially with 12,570 students from 100 universities in 35 countries, collected in 21 languages! But that’s both the dream and the general idea.↩︎\nHave you seen the size of this tutorial?? Of course it isn’t!↩︎"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/07_dataviz.html",
    "href": "tutorials/psychrlogy/02_essentials/07_dataviz.html",
    "title": "07: Visualisations",
    "section": "",
    "text": "Under Construction\n\n\n\nThis tutorial is still under construction. Check back another time!"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/09_qnaires.html",
    "href": "tutorials/psychrlogy/03_improvRs/09_qnaires.html",
    "title": "09: Questionnaires",
    "section": "",
    "text": "Under Construction\n\n\n\nThis tutorial is still under construction. Check back another time!"
  },
  {
    "objectID": "tutorials/psychrlogy/03_improvRs/11_fx.html",
    "href": "tutorials/psychrlogy/03_improvRs/11_fx.html",
    "title": "11: Functions",
    "section": "",
    "text": "Under Construction\n\n\n\nThis tutorial is still under construction. Check back another time!"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html",
    "href": "workbooks/03_datasets_workbook.html",
    "title": "03: Datasets",
    "section": "",
    "text": "Use the following code chunks to open the accompanying tutorial.\n\n\n\nrstudioapi::viewer('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/03_datasets')\n\nError: RStudio not running\n\n\n\n\n\n\nutils::browseURL('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/03_datasets')"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#open-the-tutorial",
    "href": "workbooks/03_datasets_workbook.html#open-the-tutorial",
    "title": "03: Datasets",
    "section": "",
    "text": "Use the following code chunks to open the accompanying tutorial.\n\n\n\nrstudioapi::viewer('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/03_datasets')\n\nError: RStudio not running\n\n\n\n\n\n\nutils::browseURL('https://r-training.netlify.app/tutorials/psychrlogy/01_fundRmentals/03_datasets')"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#overview",
    "href": "workbooks/03_datasets_workbook.html#overview",
    "title": "03: Datasets",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#setup",
    "href": "workbooks/03_datasets_workbook.html#setup",
    "title": "03: Datasets",
    "section": "Setup",
    "text": "Setup\n\nProjects\n\n\nDocuments\n\nQuarto documents\n\n\nScripts\n\n\n\nInstalling and Loading Packages\nLoad the {tidyverse} package in your Quarto document."
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#reading-in",
    "href": "workbooks/03_datasets_workbook.html#reading-in",
    "title": "03: Datasets",
    "section": "Reading In",
    "text": "Reading In\n\nReading from File\nRun the here::here() function to see what it does.\nUse here::here() to generate a file path to the syn_data data file.\nThen, use readr::read_csv() to read in the syn_data.csv file and store the result in an object called syn_data.\n\nReading from URL\nRead the CSV file hosted at https://raw.githubusercontent.com/drmankin/practicum/master/data/syn_data.csv and save it to the object name syn_data."
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#codebook",
    "href": "workbooks/03_datasets_workbook.html#codebook",
    "title": "03: Datasets",
    "section": "Codebook",
    "text": "Codebook"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#viewing",
    "href": "workbooks/03_datasets_workbook.html#viewing",
    "title": "03: Datasets",
    "section": "Viewing",
    "text": "Viewing\n\nCall the Object\nCall the syn_data object to see what it contains.\n\n\nA Glimpse of the Data\nUse dplyr::glimpse() to get a glimpse of your dataset.\n\n\nView Mode\nOpen the syn_data dataset using the View() function in the Console.\nUsing only View mode, figure out the following:\n\nWhat is the range of the variable gc_score?\nHow can you arrange the dataset by score in scsq_imagery?\nHow many participants had “Yes” in the variable syn_graph_col?\nWhich gender category had more participants?\nOf the participants who said “Yes” to syn_seq_space, what was the highest SCSQ technical-spatial score?"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#overall-summaries",
    "href": "workbooks/03_datasets_workbook.html#overall-summaries",
    "title": "03: Datasets",
    "section": "Overall Summaries",
    "text": "Overall Summaries\n\nBasic Summary\nPrint out a summary of syn_data using the summary() function.\n\n\nOther Summaries\nPrint out a summary of syn_data using the datawizard::describe_distribution() function.\nCHALLENGE: There are some variables missing from this output. What are they? Why aren’t they included?"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#the-pipe",
    "href": "workbooks/03_datasets_workbook.html#the-pipe",
    "title": "03: Datasets",
    "section": "The Pipe",
    "text": "The Pipe"
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#describing-datasets",
    "href": "workbooks/03_datasets_workbook.html#describing-datasets",
    "title": "03: Datasets",
    "section": "Describing Datasets",
    "text": "Describing Datasets\nUsing the native pipe, print out the number of columns and the names of those columns in the syn_data dataset.\nHint: This will be two separate commands!\nUsing the native pipe, save the number of participants in the syn_data dataset in a new object of your choice."
  },
  {
    "objectID": "workbooks/03_datasets_workbook.html#describing-variables",
    "href": "workbooks/03_datasets_workbook.html#describing-variables",
    "title": "03: Datasets",
    "section": "Describing Variables",
    "text": "Describing Variables\n\nCounting\nUsing the syn_data dataset, produce a tibble of counts of how many participants had any kind of synaesthesia. Then, produce a second tibble, adding in gender as well.\nHint: Use the codebook to find the variables to use.\n\n\nSubsetting\nSubset syn_data using $ to get out all the values stored in the scsq_organise variable.\nSubset syn_data using dplyr::pull() to get out all the values stored in the gc_score variable. How would you read this code?\n\n\nDescriptives\nCalculate the mean, standard deviation, and median of the SCSQ global subscale, and the range of the grapheme-colour synaesthesia score.\nTry using each subsetting method at least once.\n\n\nVisualisations\nTry making a histogram and a boxplot, using any of the variables in the syn_data dataset. Try using $ and pull() once each.\nOptionally, if you feel so inclined, use the help documentation to spruce up your plots a bit, such as changing the title and axis labels.\nCHALLENGE: Try making a barplot and a scatterplot.\nFor the barplot, make a visualisation of how many people are synaesthetes or not (regardless of synaesthesia type).\nFor the scatterplot, choose any two SCSQ measures.\nBoth of these require some creative problem-solving using the help documentation and the skills and functions covered in this tutorial."
  },
  {
    "objectID": "data_workbooks.html",
    "href": "data_workbooks.html",
    "title": "Data and Workbooks",
    "section": "",
    "text": "Download and save datasets to use for tutorial tasks here. Either copy the link to use in readr::read_csv(), or save the data at the link to a .csv file to read in.\n\n\n\nFilename\nCitation/Source\nComments\nURL\n\n\n\n\nsyn_data.csv\nMealor et al., 2016\nDataset publicly available\nLink\n\n\nanx_data.csv\nTerry, Lea, & Field (in prep)\nDataset shared for teaching purposes\nLink"
  },
  {
    "objectID": "data_workbooks.html#datasets",
    "href": "data_workbooks.html#datasets",
    "title": "Data and Workbooks",
    "section": "",
    "text": "Download and save datasets to use for tutorial tasks here. Either copy the link to use in readr::read_csv(), or save the data at the link to a .csv file to read in.\n\n\n\nFilename\nCitation/Source\nComments\nURL\n\n\n\n\nsyn_data.csv\nMealor et al., 2016\nDataset publicly available\nLink\n\n\nanx_data.csv\nTerry, Lea, & Field (in prep)\nDataset shared for teaching purposes\nLink"
  },
  {
    "objectID": "data_workbooks.html#workbooks",
    "href": "data_workbooks.html#workbooks",
    "title": "Data and Workbooks",
    "section": "Workbooks",
    "text": "Workbooks\nDownload and save workbook Qmd files to complete the tutorial tasks in.\n\n\n\nTutorial\nDownload\n\n\n\n\n01/02: IntRoduction\nDownload 01_02_intro_workbook.qmd\n\n\n03: Datasets\nDownload 03_datasets_workbook.qmd\n\n\n04: Reporting Linear Models with Quarto\nDownload 04_lm_workbook.qmd"
  },
  {
    "objectID": "tutorials/psychrlogy/02_essentials/05_filter.html#quick-test-t-test-and-chi2",
    "href": "tutorials/psychrlogy/02_essentials/05_filter.html#quick-test-t-test-and-chi2",
    "title": "05: Filter and Select",
    "section": "Quick Test: t-test and \\(\\chi^2\\)",
    "text": "Quick Test: t-test and \\(\\chi^2\\)\nFor our quick tests today, we’re going to revisit the t-test and also"
  }
]