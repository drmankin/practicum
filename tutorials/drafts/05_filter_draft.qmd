---
title: "05: Filter and Select"
---

## Overview

This tutorial covers two important {dplyr} functions: `filter()` and `select()`. Easy to confuse, `filter()` uses logical assertions to return a subset of rows (cases) in a dataset, while `select()` returns a subset of the columns (variables) in the dataset.

::: callout-tip
To remember which does which:

-   `filter()` works on **r**ows, which starts with "r", so it contains the letter "r".
-   `select()` works on **c**olumns, which starts with "c", so it contains the letter "c".
:::

## Setup

### Packages

We will be focusing on {dplyr} today, which contains both the `filter()` and `select()` functions. You can either load {dplyr} alone, or all of {tidyverse} - it won't make a difference, but you only need one or the other.

We will also make use of the {GGally} package later on for some snazzy visualisations.

::: {.callout-note appearance="minimal" title="Exercise"}

Load the necessary packages.

::: {.callout-note collapse="true" title="Solution"}
```{r}
#| eval: false

library(dplyr)
## OR
library(tidyverse)

library(GGally)
```
:::
:::

### Data and Codebook

For today's data, we're going to be using the delightful `penguins` dataset from the {palmerpenguins} package. This dataset is widely used for constructing examples and demonstrations with R, so you're likely to come across it *in the wild* (so to speak).

::: callout-tip

For a complete description of the dataset, along with some absolutely adorable illustrations of the eponymous penguins, see the [introduction on illustrator Allison Horst's Github](https://allisonhorst.github.io/palmerpenguins/articles/intro.html#exploring-correlations).

:::

One of the great things about this dataset is that you don't have to have it saved anywhere. Rather, it's installed when you install the `palmerpenguins` package, and once you have it installed, you can always get access to it.

::: {.callout-note appearance="minimal" title="Exercise"}

Create a new object, `peng_dat`, containing the dataset `palmerpenguins::penguins`. Have a quick look at the dataset to see what it contains.

::: {.callout-note collapse="true" title="Solution"}

```{r}
peng_dat <- palmerpenguins::penguins

dplyr::glimpse(peng_dat)
```
:::
:::

::: callout-important
If you aren't working on the R Training Posit Cloud workspace, you may need to install the {palmerpenguins} package first **in the Console**:

```r
install.packages("palmerpenguins")
```
:::

### Codebook

Because this dataset is from an R package, the codebook is stored as help documentation.

::: {.callout-note appearance="minimal" title="Exercise"}

Open the codebook for the `palmerpenguins::penguins` dataset as help documentation.

*Hint*: Use `?` and run this code **in the Console**.

::: {.callout-note collapse="true" title="Solution"}
```r
?palmerpenguins::penguins
```
:::
:::

Right, let's get started with our first of two major functions for this tutorial: `flipper()`. Oops, I mean `filter()`! (Sorry, penguin joke!)

## Filter

The `filter()` function's primary job is to easily and transparently **subset the rows** within a dataset - in particular, a `tibble`. `filter()` takes one or more logical assertions and returns only the rows for which the assertion is `TRUE`. Columns are not affected by `filter()`, only rows.

### General Format

```r
dataset_name |>
  dplyr::filter(
    logical_assertion
  )
```

### Using `filter()`

`logical_assertion` is a statement about the variable(s) in `dataset_name` that returns logical values, just like the assertions we saw in [the first tutorial](..\01_fundRmentals\01_02_intro.qmd). The rows where the assertion returns `TRUE` will be included in the output; those that return `FALSE` will not. Inside the `filter()` command, use the names of the variable in the piped dataset to create the logical assertions.

As a first example, let's use some of our familiar operators from the first tutorial. To retain only female penguins, we can run:

```{r}
peng_dat |> #<1>
  dplyr::filter( #<2>
    sex == "female" #<3>
    )
```

1. Take the dataset `peng_dat`, *and then*
1. Filter it keeping only the cases where the following assertion is true:
1. The value in the `sex` variable is exactly and only equal to `"female"`.

So, the tibble we get as output contains cases that have the value `"female"` in the `sex` variable, and NOT anything else. So `"male"` is not included, but neither is `NA` (because `NA` does not equal `"female"`!).

::: {.callout-warning title="Error Watch: Detected a named input" collapse="true"}
Remember that for exact matches like this, we must use double-equals `==` and not single-equals `=`. If you use single equals, you're not alone - this is such a common thing that the (incredibly friendly and helpful) error message tells you what to do to fix it!

```{r}
#| error: true

peng_dat |> 
  dplyr::filter(sex = "female")
```
:::

Naturally, we can also filter on numeric values. If we wanted to keep only penguins with short bills (say, less than 40mm), we can filter as follows:

```{r}
peng_dat |> #<1>
  dplyr::filter( #<2>
    bill_length_mm < 40 #<3>
    )
```

1. Take the dataset `peng_dat`, *and then*
1. Filter it keeping only the cases where the following assertion is true:
1. The value in the `bill_length_mm` variable is less than 30.

::: {.callout-note appearance="minimal" title="Exercise"}

Produce a subset of `peng_dat` that doesn't contain any Gentoo penguins.

::: {.callout-note collapse="true" title="Solution"}

```{r}
peng_dat |> 
  dplyr::filter(
    species != "Gentoo"
  )
```
:::
:::

::: {.callout-note appearance="minimal" title="Exercise"}

Produce a subset of `peng_dat` that contains only penguins with flippers as long as the median flipper length, or longer.

::: {.callout-note collapse="true" title="Solution"}

Here we can take advantage of the fact that we can use variable names as objects inside {dplyr} functions like `filter()`[^dm]. Then we write a logical assertion just like we have done in previous tutorials.

[^dm]: This incredibly useful property is called "data masking". If you want to know more, run `vignette("programming")` in the Console.

```{r}
peng_dat |> 
  dplyr::filter(
    flipper_length_mm >= median(flipper_length_mm, na.rm = TRUE)
  )
```

If you mysteriously got an empty tibble, you may have missed out the `na.rm = TRUE` argument to `median()`.
:::
:::

As a final example, let's consider a situation where we want to retain only penguins from either Biscoe or Dream Islands.

There are few ways to do this. If you have only three categories - as we do here - we can just drop the one we don't want, as we did above.

```{r}
peng_dat |> 
  dplyr::filter(island != "Torgersen")
```

However, if we *did* have more than three total categories, we'd face the same issue: namely, we want to compare a single value to more than one possible match, and get `TRUE` if it matches any of them. To do this, we need a new operator: `%in%`, which God knows I just pronounce as "in" (try saying "percent-in-percent" three times fast!). This looks for any matches with any of the elements that come after it:

```{r}
peng_dat |> #<1>
  dplyr::filter( #<2>
    island %in% c("Dream", "Biscoe") #<3>
    )
```

1. Take the dataset `peng_dat`, *and then*
1. Filter it keeping only the cases where the following assertion is true:
1. The value in the `island` variable matches any of the values "Dream" or "Biscoe".

::: {.callout-tip collapse="true" title="Why not `==`?"}

::: {.callout-important}
What follows here is a rabbit hole that gets into some gritty detail. If you're happy to take my word for it that you absolutely, definitely needed `%in%` and not `==` in the previous exercise, you can skip this. If you're keen to understand all the nuance, read on!
:::

For this matching task, you might have thought we'd use `island == c("Dream", "Biscoe")`, which runs successfully and sure looks okay. So why isn't this right?

```{r}
## DO NOT DO THIS
## IT DOES NOT DO WHAT WE WANT!!
peng_dat |> 
  dplyr::filter(island == c("Dream", "Biscoe"))
## DANGER WILL ROBINSON
```

At a glance it looks like this produces the same output as the solution above - `island` now contains only penguins from Dream or Biscoe. As you might have gathered from the all-caps comments above - intended to prevent you from accidentally using this code in the future for tasks like this - this is NOT what this code does. 

To demonstrate what it *does* do, I need the `dplyr::mutate()` function [from the next tutorial](06_changes.qmd#mutate) to create some new variables. The first new variable, `double_equals`, contains `TRUE`s and `FALSE`s for each case using the assertion with `==`. The second is exactly the same, but reverses the order of the island names - something that should NOT make a difference to the matching! (We want either Dream OR Biscoe penguins, regardless of which we happen to write first.) The third, `in_op`, contains the same again but this time with `%in%`. The final `filter()` line drops the Torgersen penguins to make the output easier to read.

```{r}
peng_dat |> 
  dplyr::mutate(
    double_equals = (island == c("Biscoe", "Dream")),
    double_equals_rev = (island == c("Dream", "Biscoe")),
    in_op = (island %in% c("Biscoe", "Dream")),
    .keep = "used"
  ) |> 
  dplyr::filter(island != "Torgersen")
```

Notice anything *wild*? 

For penguins with the same value in `island`, the assertions with `==` both flip between `TRUE` and `FALSE`, but in a reverse pattern to each other. The assertion with `%in%` correctly labels them all as `TRUE`. WTF?

What's happening is that because the vector `c("Biscoe", "Dream")` contains two elements, the assertion with `==` matches the first case to the first element - Biscoe - and returns `TRUE`. Then it matches the second case to the second element - Dream - and this time returns `FALSE`. Then because there are more cases, it repeats: the next (third) case matches Biscoe and returns `TRUE`, the next Dream and `FALSE`, and so forth. The `==` assertion with the island names reversed does the same, but starts with Dream first and Biscoe second. **Only `%in%` actually does what we wanted**, which was to return `TRUE` for any case that matches Biscoe OR Dream.

This is a good example of what I think of as "dangerous" code. I don't mean "reckless" or "irresponsible" - R is just doing exactly what I asked it to do, and it's not the job of the language or package creators to make sure *my* code is right. I mean dangerous because it runs as expected, produces (what looks like) the right output, and even with some brief checking, would appear to contain the right cases - but would quietly result in a large chunk of the data being wrongly discarded. If you didn't know about `%in%`, or how to carefully double-check your work, you could easily carry on from here and think no more about it.

So, how can we avoid a problem like this? Think of any coding task - especially new ones, where you're not completely familiar with the code or functions you're working with - as a three-step process[^acro].

[^acro]: I did try to think of a snazzy acronym here, but all I came up with is AEC (yikes). I'll keep thinking and try to update this with something better, and I welcome suggestions if you've made it this far!

- **Anticipate.** Form a clear picture of the task you are trying to achieve with your code. What do you expect the output of the code to look like when it runs successfully?
- **Execute.** Develop and run the code to perform the task.
- **Confirm.** Compare the output to your expectations, and perform tests to confirm that what you *think* the code has done, is in fact what it has done.

So, what might the Confirm step look like for a situation like this?

One option is the code I created above, with new columns for the different assertion options - but this might be something you'd only think to do if you already knew about `%in%` or suspected there was a problem. A more routine check might look like:

> I expect that when my filtering is accomplished, my dataset will contain all and only the penguins from Dream and Biscoe Islands, and none from Torgersen Island. I will also have the same number of cases as the original dataset, less the number of Torgersen penguins.

First, I'll create a new dataset using the filtered data.

```{r}
## SERIOUSLY THIS IS BAD
## DON'T USE THIS CODE FOR MATCHING
peng_dat_bd <- peng_dat |> 
  dplyr::filter(island == c("Dream", "Biscoe"))
## STOP OH GOD PLEASE JUST DON'T
```

Check 1: Filtered data contains only Dream and Biscoe penguins.

```{r}
peng_dat_bd |> 
  dplyr::count(island)
```
Only Biscoe and Dream. Tick!

At this point, though, I might become suspicious. The original dataset contained `r nrow(peng_dat)` cases - we've lost more than half! Can that be right?? Better check the numbers.

```{r}
## Get the numbers from the original dataset
peng_dat |> 
  dplyr::count(island)
```
Uh oh. Already we can see that something's wrong with the Biscoe and Dream groups. But instead of relying on visual checks, let's let R tell us.

```{r}
## Calculate how many cases we expect if the filtering had gone right
expected_n <- peng_dat |> 
  dplyr::count(island) |> 
  dplyr::filter(island != "Torgersen") |> 
  dplyr::pull(n) |> 
  sum()

## Ask R whether the expected number of rows is equal to the actual number of rows in the filtered data
expected_n == nrow(peng_dat_bd)
```
Now we know for sure there's a problem and can investigate what happened more thoroughly.

As a final stop on this *incredibly* lengthy detour (are you still here? ), you might wonder whether the check above would give me the wrong answer, because I used `island != "Torgersen"` to in my checks, and the whole point of this goose chase is how to accomplish that exact filtering task. Let's look at a few possibilities:

First, for this particular case there are three values in `island`. If I try `island == c("Biscoe", "Dream")` here, I get a warning that the length of one of the vector I'm trying to match (3) doesn't correspond to a multiple of the other (2 or 4). BUT, the filtering still works!!

```{r}
#| warning: true
peng_dat |> 
  dplyr::count(island) |> 
  dplyr::filter(island == c("Biscoe", "Dream"))
```

If I happened to have had the islands the other way round, I would have got an empty tibble, and hopefully that also would have clued me in that there was a problem with the original filtering.

```{r}
#| warning: true
peng_dat |> 
  dplyr::count(island) |> 
  dplyr::filter(island == c("Dream", "Biscoe"))
```
:::

### Multiple Conditions

Logical assertions can also be combined to specify exactly the cases you want to retain. The two most important operators are:

- `&` (AND): Only cases that return `TRUE` for **all** assertions will be retained.
- `|` (OR): Any cases that return `TRUE` for **at least one** assertion will be retained.

::: {.callout-tip title="More on AND and OR" collapse="true"}

Let's look at a couple minimal examples to get the hang of these two symbols. For each of these, you can think of the single response R gives as the answer to the questions, "Are ALL of these assertions true?" for AND, and "Is AT LEAST ONE of these assertions true?" for OR.

First, let's start with a few straightforward logical assertions:

```{r}
## TRUE
"apple" == "apple"
23 > 12

## FALSE
42 == "the answer"
10 > 50
```

Next, let's look at how they combine.

Two true statements, combined with `&`, return `TRUE`, because it is true that **all** of these assertions are true. (Fun huh?)

```{r}
"apple" == "apple" & 23 > 12
```

Two true statements, combined with `|`, also return `TRUE`, because it true that **at least one** of these assertions are true.

```{r}
"apple" == "apple" | 23 > 12
```

Two false statements, combined with `&`, return `FALSE`, because it is NOT true that **all** of them are true.

```{r}
42 == "the answer" & 10 > 50
```

Two false statements, combined with `|`, return `FALSE`, because it is NOT true that **at least one** of them is true.

```{r}
42 == "the answer" | 10 > 50
```

One true and one false statement, combined with `&`, return `FALSE`, because it is NOT true that **all** of them are true.

```{r}
42 == "the answer" & 23 > 12
```

One true and one false statement, combined with `|`, return `TRUE`, because it is true that **at least one** of them is true.

```{r}
42 == "the answer" | 23 > 12
```
:::

To see how this works, let's filter `peng_dat` to keep only cases that are from Dream Island, **OR** that have a body mass greater than the overall mean of body mass.

This requires two separate statements, combined with `|` "OR" - that is, we want to keep heftier penguins from Biscoe and Torgersen islands, and all of the penguins from Dream Island regardless of weight. 

```{r}
peng_dat |> #<1>
  dplyr::filter( #<2>
    island == "Dream" | #<3>
      body_mass_g > mean(body_mass_g, na.rm = TRUE) #<4>
    ) 
```

1. Take the dataset `peng_dat`, *and then*
1. Filter it keeping only the cases where the following assertion is true:
1. The value in the `island` variable only and exactly equal to `"Dream"`, **OR**
1. The value in `body_mass_g` is greater than the mean value of `body_mass_g` across the dataset, ignoring missing values.

::: {.callout-note appearance="minimal" title="Exercise"}

Filter `peng_dat` to keep only cases where the value of `bill_depth_mm` is between `15` and `20`.

*Hint*: You can use two separate assertions to do this, or check out `dplyr::between()`.

::: {.callout-note collapse="true" title="Solution"}

For the first solution, we must use `&` "AND" to ensure that both these conditions are met simultaneously.

For the second solution, the `dplyr::between()` function does the same operation, without having to worry about getting AND vs OR right.

```{r}
peng_dat |> 
  dplyr::filter(
    bill_depth_mm >= 15 & bill_depth_mm <= 20
  )

peng_dat |> 
  dplyr::filter(
    dplyr::between(bill_depth_mm, 15, 20)
  )
```
:::
:::

### Data Cleaning

Filtering is absolutely invaluable in the process of data cleaning. Penguins aren't the most typical participants in psychological studies, but with a little imagination, we can do the same kind of tasks we would undertake as standard for cleaning a study dataset here.

#### Recording Exclusions

As a part of complete and transparent reporting, we will want to report all of the reasons we excluded cases from our dataset, along with the number excluded. We can build this counting process into our workflow so that at the end, we have a record of each exclusion along with initial and final numbers.

::: {.callout-note appearance="minimal" title="Exercise"}

Follow along with the following steps, trying them out in a code chunk for yourself as you go. You'll need them at the end!

:::

To begin, we will count the initial number of cases before any exclusions.

```{r}
n_initial <- nrow(peng_dat)
```

(Remember that we can use `nrow()` because there is only one participant per row. If we had long-form data with observations from the same participant across multiple rows, we would have to do something a bit different!)

For each check below, our recording process will have two steps:

1. Produce a dataset of the cases you will **exclude**, and count the number of rows (cases).
1. Remove the cases and overwrite the old dataset with the new one[^overwrite].

In my process, I'm going to keep `peng_dat` as the original, "raw" version of the dataset. So, I'll create a copy in a new dataset object to use while "processing" that I will update as I go.

```{r}
peng_dat_proc <- peng_dat
```

#### Consent

If you have done a study online, you would likely have a variable with responses from your participants about informed consent. How you filter this depends on what that variable contains, of course. However, we've already seen examples of this kind of operation earlier in this tutorial, and it would probably look something like `consent == "Yes"`. As we saw before, this would discard cases that answered "No" (along with any other value not exactly matching "Yes") *and* cases with `NA`s from people who didn't answer.

For our penguins, we will begin the process by saying that only Adelie penguins consented to participate. So, following the two-step process, we first produce a dataset of all penguins EXCEPT Adelies, and count how many we are about to exclude by saving the resulting number in a new object

```{r}
n_no_consent <- peng_dat_proc |> 
  dplyr::filter(species != "Adelie") |> 
  nrow()
```

Then, we remove all non-Adelie penguins and assign the resulting dataset to the same name, overwriting the previous version.

```{r}
peng_dat_proc <- peng_dat_proc |> 
  dplyr::filter(species == "Adelie")
```

#### Age

For low-risk ethics applications, you may want to exclude people who reported an age below the age of informed consent (typically 18). This may look like ` age >= 18` or similar in your dataset.

Our penguins aren't on quite the same scale, but we can approximate it by saying that penguins measured in 2009 were too young, so we only want to keep penguins from 2008 or before. Again, we first count how many penguins we will exclude, then perform the exclusion.

```{r}
## Store the number to be removed
n_too_young <- peng_dat_proc |> 
  dplyr::filter(year > 2008) |> 
  nrow()

## Remove them
peng_dat_proc <- peng_dat_proc |> 
  dplyr::filter(
    year <= 2008
  )
```

#### Missing Values

Finally (for now), just about any study will have to decide how to deal with missing values. The possibilities for your own work are too complex for me to have a guess at here[^encourage], so for now we'll only look at how to identify and remove missing values.

Let's say our study focuses on sex differences in penguins. So, we must drop any penguins that don't have a sex recorded. The first thing you might think to try is to filter on `sex == NA`, but weirdly enough this doesn't work. Instead, we need to use a function from a family we met all the way back in tutorial 01, namely `is.na()`.

You can think of `is.na()` as a question about whatever is in its brackets: "Is (this) `NA`?" If the value IS an `NA`, R will return `TRUE`; if it's anything else at all, R will return `FALSE`. Let's see this in action:

```{r}
peng_dat_proc |> #<1>
  dplyr::filter( #<2>
    is.na(sex) #<3>
  )
```

1. Take the dataset `peng_dat_proc`, *and then*
1. Filter it keeping only the cases where the following assertion is true:
1. The value in the `sex` variable IS missing (is `NA`).

These are the cases we want to *remove*, so we count how many there are and assign that number to a useful object name, as we did before.

```{r}
n_sex_missing <- peng_dat_proc |> #<1>
  dplyr::filter( #<2>
    is.na(sex) #<3>
  )
```

Next, we need to actually exclude these cases. This time, we want to retain the inverse of the previous filtering requirement: that is, we only want to keep the cases that are NOT missing a value in sex, the opposite of what we got from `is.na(sex)`. You may recognise "the inverse" or "not-x" as something we've seen before with `!=`, "not-equals". For anything that returns `TRUE` and `FALSE`, you can get the inverse by putting an `!` before it. (Try running `!TRUE`, for example!)

So, to create my clean `peng_dat_final` dataset, I can use the assertion `!is.na(sex)` to keep only the penguins who do **NOT** have `NA` in the `sex` variable. 

Finally, I can store the actual number of usable cases, according to my cleaning requirements, in a final object to use when reporting.

```{r}
peng_dat_final <- peng_dat_proc |> #<1>
  dplyr::filter( #<2>
    !is.na(sex) #<3>
  )

n_final <- nrow(peng_dat_final)
```

::: {.callout-note appearance="minimal" title="Exercise"}

**CHALLENGE**: Using the objects counting intial, final, and excluded cases and what we covered last time about inline code, write a brief journal-style description of your exclusion process.

What is the benefit of taking the extra effort to store these counts in objects? Under what circumstances might this be (particularly) useful?

::: {.callout-note collapse="true" title="Solution"}

You can write whatever you like, but here's an example using inline code. I've written this to be generic, rather than about penguins, in case you'd like to refer to it for non-penguin-related studies in the future.

> The initial sample consisted of `` `r knitr::inline_expr("n_intial")` `` cases. We removed `` `r knitr::inline_expr("n_no_consent")` `` cases that did not consent, and `` `r knitr::inline_expr("n_too_young")` `` cases that reported an age below the ethical age of consent. Finally, we removed `` `r knitr::inline_expr("n_sex_missing")` `` cases with no recorded sex. This left us with a final sample of `` `r knitr::inline_expr("n_final")` `` cases.

When you render your document, this should come out as:

> The initial sample consisted of `r n_intial` cases. We removed `r n_no_consent` cases that did not consent, and `r n_too_young` cases that reported an age below the ethical age of consent. Finally, we removed `r n_sex_missing` cases with no recorded sex. This left us with a final sample of `r n_final` cases.

There's a huge advantage of this, namely *ease of change*. Imagine you had a collaborator join the study, with records from more Adelie penguins to use. In order to update all your numbers, all you have to do is update your initial `peng_dat` dataset with the new cases, and then re-run all your code as is. Because these objects count whatever is in the data, they will automatically contain and record the correct numbers for the data you put into them.

There are other advantages too - like confidence that you, a human person who may occasionally make errors (sorry, no offence meant!), won't misread, mistype, or otherwise mistake the numbers, because at no point do you actually interact with a particular count directly.

Nifty, eh? 
:::
:::

## Select

The `select()` function is probably the most straightforward of the core {dplyr} functions. Its primary job is to easily and transparently **subset the columns** within a dataset - in particular, a `tibble`. Rows are not affected by `select()`, only columns.

### Basic Structure

To subset a tibble, use the general format:

```{r}
#| eval: false

my_tibble |>
  dplyr::select(
    variable_to_keep, !variable_to_drop, 
    keep_this_one:through_this_one,
    new_name = variable_to_rename,
    variable_number
  )

```

#### Usage

-   `variable_to_keep`: Choose a variable to include in the output by including its name. Multiple variables can be selected separated by commas.
-   `!variable_to_drop`: Drop a variable from the output by putting an exclamation mark (`!`) or a minus sign (`-`) in front of its name. Multiple variables can be dropped, separated by commas with a `!` (or `-`) before each.
-   `keep_this_one:through_this_one`: Choose a range of variables to include in the output with a colon (`:`). All the variables between and including the two named will be selected (or dropped, with `!(drop_this_one:through_this_one)`)
-   `rename_variable`: Choose a variable to include in the output, but give it a new name.
-   `variable_number`: Choose a variable to include in the output by where it appears in the dataset, numbered left to right. For example, "2" will select the second column in the original dataset.

Columns will appear in the output in the order they are selected in `select()`, so this function can also be used to reorder columns.

### `<tidyselect>`

The real power in `select()`, and in many other {tidyverse} functions, is in a system of helper functions and notations collectively called `<tidyselect>`. The overall goal of "`<tidyselect>` semantics" is to make selecting variables easy, efficient, and clear.

These helper functions can be combined with the selection methods above in any combination. Some very convenient options include:

-   `everything()` for all columns
-   `starts_with()`, `ends_with()`, and `contains()` for selecting columns by name
-   `num_range()` for selecting columns with a shared prefix and numerical range, e.g. `item_1`, `item_2`, `item_3` etc.
-   `all_of()` and `any_of()` for selecting using character vectors
-   `where()` for [selecting with a function](06_filter.qmd#using-functions)
-   `matches()` for selecting with [regular expressions](https://ladal.edu.au/regex.html) (not covered in this tutorial)

Rather than list examples of all the helper functions here, it's best to just try them out for yourself!s

::: {.callout-note appearance="minimal" title="Exercises"}
Open the help documentation by running `?dplyr::select` in the Console to see examples of how to use all of the `<tidyselect>` helper functions.

Select the columns that start with

::: {.callout-note collapse="true" title="Solution"}
```{r}

```
:::

Select all of the columns that contain a score

::: {.callout-note collapse="true" title="Solution"}
```{r}

```
:::

Select the first ten items of the C and D subscales

::: {.callout-note collapse="true" title="Solution"}
```{r}

```
:::

Redo the exercise from the previous section using `<tidyselect>` helpers.

```{r}

```
:::

#### Using Functions

Let's say we want to create a summary table of all of the numeric variables in our dataset. Before we can [create our summary in the next tutorial](07_changes.qmd#summarise), we may first want to produce a subset of our dataset that only contains numeric variables.

To do this, we can use the `<tidyselect>` helper function `where()`. This helper function lets us use *any* function that returns `TRUE` and `FALSE` to select columns. Essentially, we don't have to select columns by name - we can use any criteria we want, as long as we have (or can create!) a function that expresses that criteria.

Especially helpful here is the `is.*()` family of functions in base R. This group of functions all have the same format, where the `*` is a stand-in for any type of data or object, e.g. `is.logical()`, `is.numeric()`, `is.factor()` etc. (The very useful `is.na()` that we've seen above is also a member of this family.) These functions work like a question about whatever you put into them - for example, `is.numeric()` can be read as, "Is (whatever's in the brackets) numeric?"

::: callout-tip
You can quickly find all of the functions in this family by typing `is.` in the Console and pressing Tab.
:::

Putting these two together, we could accomplish the task of selecting only numeric variables as follows:

```{r}
#| eval: false

my_tibble |> 
  dplyr::select(
    where(is.numeric)
  )
```

This command evaluates each column and determines whether they contain numeric data (`TRUE`) or not (`FALSE`), and only returns the columns that return `TRUE`.

::: callout-warning
The following material in this section isn't covered in the Practicum live workshops. It's included here for reference because it's extremely useful in real R analysis workflows, but it won't be essential for any of the Practicum tasks.
:::

The function in `where()` that determines which columns to keep doesn't have to be an existing named function. Another option is to use a "purrr-style lambda" or formula (a phrase you may see in help documentation) to write our own criteria on the spot. What we are essentially doing is writing an ad-hoc function.

For example, let's select all of the numeric variables that have a minimum value of 3:

```{r}
#| eval: false

my_tibble |> 
  dplyr::select(
    where(is.numeric & ~ min(.x, na.rm = TRUE) >= 3)
  )
```

The second of these criteria is one of these "purrr-style" formulae. Is has two components:

-   The `~` (apparently [pronounced "twiddle"!](https://adv-r.hadley.nz/functionals.html#purrr-shortcuts)) at the beginning, which is a shortcut for the longer `function(x) ...` notation for creating functions.
-   The `.x`, which is a placeholder for each of the variables that the function will be applied to.

So, this command can be read: "Take my tibble and select all the columns where the following is true: the data type is numeric and the minimum value in that column is greater than or equal to 3 (ignoring missing values)."

::: {.callout-note appearance="minimal" title="Optional Exercises"}
::: {.callout-note collapse="true" title="Solution"}
```{r}

```
:::
:::
